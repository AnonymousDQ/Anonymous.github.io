<!DOCTYPE html>



  


<html class="theme-next pisces use-motion" lang="zh-Hans">
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT">





  <link rel="alternate" href="/atom.xml" title="drqblog" type="application/atom+xml">






<meta property="og:type" content="website">
<meta property="og:title" content="drqblog">
<meta property="og:url" content="http://yoursite.com/index.html">
<meta property="og:site_name" content="drqblog">
<meta property="og:locale" content="zh-Hans">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="drqblog">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '5.1.4',
    sidebar: {"position":"right","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/">





  <title>drqblog</title>
  








</head>

<body itemscope="" itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-right 
  page-home">
    <div class="headband"></div>
	<a href="https://github.com/AnonymousDQ">
	<img style="position: absolute; top: 0; right: 0; border: 0;" src="https://s3.amazonaws.com/github/ribbons/forkme_right_green_007200.png" alt="Fork me on GitHub">
	</a>

    <header id="header" class="header" itemscope="" itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">drqblog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/index" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br>
            
            关于
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br>
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br>
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-schedule">
          <a href="/schedule/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-calendar"></i> <br>
            
            日程表
          </a>
        </li>
      
        
        <li class="menu-item menu-item-sitemap">
          <a href="/sitemap.xml" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-sitemap"></i> <br>
            
            站点地图
          </a>
        </li>
      
        
        <li class="menu-item menu-item-commonweal">
          <a href="/404/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-heartbeat"></i> <br>
            
            公益404
          </a>
        </li>
      
        
        <li class="menu-item menu-item-something">
          <a href="/something" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-question-circle"></i> <br>
            
            有料
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/12/16/SpringBoot入门/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Victor Drq">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/header.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="drqblog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/12/16/SpringBoot入门/" itemprop="url">SprintBoot入门</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-12-16T17:39:13+08:00">
                2018-12-16
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/categories/JAVA/" itemprop="url" rel="index">
                    <span itemprop="name">JAVA</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">字数统计&#58;</span>
                
                <span title="字数统计">
                  1.4k
                </span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                
                <span title="阅读时长">
                  5
                </span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="一、Spring-Boot入门"><a href="#一、Spring-Boot入门" class="headerlink" title="一、Spring Boot入门"></a>一、Spring Boot入门</h1><p>Spring Boot来简化Spring应用开发，约定大于配置，去繁从简，just run就能创建一个独立的，产品级别的应用</p>
<h4 id="背景："><a href="#背景：" class="headerlink" title="背景："></a>背景：</h4><p>J2EE笨重的开发，繁多的配置，低下的开发效率，复杂的部署流程，第三方技术集成难度大</p>
<h4 id="解决："><a href="#解决：" class="headerlink" title="解决："></a>解决：</h4><p>“Spring全家桶”时代</p>
<p>Spring Boot–&gt;J2EE一站式解决方案</p>
<p>Spring Cloud–&gt;分布式整体解决方案</p>
<h4 id="优点："><a href="#优点：" class="headerlink" title="优点："></a>优点：</h4><ul>
<li>快速创建独立运行的Spring项目以及主流框架集成</li>
<li>使用嵌入式的Servlet容器，应用无需打成WAR包</li>
<li>starters(启动器)自动依赖与版本控制，想用web功能就导入web功能的启动器starters，想用redis就导入redis的starters，所有的企业级开发场景都有相应的starters启动器，导入就可自动依赖。</li>
<li>大量的自动的配置，简化开发，也可修改默认值（通过spring boot的配置文件）</li>
<li>无需配置XML，无代码生成，开箱即用（都是用一些写好的API，自动配置好XML，SpringBoot应用创建出来就能用，并不是说有一些自动生成的XML工具）</li>
<li>准生产环境的运行时应用监控</li>
<li>与云计算的天然集成</li>
<li>ps(shift+tab退出无序列表)</li>
</ul>
<h4 id="缺点：易学难精"><a href="#缺点：易学难精" class="headerlink" title="缺点：易学难精"></a>缺点：易学难精</h4><h4 id="1、Spring-Boot简介"><a href="#1、Spring-Boot简介" class="headerlink" title="1、Spring Boot简介"></a>1、Spring Boot简介</h4><p>简化Spring应用开发的一个框架</p>
<p>整个Spring技术栈的一个大集合</p>
<p>J2EE开发的一站式解决方案</p>
<h4 id="2、微服务"><a href="#2、微服务" class="headerlink" title="2、微服务"></a>2、微服务</h4><p><a href="https://martinfowler.com/articles/microservices.html" target="_blank" rel="noopener">微服务文档</a></p>
<p>Microservices:a definition of this new architectural term</p>
<p>微服务：架构风格</p>
<p>the microservice architectural style  is an approach to developing a single application as a suite of small services, each running in its own process and communicating with lightweight mechanisms, often an HTTP resource API.</p>
<p>一个应用应该是一组小型服务：可以通过HTTP的方式进行互通</p>
<h3 id="单体应用：ALL-IN-ONE所有的东西都写在一个里面"><a href="#单体应用：ALL-IN-ONE所有的东西都写在一个里面" class="headerlink" title="单体应用：ALL IN ONE所有的东西都写在一个里面"></a>单体应用：ALL IN ONE所有的东西都写在一个里面</h3><p>Traditional web application architecture</p>
<p><img src="/2018/12/16/SpringBoot入门/SprintBoot入门/单体.png" alt="单体应用"></p>
<p>OA,CRM,ERP系统,以前都是创建一个应用然后将所有的页面，代码都放在这个应用里，然后把整个应用打包打成WAR包，然后部署到Tomcat上，应用访问数据库，提供前端访问的页面，这个应用就跑起来了，这是传统的WEB应用架构模式，传统的优点：比如开发测试简单，develop test deploy scale,开发，测试，部署，扩展也都简单。</p>
<p>水平扩展也简单，当我们应用负载能力不行的时候，我们把相同的应用复制上十几份，放在十几个服务器里，十几个服务器都来跑我们这些应用程序，我们通过负载均衡机制，就可以来提高我们的并发能力。</p>
<h4 id="单体应用的问题："><a href="#单体应用的问题：" class="headerlink" title="单体应用的问题："></a>单体应用的问题：</h4><p>这是一个牵一发而动全身的问题，有可能因为我们一个小小的修改，导致我们整个应用重新打包部署运行。</p>
<p>更大的挑战是我们日益增长的软件需求，现在可能随便一个应用都有可能成为一个大的需求，大应用不可能全部ALL IN ONE写在一个里面，然后应用到底有多大，该如何维护，该如何分工合作，这是一个问题。</p>
<h3 id="微服务"><a href="#微服务" class="headerlink" title="微服务"></a>微服务</h3><p><img src="/2018/12/16/SpringBoot入门/SprintBoot入门/微服务.png" alt="微服务"></p>
<h4 id="单体应用："><a href="#单体应用：" class="headerlink" title="单体应用："></a>单体应用：</h4><p>就是打破以前的传统方式，以前是将所有的功能单元放在一个应用里面。然后把整个应用部署到服务器上，如果服务器负载能力不行，把同一份应用水平复制，然后扩展到其他服务器。</p>
<h4 id="微服务："><a href="#微服务：" class="headerlink" title="微服务："></a>微服务：</h4><p>一个微服务架构把每个功能元素放进一个独立的服务中，把每个功能元素独立出来，通过功能元素的动态组合，比如A服务器需要某个功能元素多，就多放一点，B服务器需要某个功能元素少，就少放一点</p>
<p>并且通过跨服务器分发这些服务进行扩展，某些功能只在需要时才复制。也就是功能元素级别的复制，并不是整个应用的复制。1、节省了调用资源，把服务微化起来2、每一个服务都应该是一个可替换的，可独立升级的软件单元</p>
<p><img src="/2018/12/16/SpringBoot入门/SprintBoot入门/timg.jpg" alt="区别"></p>
<p>每一个功能元素最终都是可独立替换，可独立升级的软件单元</p>
<h4 id="SOA架构和微服务架构区别："><a href="#SOA架构和微服务架构区别：" class="headerlink" title="SOA架构和微服务架构区别："></a>SOA架构和微服务架构区别：</h4><ul>
<li><p>SOA(Service Oriented Architecture)：面向服务的架构，一种设计方法，其中包含多个服务，服务之间通过相互依赖最终提供一系列的功能，一个服务通常以独立的形式存在于操作系统进程中，各个服务之间通过网络调用。</p>
</li>
<li><p>微服务架构：其实和SOA架构类似，微服务是在SOA上做的升华，微服务架构强调的一个重点是“业务需要彻底的组件化和服务化”，原有的但各业务系统会拆分成多个可以独立开发，设计，运行的小应用</p>
<p>这些小应用之间通过服务完成交互和集成</p>
</li>
</ul>
<h4 id="主要区别："><a href="#主要区别：" class="headerlink" title="主要区别："></a>主要区别：</h4><table>
<thead>
<tr>
<th style="text-align:center">功能</th>
<th style="text-align:center">SOA</th>
<th style="text-align:center">微服务</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">组件大小</td>
<td style="text-align:center">大块业务逻辑</td>
<td style="text-align:center">单独任务或小块业务逻辑</td>
</tr>
<tr>
<td style="text-align:center">耦合</td>
<td style="text-align:center">通常松耦合</td>
<td style="text-align:center">总是松耦合</td>
</tr>
<tr>
<td style="text-align:center">公司架构</td>
<td style="text-align:center">任何类型</td>
<td style="text-align:center">小型，专注于功能交叉团队</td>
</tr>
<tr>
<td style="text-align:center">管理</td>
<td style="text-align:center">着重中央管理</td>
<td style="text-align:center">着重分散管理</td>
</tr>
<tr>
<td style="text-align:center">目标</td>
<td style="text-align:center">确保应用能够交互操作</td>
<td style="text-align:center">执行新功能，快速拓展开发团队</td>
</tr>
</tbody>
</table>
<h3 id="掌握内容："><a href="#掌握内容：" class="headerlink" title="掌握内容："></a>掌握内容：</h3><ul>
<li>Spring框架</li>
<li>熟练使用Maven进行项目构建和依赖管理</li>
<li>熟练使用Eclipse、IDEA</li>
</ul>
<h3 id="环境约束："><a href="#环境约束：" class="headerlink" title="环境约束："></a>环境约束：</h3><ul>
<li>JDK1.8</li>
<li>Maven3.x:Maven 3.3以上版本</li>
<li>IntelliJ IDEA</li>
<li>Spring Boot 1.5.9.RELEASE</li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/12/16/multiplayer-perceptron多层感知器/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Victor Drq">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/header.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="drqblog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/12/16/multiplayer-perceptron多层感知器/" itemprop="url">multiplayer_perceptron多层感知器</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-12-16T17:06:43+08:00">
                2018-12-16
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/categories/TensorFlow/" itemprop="url" rel="index">
                    <span itemprop="name">TensorFlow</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">字数统计&#58;</span>
                
                <span title="字数统计">
                  525
                </span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                
                <span title="阅读时长">
                  2
                </span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="多层感知器MultiLayer-Perceptron"><a href="#多层感知器MultiLayer-Perceptron" class="headerlink" title="多层感知器MultiLayer Perceptron"></a>多层感知器MultiLayer Perceptron</h2><p>​    多层感知器又感知机推广而来，最主要的特点是有多个神经元层，因此也叫深度神经网络(DNN:Deep Neural Networks)。MLP是一种前馈人工神经完了过，它将输入的多个数据集映射到单一的输出数据集上。</p>
<p>​    MLP可以看作是一个有向图，由多个的节点层组成，每一层都<strong>全连接</strong>到下一层。除了输入节点，每个节点都是一个带有<strong>非线性激活函数</strong>的神经元。而<strong>反向传播算法(BP:Back Propagation算法)</strong>的监督学习方法用来训练MLP。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#author:victor</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#import module</span></span><br><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="comment"># Import MNIST data</span></span><br><span class="line"><span class="keyword">from</span> tensorflow.examples.tutorials.mnist <span class="keyword">import</span> input_data</span><br><span class="line">mnist = input_data.read_data_sets(<span class="string">"C:/Users/DELL/Desktop/TensorFlow/MINISTdatabase/MNIST_data"</span>, one_hot=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Parameters</span></span><br><span class="line">learning_rate = <span class="number">0.001</span></span><br><span class="line">training_epochs = <span class="number">15</span></span><br><span class="line">batch_size = <span class="number">100</span></span><br><span class="line">display_step = <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Network Parameters</span></span><br><span class="line">n_hidden_1 = <span class="number">256</span> <span class="comment"># 1st layer number of neurons</span></span><br><span class="line">n_hidden_2 = <span class="number">256</span> <span class="comment"># 2nd layer number of neurons</span></span><br><span class="line">n_input = <span class="number">784</span> <span class="comment"># MNIST data input (img shape: 28*28)</span></span><br><span class="line">n_classes = <span class="number">10</span> <span class="comment"># MNIST total classes (0-9 digits)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># tf Graph input</span></span><br><span class="line">X = tf.placeholder(<span class="string">"float"</span>, [<span class="keyword">None</span>, n_input])</span><br><span class="line">Y = tf.placeholder(<span class="string">"float"</span>, [<span class="keyword">None</span>, n_classes])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Store layers weight &amp; bias</span></span><br><span class="line">weights = &#123;</span><br><span class="line">    <span class="string">'h1'</span>: tf.Variable(tf.random_normal([n_input, n_hidden_1])),</span><br><span class="line">    <span class="string">'h2'</span>: tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),</span><br><span class="line">    <span class="string">'out'</span>: tf.Variable(tf.random_normal([n_hidden_2, n_classes]))</span><br><span class="line">&#125;</span><br><span class="line">biases = &#123;</span><br><span class="line">    <span class="string">'b1'</span>: tf.Variable(tf.random_normal([n_hidden_1])),</span><br><span class="line">    <span class="string">'b2'</span>: tf.Variable(tf.random_normal([n_hidden_2])),</span><br><span class="line">    <span class="string">'out'</span>: tf.Variable(tf.random_normal([n_classes]))</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Create model</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">multilayer_perceptron</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="comment"># Hidden fully connected layer with 256 neurons</span></span><br><span class="line">    layer_1 = tf.add(tf.matmul(x, weights[<span class="string">'h1'</span>]), biases[<span class="string">'b1'</span>])</span><br><span class="line">    <span class="comment"># Hidden fully connected layer with 256 neurons</span></span><br><span class="line">    layer_2 = tf.add(tf.matmul(layer_1, weights[<span class="string">'h2'</span>]), biases[<span class="string">'b2'</span>])</span><br><span class="line">    <span class="comment"># Output fully connected layer with a neuron for each class</span></span><br><span class="line">    out_layer = tf.matmul(layer_2, weights[<span class="string">'out'</span>]) + biases[<span class="string">'out'</span>]</span><br><span class="line">    <span class="keyword">return</span> out_layer</span><br><span class="line"></span><br><span class="line"><span class="comment"># Construct model</span></span><br><span class="line">logits = multilayer_perceptron(X)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Define loss and optimizer</span></span><br><span class="line">loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(</span><br><span class="line">    logits=logits, labels=Y))</span><br><span class="line">optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)</span><br><span class="line">train_op = optimizer.minimize(loss_op)</span><br><span class="line"><span class="comment"># Initializing the variables</span></span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(init)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Training cycle</span></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(training_epochs):</span><br><span class="line">        avg_cost = <span class="number">0.</span></span><br><span class="line">        total_batch = int(mnist.train.num_examples/batch_size)</span><br><span class="line">        <span class="comment"># Loop over all batches</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(total_batch):</span><br><span class="line">            batch_x, batch_y = mnist.train.next_batch(batch_size)</span><br><span class="line">            <span class="comment"># Run optimization op (backprop) and cost op (to get loss value)</span></span><br><span class="line">            _, c = sess.run([train_op, loss_op], feed_dict=&#123;X: batch_x,</span><br><span class="line">                                                            Y: batch_y&#125;)</span><br><span class="line">            <span class="comment"># Compute average loss</span></span><br><span class="line">            avg_cost += c / total_batch</span><br><span class="line">        <span class="comment"># Display logs per epoch step</span></span><br><span class="line">        <span class="keyword">if</span> epoch % display_step == <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">"Epoch:"</span>, <span class="string">'%04d'</span> % (epoch+<span class="number">1</span>), <span class="string">"cost=&#123;:.9f&#125;"</span>.format(avg_cost))</span><br><span class="line">    print(<span class="string">"Optimization Finished!"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Test model</span></span><br><span class="line">    pred = tf.nn.softmax(logits)  <span class="comment"># Apply softmax to logits</span></span><br><span class="line">    correct_prediction = tf.equal(tf.argmax(pred, <span class="number">1</span>), tf.argmax(Y, <span class="number">1</span>))</span><br><span class="line">    <span class="comment"># Calculate accuracy</span></span><br><span class="line">    accuracy = tf.reduce_mean(tf.cast(correct_prediction, <span class="string">"float"</span>))</span><br><span class="line">    print(<span class="string">"Accuracy:"</span>, accuracy.eval(&#123;X: mnist.test.images, Y: mnist.test.labels&#125;))</span><br></pre></td></tr></table></figure>
<h2 id="运行结果"><a href="#运行结果" class="headerlink" title="运行结果"></a>运行结果</h2><p><img src="/2018/12/16/multiplayer-perceptron多层感知器/1.gif" alt="multiplayer perceptron"></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/12/16/使用KNN分类MNIST-data/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Victor Drq">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/header.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="drqblog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/12/16/使用KNN分类MNIST-data/" itemprop="url">使用KNN分类MNIST_data</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-12-16T16:56:57+08:00">
                2018-12-16
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/categories/TensorFlow/" itemprop="url" rel="index">
                    <span itemprop="name">TensorFlow</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">字数统计&#58;</span>
                
                <span title="字数统计">
                  485
                </span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                
                <span title="阅读时长">
                  2
                </span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="使用KNN-K-NearestNeighbor"><a href="#使用KNN-K-NearestNeighbor" class="headerlink" title="使用KNN(K-NearestNeighbor)"></a>使用KNN(K-NearestNeighbor)</h2><p>​    邻近算法，或者说K最近邻(KNN，k-NearestNeighbor)分类算法是<a href="https://baike.baidu.com/item/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98/216477" target="_blank" rel="noopener">数据挖掘</a>分类技术中最简单的方法之一。所谓K最近邻，就是k个最近的邻居的意思，说的是每个样本都可以用它最接近的k个邻居来代表。</p>
<p>​    KNN算法的核心思想是如果一个样本在特征空间中的k个最相邻的样本中的大多数属于某一个类别，则该样本也属于这个类别，并具有这个类别上样本的特性。该方法在确定分类决策上只依据最邻近的一个或者几个样本的类别来决定待分样本所属的类别。 KNN方法在类别决策时，只与极少量的相邻样本有关。由于KNN方法主要靠周围有限的邻近的样本，而不是靠判别类域的方法来确定所属类别的，因此对于类域的交叉或重叠较多的待分样本集来说，KNN方法较其他方法更为适合。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#import module</span></span><br><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="comment"># Import MNIST data</span></span><br><span class="line"><span class="keyword">from</span> tensorflow.examples.tutorials.mnist <span class="keyword">import</span> input_data</span><br><span class="line">mnist = input_data.read_data_sets(<span class="string">"C:/Users/DELL/Desktop/TensorFlow/MINISTdatabase/MNIST_data"</span>, one_hot=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># In this example, we limit mnist data</span></span><br><span class="line">Xtr, Ytr = mnist.train.next_batch(<span class="number">5000</span>) <span class="comment">#5000 for training (nn candidates)</span></span><br><span class="line">Xte, Yte = mnist.test.next_batch(<span class="number">200</span>) <span class="comment">#200 for testing</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># tf Graph Input</span></span><br><span class="line">xtr = tf.placeholder(<span class="string">"float"</span>, [<span class="keyword">None</span>, <span class="number">784</span>])</span><br><span class="line">xte = tf.placeholder(<span class="string">"float"</span>, [<span class="number">784</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Nearest Neighbor calculation using L1 Distance</span></span><br><span class="line"><span class="comment"># Calculate L1 Distance</span></span><br><span class="line">distance = tf.reduce_sum(tf.abs(tf.add(xtr, tf.negative(xte))), reduction_indices=<span class="number">1</span>)</span><br><span class="line"><span class="comment"># Prediction: Get min distance index (Nearest neighbor)</span></span><br><span class="line">pred = tf.arg_min(distance, <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">accuracy = <span class="number">0.</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Initialize the variables (i.e. assign their default value)</span></span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Start training</span></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Run the initializer</span></span><br><span class="line">    sess.run(init)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># loop over test data</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(Xte)):</span><br><span class="line">        <span class="comment"># Get nearest neighbor</span></span><br><span class="line">        nn_index = sess.run(pred, feed_dict=&#123;xtr: Xtr, xte: Xte[i, :]&#125;)</span><br><span class="line">        <span class="comment"># Get nearest neighbor class label and compare it to its true label</span></span><br><span class="line">        print(<span class="string">"Test"</span>, i, <span class="string">"Prediction:"</span>, np.argmax(Ytr[nn_index]), \</span><br><span class="line">            <span class="string">"True Class:"</span>, np.argmax(Yte[i]))</span><br><span class="line">        <span class="comment"># Calculate accuracy</span></span><br><span class="line">        <span class="keyword">if</span> np.argmax(Ytr[nn_index]) == np.argmax(Yte[i]):</span><br><span class="line">            accuracy += <span class="number">1.</span>/len(Xte)</span><br><span class="line">    print(<span class="string">"Done!"</span>)</span><br><span class="line">    print(<span class="string">"Accuracy:"</span>, accuracy)</span><br></pre></td></tr></table></figure>
<h2 id="运行结果"><a href="#运行结果" class="headerlink" title="运行结果"></a>运行结果</h2><p><img src="/2018/12/16/使用KNN分类MNIST-data/6.gif" alt="KNN Classfication"></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/12/16/Neural-Network普通神经网络/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Victor Drq">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/header.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="drqblog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/12/16/Neural-Network普通神经网络/" itemprop="url">Neural Network普通神经网络</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-12-16T16:48:39+08:00">
                2018-12-16
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/categories/TensorFlow/" itemprop="url" rel="index">
                    <span itemprop="name">TensorFlow</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">字数统计&#58;</span>
                
                <span title="字数统计">
                  336
                </span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                
                <span title="阅读时长">
                  1
                </span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="Neural-Networks神经网络"><a href="#Neural-Networks神经网络" class="headerlink" title="Neural Networks神经网络"></a>Neural Networks神经网络</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#author:victor</span></span><br><span class="line"><span class="comment">#Nenural Networks神经网络</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">input layer-&gt;hidden layer1-&gt;hidden layer2-&gt;hidden layer3...-&gt;output layer</span></span><br><span class="line"><span class="string">#输入层-&gt;隐藏层-&gt;输出层</span></span><br><span class="line"><span class="string">#激活函数Activation Function（激励函数）</span></span><br><span class="line"><span class="string">#神经网络的基本原理：梯度下降Gradient Descent in Neural Nets</span></span><br><span class="line"><span class="string">#Optimization优化器</span></span><br><span class="line"><span class="string">1、Newton's method牛顿法</span></span><br><span class="line"><span class="string">2、Least Squares method最小二乘法</span></span><br><span class="line"><span class="string">3、Gradient Descent梯度下降法（也就是求导，求微分）神经网络就是梯度下降里的分支</span></span><br><span class="line"><span class="string">Cost=(predicted-real)^2=(Wx-y)^2=(W-0)^2(误差曲线)</span></span><br><span class="line"><span class="string">局部最优解，全局最优解</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="comment">#import module</span></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment">#create data</span></span><br><span class="line">x_data=np.random.rand(<span class="number">100</span>).astype(np.float32)<span class="comment">#tensorflow大部分数据是float32</span></span><br><span class="line">y_data=x_data*<span class="number">0.1</span>+<span class="number">0.3</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#create tensorflow structure start#</span></span><br><span class="line">Weights=tf.Variable(tf.random_uniform([<span class="number">1</span>],<span class="number">-1.0</span>,<span class="number">1.0</span>))<span class="comment">#random_uniform()：随机均匀分布</span></span><br><span class="line"><span class="comment">#define the biases</span></span><br><span class="line">biases=tf.Variable(tf.zeros([<span class="number">1</span>]))</span><br><span class="line"></span><br><span class="line">y=Weights*x_data+biases<span class="comment">#预测的y</span></span><br><span class="line"></span><br><span class="line">loss=tf.reduce_mean(tf.square(y-y_data))<span class="comment">#计算预测的y与真实的y的差值，也就是损失函数</span></span><br><span class="line">optimizer=tf.train.GradientDescentOptimizer(<span class="number">0.5</span>)<span class="comment">#learning rate学习效率一般是小于1的数</span></span><br><span class="line">train=optimizer.minimize(loss)</span><br><span class="line"></span><br><span class="line">init=tf.initialize_all_variables()<span class="comment">#初始化全局变量</span></span><br><span class="line"><span class="comment">#create tensorflow structure end#</span></span><br><span class="line"></span><br><span class="line">sess=tf.Session()</span><br><span class="line">sess.run(init)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> step <span class="keyword">in</span> range(<span class="number">201</span>):<span class="comment">#0到200，也就是201步</span></span><br><span class="line">    sess.run(train)</span><br><span class="line">    <span class="keyword">if</span> step%<span class="number">20</span>==<span class="number">0</span>:</span><br><span class="line">        print(step,sess.run(Weights),sess.run(biases))</span><br></pre></td></tr></table></figure>
<h2 id="运行结果"><a href="#运行结果" class="headerlink" title="运行结果"></a>运行结果</h2><p><img src="/2018/12/16/Neural-Network普通神经网络/1.png" alt="Neural Network"></p>
<p><strong>总结</strong>：使用普通神经网络，计算误差用的最小二乘法，也即是真实值-预测值的平方</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/12/16/Session的用法/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Victor Drq">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/header.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="drqblog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/12/16/Session的用法/" itemprop="url">Session的用法</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-12-16T16:37:54+08:00">
                2018-12-16
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/categories/TensorFlow/" itemprop="url" rel="index">
                    <span itemprop="name">TensorFlow</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">字数统计&#58;</span>
                
                <span title="字数统计">
                  109
                </span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                
                <span title="阅读时长">
                  1
                </span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="Session的用法"><a href="#Session的用法" class="headerlink" title="Session的用法"></a>Session的用法</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#author:victor</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#Session的用法讲解</span></span><br><span class="line"><span class="comment">#import module</span></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="comment">#define two constant matrix</span></span><br><span class="line">matrix1=tf.constant([[<span class="number">3</span>,<span class="number">3</span>]])</span><br><span class="line">matrix2=tf.constant([[<span class="number">2</span>],</span><br><span class="line">                     [<span class="number">2</span>]])</span><br><span class="line"><span class="comment">#using matrix multiply</span></span><br><span class="line">product=tf.matmul(matrix1,matrix2)<span class="comment">#matrix multiply 在numpy中是np.dot(matrix1,matrix2)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#method 1</span></span><br><span class="line"><span class="comment">#从session中的run中获取结果</span></span><br><span class="line">sess=tf.Session()</span><br><span class="line">result=sess.run(product)</span><br><span class="line">print(result)</span><br><span class="line">sess.close()</span><br><span class="line"></span><br><span class="line"><span class="comment">#method 2</span></span><br><span class="line"><span class="comment">#session自动close,推荐适用这个</span></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    result2=sess.run(product)</span><br><span class="line">    print(result2)</span><br></pre></td></tr></table></figure>
<h2 id="运行结果"><a href="#运行结果" class="headerlink" title="运行结果"></a>运行结果</h2><p><img src="/2018/12/16/Session的用法/1.png" alt="Session"></p>
<p><strong>总结：</strong></p>
<ul>
<li>sess=tf.InteractiveSession()</li>
<li>with sess.as_default():</li>
<li>with tf.Session() as sess:</li>
<li>sess=tf.Session()</li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/12/16/variable与constant的用法/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Victor Drq">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/header.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="drqblog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/12/16/variable与constant的用法/" itemprop="url">variable与constant的用法</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-12-16T16:26:14+08:00">
                2018-12-16
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/categories/TensorFlow/" itemprop="url" rel="index">
                    <span itemprop="name">TensorFlow</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">字数统计&#58;</span>
                
                <span title="字数统计">
                  509
                </span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                
                <span title="阅读时长">
                  2
                </span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="variable与constant"><a href="#variable与constant" class="headerlink" title="variable与constant"></a>variable与constant</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#author:victor</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#import module</span></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="comment">#常量constant</span></span><br><span class="line"><span class="comment">#tf.constant()函数定义</span></span><br><span class="line"><span class="comment">#def constant(value,dtype=None,shape=None,name='Const',verify_shape=False):</span></span><br><span class="line">    <span class="comment">#value:符合tf中定义的数据类型的常数值或者常数列表</span></span><br><span class="line">    <span class="comment">#dtype:数据类型，可选</span></span><br><span class="line">    <span class="comment">#shape：常量的形状，可选</span></span><br><span class="line">    <span class="comment">#name:常量的名字，可选</span></span><br><span class="line">    <span class="comment">#verify_shape:常量的形状是否可以被更改，默认不可更改</span></span><br><span class="line"><span class="comment">#Simple hello world using TensorFlow</span></span><br><span class="line"><span class="comment">#The op is added as a node to the default graph</span></span><br><span class="line"><span class="comment">#The value returned by the constructor represents the output of the Constant op.</span></span><br><span class="line"></span><br><span class="line">hello=tf.constant(<span class="string">"Hello,TensorFlow!"</span>)</span><br><span class="line"><span class="comment"># Constant 1-D Tensor populated with value list.</span></span><br><span class="line">tensor1 = tf.constant([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Constant 2-D tensor populated with scalar value -1.</span></span><br><span class="line">tensor2 = tf.constant(<span class="number">-1.0</span>, shape=[<span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment">#变量Variable</span></span><br><span class="line"><span class="comment">#tf.Variable()函数定义</span></span><br><span class="line"><span class="comment">#def Variable(initializer,name):</span></span><br><span class="line">    <span class="comment">#initializer:是初始化参数 </span></span><br><span class="line">    <span class="comment">#name:可自定义的变量名</span></span><br><span class="line">tensor3=tf.Variable(tf.random_normal(shape=[<span class="number">4</span>,<span class="number">3</span>],mean=<span class="number">0</span>,stddev=<span class="number">1</span>),name=<span class="string">'tensor3'</span>)</span><br><span class="line"><span class="comment">#def random_normal(shape,mean=0.0,stddev=1.0,dtype=dtypes.float32,seed=None,name=None):</span></span><br><span class="line">    <span class="comment">#shape:变量的形状，必选，shape=[4,3]，4行3列矩阵</span></span><br><span class="line">    <span class="comment">#mean:正态分布（the normal distribution）的均值E(x)，默认是0</span></span><br><span class="line">    <span class="comment">#stddev:正态分布的标准差sqrt(D(x))，默认是1.0</span></span><br><span class="line">    <span class="comment">#dtype：输出的类型，默认为tf.float32</span></span><br><span class="line">    <span class="comment">#seed:随机数种子，是一个整数，当设置后，每次运行的时候生成的随机数都一样</span></span><br><span class="line">    <span class="comment">#name:操作的名称</span></span><br><span class="line"><span class="comment">#Start tf session</span></span><br><span class="line"><span class="comment">#推荐适用with tf.Session() as sess，因为它创建完Session后可以自动关闭上下文</span></span><br><span class="line"><span class="comment">#一个Session对象封装了Operation执行对象的环境，并对Tensor对象进行计算</span></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    <span class="comment">#Run graph</span></span><br><span class="line">    print(sess.run(hello))</span><br><span class="line">    print(sess.run(tensor1))</span><br><span class="line">    print(sess.run(tensor2))</span><br><span class="line">    <span class="comment">#必须要加上这句，初始化全局变量，否则会报错Attempting to use uninitialized value tensor3</span></span><br><span class="line">    sess.run(tf.global_variables_initializer())</span><br><span class="line">    print(sess.run(tensor3))</span><br></pre></td></tr></table></figure>
<h2 id="运行结果："><a href="#运行结果：" class="headerlink" title="运行结果："></a>运行结果：</h2><p><img src="/2018/12/16/variable与constant的用法/1.png" alt="variable与constant"></p>
<h2 id="variable的用法"><a href="#variable的用法" class="headerlink" title="variable的用法"></a>variable的用法</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#author:victor</span></span><br><span class="line"><span class="comment">#Variable变量的用法</span></span><br><span class="line"><span class="comment">#import module</span></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="comment">#define the variable</span></span><br><span class="line">state=tf.Variable(<span class="number">0</span>,name=<span class="string">'counter'</span>)</span><br><span class="line"><span class="comment">#print(state.name)</span></span><br><span class="line">con=tf.constant(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">new_value=tf.add(state,con)</span><br><span class="line">update=tf.assign(state,new_value)</span><br><span class="line"></span><br><span class="line"><span class="comment">#must have if define variable,使用变量Variable必须使用</span></span><br><span class="line">init=tf.global_variables_initializer()</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(init)</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">3</span>):</span><br><span class="line">        sess.run(update)</span><br><span class="line">        print(sess.run(state))</span><br></pre></td></tr></table></figure>
<h2 id="运行结果"><a href="#运行结果" class="headerlink" title="运行结果"></a>运行结果</h2><p><img src="/2018/12/16/variable与constant的用法/2.png" alt="variable"></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/12/16/Batch-Normalization批标准化/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Victor Drq">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/header.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="drqblog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/12/16/Batch-Normalization批标准化/" itemprop="url">Batch Normalization批标准化</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-12-16T16:04:51+08:00">
                2018-12-16
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/categories/TensorFlow/" itemprop="url" rel="index">
                    <span itemprop="name">TensorFlow</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">字数统计&#58;</span>
                
                <span title="字数统计">
                  1.4k
                </span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                
                <span title="阅读时长">
                  6
                </span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="Batch-Normalization批标准化"><a href="#Batch-Normalization批标准化" class="headerlink" title="Batch Normalization批标准化"></a>Batch Normalization批标准化</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#author:victor</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#为什么要批标准化（Batch Normalization）？</span></span><br><span class="line"><span class="comment">#Why need batch normalization?</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">将分散的数据的统一标准化的方法。</span></span><br><span class="line"><span class="string">数据分布会对神经网络训练产生影响</span></span><br><span class="line"><span class="string">因为没有进行标准化，导致数据不敏感。</span></span><br><span class="line"><span class="string">是为了克服神经网络层数加深导致难以训练而诞生的一个算法。</span></span><br><span class="line"><span class="string">根据ICS理论，当训练集的样本数据和目标样本集分布不一致的时候，</span></span><br><span class="line"><span class="string">训练得到的模型无法很好的泛化</span></span><br><span class="line"><span class="string">在神经网络中，每一层的输入在经过层内操作之后必然会导致与原来对应的输入信号分布不同</span></span><br><span class="line"><span class="string">,并且前层神经网络的增加会被后面的神经网络不对的累积放大。</span></span><br><span class="line"><span class="string">这个问题的一个解决思路就是根据训练样本</span></span><br><span class="line"><span class="string">与目标样本的比例对训练样本进行一个矫正，</span></span><br><span class="line"><span class="string">而BN算法（批标准化）则可以用来规范化某些层或者所有层的输入</span></span><br><span class="line"><span class="string">从而固定每层输入信号的均值与方差</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Batch也就是把Data分成小批小批的来进行梯度下降。</span></span><br><span class="line"><span class="string">解决方法：</span></span><br><span class="line"><span class="string">显示数据X，然后经过全连接层fully connection layer，然后Batch Normalization(BN)</span></span><br><span class="line"><span class="string">添加在数据X和全连接层之间。</span></span><br><span class="line"><span class="string">然后在经过激励函数，再经过全连接层，这么下去</span></span><br><span class="line"><span class="string">BN可以加快你的机器学习，也可以很有效的训练。</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="comment">#import module</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#ACTIVATION = tf.nn.relu#activation function,所有层都使用relu</span></span><br><span class="line">ACTIVATION = tf.nn.tanh<span class="comment">#activation function,所有层都使用tanh</span></span><br><span class="line">N_LAYERS = <span class="number">7</span><span class="comment">#搭建7个hidden layer</span></span><br><span class="line">N_HIDDEN_UNITS = <span class="number">30</span><span class="comment">#每个hidden layer有30个神经元</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#重复观看的功能</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fix_seed</span><span class="params">(seed=<span class="number">1</span>)</span>:</span></span><br><span class="line">    <span class="comment"># reproducible</span></span><br><span class="line">    np.random.seed(seed)</span><br><span class="line">    tf.set_random_seed(seed)</span><br><span class="line"></span><br><span class="line"><span class="comment">#打印图</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_his</span><span class="params">(inputs, inputs_norm)</span>:</span></span><br><span class="line">    <span class="comment"># plot histogram for the inputs of every layer</span></span><br><span class="line">    <span class="keyword">for</span> j, all_inputs <span class="keyword">in</span> enumerate([inputs, inputs_norm]):</span><br><span class="line">        <span class="keyword">for</span> i, input <span class="keyword">in</span> enumerate(all_inputs):</span><br><span class="line">            plt.subplot(<span class="number">2</span>, len(all_inputs), j*len(all_inputs)+(i+<span class="number">1</span>))</span><br><span class="line">            plt.cla()</span><br><span class="line">            <span class="keyword">if</span> i == <span class="number">0</span>:</span><br><span class="line">                the_range = (<span class="number">-7</span>, <span class="number">10</span>)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                the_range = (<span class="number">-1</span>, <span class="number">1</span>)</span><br><span class="line">            plt.hist(input.ravel(), bins=<span class="number">15</span>, range=the_range, color=<span class="string">'#FF5733'</span>)</span><br><span class="line">            plt.yticks(())</span><br><span class="line">            <span class="keyword">if</span> j == <span class="number">1</span>:</span><br><span class="line">                plt.xticks(the_range)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                plt.xticks(())</span><br><span class="line">            ax = plt.gca()</span><br><span class="line">            ax.spines[<span class="string">'right'</span>].set_color(<span class="string">'none'</span>)</span><br><span class="line">            ax.spines[<span class="string">'top'</span>].set_color(<span class="string">'none'</span>)</span><br><span class="line">        plt.title(<span class="string">"%s normalizing"</span> % (<span class="string">"Without"</span> <span class="keyword">if</span> j == <span class="number">0</span> <span class="keyword">else</span> <span class="string">"With"</span>))</span><br><span class="line">    plt.draw()</span><br><span class="line">    plt.pause(<span class="number">0.01</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#搭建神经网络</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">built_net</span><span class="params">(xs, ys, norm)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">add_layer</span><span class="params">(inputs, in_size, out_size, activation_function=None, norm=False)</span>:</span></span><br><span class="line">        <span class="comment"># weights and biases (bad initialization for this case)</span></span><br><span class="line">        Weights = tf.Variable(tf.random_normal([in_size, out_size], mean=<span class="number">0.</span>, stddev=<span class="number">1.</span>))</span><br><span class="line">        biases = tf.Variable(tf.zeros([<span class="number">1</span>, out_size]) + <span class="number">0.1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># fully connected product</span></span><br><span class="line">        Wx_plus_b = tf.matmul(inputs, Weights) + biases</span><br><span class="line"></span><br><span class="line">        <span class="comment"># normalize fully connected product</span></span><br><span class="line">        <span class="keyword">if</span> norm:</span><br><span class="line">            <span class="comment"># Batch Normalize</span></span><br><span class="line">            <span class="comment">#fc_mean：整批数据的均值</span></span><br><span class="line">            <span class="comment">#fc_var：整批数据的方差</span></span><br><span class="line">            fc_mean, fc_var = tf.nn.moments(</span><br><span class="line">                Wx_plus_b,</span><br><span class="line">                axes=[<span class="number">0</span>],   <span class="comment"># the dimension you wanna normalize, here [0] for batch</span></span><br><span class="line">                            <span class="comment"># for image, you wanna do [0, 1, 2] for [batch, height, width] but not channel</span></span><br><span class="line">                            <span class="comment">#如果你是图片的话，就在0，1，2（batch, height, width）三个维度上求均值，方差</span></span><br><span class="line">            )</span><br><span class="line">            scale = tf.Variable(tf.ones([out_size]))</span><br><span class="line">            shift = tf.Variable(tf.zeros([out_size]))</span><br><span class="line">            epsilon = <span class="number">0.001</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># apply moving average for mean and var when train on batch</span></span><br><span class="line">            ema = tf.train.ExponentialMovingAverage(decay=<span class="number">0.5</span>)</span><br><span class="line">            <span class="function"><span class="keyword">def</span> <span class="title">mean_var_with_update</span><span class="params">()</span>:</span></span><br><span class="line">                ema_apply_op = ema.apply([fc_mean, fc_var])</span><br><span class="line">                <span class="keyword">with</span> tf.control_dependencies([ema_apply_op]):</span><br><span class="line">                    <span class="keyword">return</span> tf.identity(fc_mean), tf.identity(fc_var)</span><br><span class="line">            mean, var = mean_var_with_update()</span><br><span class="line"></span><br><span class="line">            </span><br><span class="line">            Wx_plus_b = tf.nn.batch_normalization(Wx_plus_b, mean, var, shift, scale, epsilon)</span><br><span class="line">            <span class="comment">#使用了tf.nn.batch_normalization方法就是和下面注释计算的本质88一样。</span></span><br><span class="line">            <span class="comment"># similar with this two steps:</span></span><br><span class="line">            <span class="comment"># Wx_plus_b = (Wx_plus_b - fc_mean) / tf.sqrt(fc_var + 0.001)</span></span><br><span class="line">            <span class="comment"># Wx_plus_b = Wx_plus_b * scale + shift</span></span><br><span class="line">            <span class="comment">#scale是扩大的参数</span></span><br><span class="line">            <span class="comment">#shift是平移的参数</span></span><br><span class="line">            </span><br><span class="line"></span><br><span class="line">        <span class="comment"># activation，也就是上面的Weights+biases计算完后放到激活函数激活</span></span><br><span class="line">        <span class="keyword">if</span> activation_function <span class="keyword">is</span> <span class="keyword">None</span>:</span><br><span class="line">            outputs = Wx_plus_b</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            outputs = activation_function(Wx_plus_b)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> outputs</span><br><span class="line"></span><br><span class="line">    fix_seed(<span class="number">1</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">#如果使用normalization，也就是加入BN层</span></span><br><span class="line">    <span class="keyword">if</span> norm:</span><br><span class="line">        <span class="comment"># BN for the first input</span></span><br><span class="line">        <span class="comment">#fc_mean：整批数据的均值</span></span><br><span class="line">        <span class="comment">#fc_var：整批数据的方差</span></span><br><span class="line">        fc_mean, fc_var = tf.nn.moments(</span><br><span class="line">            xs,</span><br><span class="line">            axes=[<span class="number">0</span>],</span><br><span class="line">        )</span><br><span class="line">        scale = tf.Variable(tf.ones([<span class="number">1</span>]))</span><br><span class="line">        shift = tf.Variable(tf.zeros([<span class="number">1</span>]))</span><br><span class="line">        epsilon = <span class="number">0.001</span></span><br><span class="line">        <span class="comment"># apply moving average for mean and var when train on batch</span></span><br><span class="line">        ema = tf.train.ExponentialMovingAverage(decay=<span class="number">0.5</span>)</span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">mean_var_with_update</span><span class="params">()</span>:</span></span><br><span class="line">            ema_apply_op = ema.apply([fc_mean, fc_var])</span><br><span class="line">            <span class="keyword">with</span> tf.control_dependencies([ema_apply_op]):</span><br><span class="line">                <span class="keyword">return</span> tf.identity(fc_mean), tf.identity(fc_var)</span><br><span class="line">        mean, var = mean_var_with_update()</span><br><span class="line">        xs = tf.nn.batch_normalization(xs, mean, var, shift, scale, epsilon)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># record inputs for every layer</span></span><br><span class="line">    layers_inputs = [xs]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># build hidden layers</span></span><br><span class="line">    <span class="keyword">for</span> l_n <span class="keyword">in</span> range(N_LAYERS):</span><br><span class="line">        layer_input = layers_inputs[l_n]</span><br><span class="line">        in_size = layers_inputs[l_n].get_shape()[<span class="number">1</span>].value</span><br><span class="line"></span><br><span class="line">        output = add_layer(</span><br><span class="line">            layer_input,    <span class="comment"># input</span></span><br><span class="line">            in_size,        <span class="comment"># input size</span></span><br><span class="line">            N_HIDDEN_UNITS, <span class="comment"># output size</span></span><br><span class="line">            ACTIVATION,     <span class="comment"># activation function</span></span><br><span class="line">            norm,           <span class="comment"># normalize before activation</span></span><br><span class="line">        )</span><br><span class="line">        layers_inputs.append(output)    <span class="comment"># add output for next run</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># build output layer</span></span><br><span class="line">    prediction = add_layer(layers_inputs[<span class="number">-1</span>], <span class="number">30</span>, <span class="number">1</span>, activation_function=<span class="keyword">None</span>)</span><br><span class="line"></span><br><span class="line">    cost = tf.reduce_mean(tf.reduce_sum(tf.square(ys - prediction), reduction_indices=[<span class="number">1</span>]))</span><br><span class="line">    train_op = tf.train.GradientDescentOptimizer(<span class="number">0.001</span>).minimize(cost)</span><br><span class="line">    <span class="keyword">return</span> [train_op, cost, layers_inputs]<span class="comment">#network的功能就是输出train_op, cost, layers_inputs</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># make up data</span></span><br><span class="line">fix_seed(<span class="number">1</span>)</span><br><span class="line">x_data = np.linspace(<span class="number">-7</span>, <span class="number">10</span>, <span class="number">2500</span>)[:, np.newaxis]</span><br><span class="line">np.random.shuffle(x_data)</span><br><span class="line">noise = np.random.normal(<span class="number">0</span>, <span class="number">8</span>, x_data.shape)</span><br><span class="line">y_data = np.square(x_data) - <span class="number">5</span> + noise</span><br><span class="line"></span><br><span class="line"><span class="comment"># plot input data</span></span><br><span class="line">plt.scatter(x_data, y_data)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line">xs = tf.placeholder(tf.float32, [<span class="keyword">None</span>, <span class="number">1</span>])  <span class="comment"># [num_samples, num_features]</span></span><br><span class="line">ys = tf.placeholder(tf.float32, [<span class="keyword">None</span>, <span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">train_op, cost, layers_inputs = built_net(xs, ys, norm=<span class="keyword">False</span>)   <span class="comment"># without BN</span></span><br><span class="line">train_op_norm, cost_norm, layers_inputs_norm = built_net(xs, ys, norm=<span class="keyword">True</span>) <span class="comment"># with BN</span></span><br><span class="line"></span><br><span class="line">sess = tf.Session()</span><br><span class="line"><span class="keyword">if</span> int((tf.__version__).split(<span class="string">'.'</span>)[<span class="number">1</span>]) &lt; <span class="number">12</span> <span class="keyword">and</span> int((tf.__version__).split(<span class="string">'.'</span>)[<span class="number">0</span>]) &lt; <span class="number">1</span>:</span><br><span class="line">    init = tf.initialize_all_variables()</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    init = tf.global_variables_initializer()</span><br><span class="line">sess.run(init)</span><br><span class="line"></span><br><span class="line"><span class="comment"># record cost</span></span><br><span class="line">cost_his = []</span><br><span class="line">cost_his_norm = []</span><br><span class="line">record_step = <span class="number">5</span></span><br><span class="line"></span><br><span class="line">plt.ion()</span><br><span class="line">plt.figure(figsize=(<span class="number">7</span>, <span class="number">3</span>))</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">250</span>):</span><br><span class="line">    <span class="keyword">if</span> i % <span class="number">50</span> == <span class="number">0</span>:</span><br><span class="line">        <span class="comment"># plot histogram</span></span><br><span class="line">        all_inputs, all_inputs_norm = sess.run([layers_inputs, layers_inputs_norm], feed_dict=&#123;xs: x_data, ys: y_data&#125;)</span><br><span class="line">        plot_his(all_inputs, all_inputs_norm)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># train on batch</span></span><br><span class="line">    sess.run([train_op, train_op_norm], feed_dict=&#123;xs: x_data[i*<span class="number">10</span>:i*<span class="number">10</span>+<span class="number">10</span>], ys: y_data[i*<span class="number">10</span>:i*<span class="number">10</span>+<span class="number">10</span>]&#125;)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> i % record_step == <span class="number">0</span>:</span><br><span class="line">        <span class="comment"># record cost</span></span><br><span class="line">        cost_his.append(sess.run(cost, feed_dict=&#123;xs: x_data, ys: y_data&#125;))</span><br><span class="line">        cost_his_norm.append(sess.run(cost_norm, feed_dict=&#123;xs: x_data, ys: y_data&#125;))</span><br><span class="line"></span><br><span class="line"><span class="comment">#matplotlib的默认显示模式为block模式。就是使用Plt.show()，程序会暂停，</span></span><br><span class="line"><span class="comment">#并不会继续执行下去，如果要展示动态图就要使用plt.ion()</span></span><br><span class="line"><span class="comment">#把block模式改为interactive交互模式</span></span><br><span class="line"><span class="comment">#plt.show()之前一定不要忘了加plt.ioff()，否则界面一闪而过，并不会停留</span></span><br><span class="line">plt.ioff()</span><br><span class="line">plt.figure()</span><br><span class="line"><span class="comment">#display no batch normalizatoin </span></span><br><span class="line">plt.plot(np.arange(len(cost_his))*record_step, np.array(cost_his), label=<span class="string">'no BN'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#display batch normalization result</span></span><br><span class="line">plt.plot(np.arange(len(cost_his))*record_step, np.array(cost_his_norm), label=<span class="string">'BN'</span>)   </span><br><span class="line"></span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<h2 id="运行结果："><a href="#运行结果：" class="headerlink" title="运行结果："></a>运行结果：</h2><p><img src="/2018/12/16/Batch-Normalization批标准化/5.gif" alt="batch normalization"></p>
<p><strong>总结：</strong>发现批标准化后的数据更集中，而不是分散与某个极端，使得训练结果更好泛化。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/12/16/使用Autoencoder自编码进行Classfication/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Victor Drq">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/header.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="drqblog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/12/16/使用Autoencoder自编码进行Classfication/" itemprop="url">使用Autoencoder自编码进行Classfication</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-12-16T15:57:47+08:00">
                2018-12-16
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/categories/TensorFlow/" itemprop="url" rel="index">
                    <span itemprop="name">TensorFlow</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">字数统计&#58;</span>
                
                <span title="字数统计">
                  625
                </span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                
                <span title="阅读时长">
                  3
                </span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="使用Autoencoder进行Classfication"><a href="#使用Autoencoder进行Classfication" class="headerlink" title="使用Autoencoder进行Classfication"></a>使用Autoencoder进行Classfication</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#author:victor</span></span><br><span class="line"><span class="comment">#use encoder_decoder classfication</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#import module</span></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment">#import MNIST data</span></span><br><span class="line"><span class="keyword">from</span> tensorflow.examples.tutorials.mnist <span class="keyword">import</span> input_data</span><br><span class="line">mnist=input_data.read_data_sets(<span class="string">'MNIST_data'</span>,one_hot=<span class="keyword">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#Visualize decoder setting</span></span><br><span class="line"><span class="comment">#Parameters</span></span><br><span class="line">learning_rate=<span class="number">0.001</span></span><br><span class="line">training_epochs=<span class="number">20</span></span><br><span class="line">batch_size=<span class="number">256</span></span><br><span class="line">display_step=<span class="number">1</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#Network Parameters</span></span><br><span class="line">n_input=<span class="number">784</span><span class="comment">#MNIST data input(img shape:28*28),也即是784个features</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#tf.Graph input(only pictures)</span></span><br><span class="line">X=tf.placeholder(<span class="string">'float'</span>,[<span class="keyword">None</span>,n_input])</span><br><span class="line"></span><br><span class="line"><span class="comment">#hidden layer settings</span></span><br><span class="line">n_hidden_1=<span class="number">128</span><span class="comment">#first num features，先经过一个隐藏层压缩成128个features</span></span><br><span class="line">n_hidden_2=<span class="number">64</span><span class="comment">#second num features，在经过一个隐藏层压缩成64个features</span></span><br><span class="line">n_hidden_3=<span class="number">10</span><span class="comment">#third num features，先经过一个隐藏层压缩成10个features</span></span><br><span class="line">n_hidden_4=<span class="number">2</span><span class="comment">#fourth num features，在经过一个隐藏层压缩成2个features</span></span><br><span class="line"><span class="comment">#define the weights</span></span><br><span class="line">weights=&#123;</span><br><span class="line">         <span class="string">'encoder_h1'</span>:tf.Variable(tf.random_normal([n_input,n_hidden_1])),</span><br><span class="line">         <span class="string">'encoder_h2'</span>:tf.Variable(tf.random_normal([n_hidden_1,n_hidden_2])),</span><br><span class="line">         <span class="string">'encoder_h3'</span>:tf.Variable(tf.random_normal([n_hidden_2,n_hidden_3])),</span><br><span class="line">         <span class="string">'encoder_h4'</span>:tf.Variable(tf.random_normal([n_hidden_3,n_hidden_4])),</span><br><span class="line"></span><br><span class="line">         <span class="string">'decoder_h1'</span>:tf.Variable(tf.random_normal([n_hidden_4,n_hidden_3])),     </span><br><span class="line">         <span class="string">'decoder_h2'</span>:tf.Variable(tf.random_normal([n_hidden_3,n_hidden_2])),</span><br><span class="line">         <span class="string">'decoder_h3'</span>:tf.Variable(tf.random_normal([n_hidden_2,n_hidden_1])),</span><br><span class="line">         <span class="string">'decoder_h4'</span>:tf.Variable(tf.random_normal([n_hidden_1,n_input])),</span><br><span class="line">         &#125;</span><br><span class="line"><span class="comment">#define the biases</span></span><br><span class="line">biases=&#123;</span><br><span class="line">        <span class="string">'encoder_b1'</span>:tf.Variable(tf.random_normal([n_hidden_1])),</span><br><span class="line">        <span class="string">'encoder_b2'</span>:tf.Variable(tf.random_normal([n_hidden_2])),</span><br><span class="line">        <span class="string">'encoder_b3'</span>:tf.Variable(tf.random_normal([n_hidden_3])),</span><br><span class="line">        <span class="string">'encoder_b4'</span>:tf.Variable(tf.random_normal([n_hidden_4])),</span><br><span class="line">        </span><br><span class="line">        <span class="string">'decoder_b1'</span>:tf.Variable(tf.random_normal([n_hidden_3])),</span><br><span class="line">        <span class="string">'decoder_b2'</span>:tf.Variable(tf.random_normal([n_hidden_2])),</span><br><span class="line">        <span class="string">'decoder_b3'</span>:tf.Variable(tf.random_normal([n_hidden_1])),</span><br><span class="line">        <span class="string">'decoder_b4'</span>:tf.Variable(tf.random_normal([n_input])),</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">#building the encoder</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">encoder</span><span class="params">(x)</span>:</span></span><br><span class="line">    layer_1=tf.nn.sigmoid(tf.add(tf.matmul(x,weights[<span class="string">'encoder_h1'</span>]),</span><br><span class="line">                           biases[<span class="string">'encoder_b1'</span>] ))</span><br><span class="line">    <span class="comment">#Decoder hidden layer with sigmoid activation function</span></span><br><span class="line">    layer_2=tf.nn.sigmoid(tf.add(tf.matmul(layer_1,weights[<span class="string">'encoder_h2'</span>]),</span><br><span class="line">                          biases[<span class="string">'encoder_b2'</span>]))</span><br><span class="line">    layer_3=tf.nn.sigmoid(tf.add(tf.matmul(layer_2,weights[<span class="string">'encoder_h3'</span>]),</span><br><span class="line">                           biases[<span class="string">'encoder_b3'</span>] ))</span><br><span class="line">    <span class="comment">#no use activation function</span></span><br><span class="line">    layer_4=tf.add(tf.matmul(layer_3,weights[<span class="string">'encoder_h4'</span>]),</span><br><span class="line">                          biases[<span class="string">'encoder_b4'</span>])</span><br><span class="line">    <span class="keyword">return</span> layer_4</span><br><span class="line">    </span><br><span class="line"><span class="comment">#building the decoder</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">decoder</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="comment">#Encoder hidden layer with sigmoid activation</span></span><br><span class="line">    layer_1=tf.nn.sigmoid(tf.add(tf.matmul(x,weights[<span class="string">'decoder_h1'</span>]),</span><br><span class="line">                           biases[<span class="string">'decoder_b1'</span>] ))</span><br><span class="line">    <span class="comment">#Decoder hidden layer with sigmoid activation function</span></span><br><span class="line">    layer_2=tf.nn.sigmoid(tf.add(tf.matmul(layer_1,weights[<span class="string">'decoder_h2'</span>]),</span><br><span class="line">                          biases[<span class="string">'decoder_b2'</span>]))</span><br><span class="line">     <span class="comment">#Encoder hidden layer with sigmoid activation</span></span><br><span class="line">    layer_3=tf.nn.sigmoid(tf.add(tf.matmul(layer_2,weights[<span class="string">'decoder_h3'</span>]),</span><br><span class="line">                           biases[<span class="string">'decoder_b3'</span>] ))</span><br><span class="line">    <span class="comment">#Decoder hidden layer with sigmoid activation function</span></span><br><span class="line">    layer_4=tf.nn.sigmoid(tf.add(tf.matmul(layer_3,weights[<span class="string">'decoder_h4'</span>]),</span><br><span class="line">                          biases[<span class="string">'decoder_b4'</span>]))</span><br><span class="line">    <span class="keyword">return</span> layer_4</span><br><span class="line">    </span><br><span class="line"><span class="comment">#Construct model</span></span><br><span class="line">encoder_op=encoder(X)</span><br><span class="line">decoder_op=decoder(encoder_op)    </span><br><span class="line"></span><br><span class="line"><span class="comment">#Prediction</span></span><br><span class="line">y_pred=decoder_op</span><br><span class="line"><span class="comment">#Targets(Labels) are the input data</span></span><br><span class="line">y_true=X</span><br><span class="line"></span><br><span class="line"><span class="comment">#Define loss and optimizer,minimize the squre error</span></span><br><span class="line">cost=tf.reduce_mean(tf.pow(y_true-y_pred,<span class="number">2</span>))</span><br><span class="line">optimizer=tf.train.AdamOptimizer(learning_rate).minimize(cost)    </span><br><span class="line"></span><br><span class="line"><span class="comment">#Initializing the variables</span></span><br><span class="line">init=tf.initialize_all_variables()</span><br><span class="line"></span><br><span class="line"><span class="comment">#Launch the graph</span></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(init)</span><br><span class="line">    total_batch=int(mnist.train.num_examples/batch_size)</span><br><span class="line">    <span class="comment">#Train cycle</span></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(training_epochs):</span><br><span class="line">        <span class="comment">#Loop overall batches</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(total_batch):</span><br><span class="line">            batch_xs,batch_ys=mnist.train.next_batch(batch_size)<span class="comment">#max(x)=1,min(x)=0,batch_xs已经被normalize正规化过了，最大值是1</span></span><br><span class="line">            <span class="comment">#Run optimization op (backprop) and cost op (to get loss value)</span></span><br><span class="line">            _,c=sess.run([optimizer,cost],feed_dict=&#123;X:batch_xs&#125;)</span><br><span class="line">            <span class="comment">#Display logs per epoch step</span></span><br><span class="line">            <span class="keyword">if</span> epoch% display_step==<span class="number">0</span>:</span><br><span class="line">                print(<span class="string">"Epoch"</span>,<span class="string">'%04d'</span>%(epoch+<span class="number">1</span>),</span><br><span class="line">                      <span class="string">"cost="</span>,<span class="string">"&#123;:9f&#125;"</span>.format(c))</span><br><span class="line">                </span><br><span class="line">    print(<span class="string">"Optimization Finished!"</span>)</span><br><span class="line">            </span><br><span class="line">    encoder_result=sess.run(encoder_op,feed_dict=&#123;X:mnist.test.images&#125;)</span><br><span class="line">    plt.scatter(encoder_result[:,<span class="number">0</span>],encoder_result[:,<span class="number">1</span>],c=mnist.test.labels)</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure>
<h2 id="运行结果："><a href="#运行结果：" class="headerlink" title="运行结果："></a>运行结果：</h2><p><img src="/2018/12/16/使用Autoencoder自编码进行Classfication/3.gif" alt="classfication"></p>
<p><strong>总结</strong>：等运行结束后，以散点图scatter的形式展现出来，不同颜色表示MNIST data里的不同的数字lable，发现Classfication的效果还是不错</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/12/16/Autoencoder自编码/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Victor Drq">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/header.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="drqblog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/12/16/Autoencoder自编码/" itemprop="url">Autoencoder自编码</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-12-16T15:51:25+08:00">
                2018-12-16
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/categories/TensorFlow/" itemprop="url" rel="index">
                    <span itemprop="name">TensorFlow</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">字数统计&#58;</span>
                
                <span title="字数统计">
                  804
                </span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                
                <span title="阅读时长">
                  3
                </span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="Autoencoder自编码"><a href="#Autoencoder自编码" class="headerlink" title="Autoencoder自编码"></a>Autoencoder自编码</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#author:victor</span></span><br><span class="line"><span class="comment">#什么是自编码（Autoencoder）</span></span><br><span class="line"><span class="comment">#What is an Autoencoder</span></span><br><span class="line"><span class="comment">#神经网络的非监督学习</span></span><br><span class="line"><span class="comment">#因为有时候训练的样本数据很大，直接训练会很耗时的，所以把数据的feature压缩一下，然后再解压一下</span></span><br><span class="line"><span class="comment">#Autoencoder是一种数据的压缩算法，其中数据的压缩和解压函数</span></span><br><span class="line"><span class="comment">#数据相关的，有损的，从样本中自动学习的，压缩和解压缩的函数是通过神经网络实现的</span></span><br><span class="line"><span class="comment">#因为自编码不用到训练样本的分类标签，所以是非监督学习的</span></span><br><span class="line"><span class="comment">#比如PCA（principal Component Analysis）：主成分析方法。一种使用最广发的数据压缩算法。</span></span><br><span class="line"><span class="comment">#PCA一种常用的数据降维方法。通过线性变换将原始数据变换成为一组各维度线性无关的表示来提取数据的主要线性分量</span></span><br><span class="line"><span class="comment">#比如分类学习，也是非监督学习的</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#import module</span></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment">#import MNIST data</span></span><br><span class="line"><span class="keyword">from</span> tensorflow.examples.tutorials.mnist <span class="keyword">import</span> input_data</span><br><span class="line">mnist=input_data.read_data_sets(<span class="string">'MNIST_data'</span>,one_hot=<span class="keyword">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#Visualize decoder setting</span></span><br><span class="line"><span class="comment">#Parameters</span></span><br><span class="line">learning_rate=<span class="number">0.01</span></span><br><span class="line">training_epochs=<span class="number">20</span></span><br><span class="line">batch_size=<span class="number">256</span></span><br><span class="line">display_step=<span class="number">1</span></span><br><span class="line">examples_to_show=<span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#Network Parameters</span></span><br><span class="line">n_input=<span class="number">784</span><span class="comment">#MNIST data input(img shape:28*28),也即是784个features</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#tf.Graph input(only pictures)</span></span><br><span class="line">X=tf.placeholder(<span class="string">'float'</span>,[<span class="keyword">None</span>,n_input])</span><br><span class="line"></span><br><span class="line"><span class="comment">#hidden layer settings</span></span><br><span class="line">n_hidden_1=<span class="number">256</span><span class="comment">#first num features(2^8)，先经过一个隐藏层压缩成256个features</span></span><br><span class="line">n_hidden_2=<span class="number">128</span><span class="comment">#second num features(2^7)，在经过一个隐藏层压缩成128个features</span></span><br><span class="line"><span class="comment">#define the weights</span></span><br><span class="line">weights=&#123;</span><br><span class="line">         <span class="string">'encoder_h1'</span>:tf.Variable(tf.random_normal([n_input,n_hidden_1])),</span><br><span class="line">         <span class="string">'encoder_h2'</span>:tf.Variable(tf.random_normal([n_hidden_1,n_hidden_2])),</span><br><span class="line">          <span class="comment">#经过一个隐藏层解压缩把128个features解压成256个features</span></span><br><span class="line">         <span class="string">'decoder_h1'</span>:tf.Variable(tf.random_normal([n_hidden_2,n_hidden_1])),</span><br><span class="line">         <span class="comment">#经过一个隐藏层解压缩把256个features解压成原来784个features</span></span><br><span class="line">         <span class="string">'decoder_h2'</span>:tf.Variable(tf.random_normal([n_hidden_1,n_input])),</span><br><span class="line">         &#125;</span><br><span class="line"><span class="comment">#define the biases</span></span><br><span class="line">biases=&#123;</span><br><span class="line">        <span class="string">'encoder_b1'</span>:tf.Variable(tf.random_normal([n_hidden_1])),</span><br><span class="line">        <span class="string">'encoder_b2'</span>:tf.Variable(tf.random_normal([n_hidden_2])),</span><br><span class="line">        <span class="string">'decoder_b1'</span>:tf.Variable(tf.random_normal([n_hidden_1])),</span><br><span class="line">        <span class="string">'decoder_b2'</span>:tf.Variable(tf.random_normal([n_input])),</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">#building the encoder</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">encoder</span><span class="params">(x)</span>:</span></span><br><span class="line">    layer_1=tf.nn.sigmoid(tf.add(tf.matmul(x,weights[<span class="string">'encoder_h1'</span>]),</span><br><span class="line">                           biases[<span class="string">'encoder_b1'</span>] ))</span><br><span class="line">    <span class="comment">#Decoder hidden layer with sigmoid activation function</span></span><br><span class="line">    layer_2=tf.nn.sigmoid(tf.add(tf.matmul(layer_1,weights[<span class="string">'encoder_h2'</span>]),</span><br><span class="line">                          biases[<span class="string">'encoder_b2'</span>]))</span><br><span class="line">    <span class="keyword">return</span> layer_2</span><br><span class="line">    </span><br><span class="line"><span class="comment">#building the decoder</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">decoder</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="comment">#Encoder hidden layer with sigmoid activation</span></span><br><span class="line">    layer_1=tf.nn.sigmoid(tf.add(tf.matmul(x,weights[<span class="string">'decoder_h1'</span>]),</span><br><span class="line">                           biases[<span class="string">'decoder_b1'</span>] ))</span><br><span class="line">    <span class="comment">#Decoder hidden layer with sigmoid activation function</span></span><br><span class="line">    layer_2=tf.nn.sigmoid(tf.add(tf.matmul(layer_1,weights[<span class="string">'decoder_h2'</span>]),</span><br><span class="line">                          biases[<span class="string">'decoder_b2'</span>]))</span><br><span class="line">    <span class="keyword">return</span> layer_2</span><br><span class="line">    </span><br><span class="line"><span class="comment">#Construct model</span></span><br><span class="line">encoder_op=encoder(X)</span><br><span class="line">decoder_op=decoder(encoder_op)    </span><br><span class="line"></span><br><span class="line"><span class="comment">#Prediction</span></span><br><span class="line">y_pred=decoder_op</span><br><span class="line"><span class="comment">#Targets(Labels) are the input data</span></span><br><span class="line">y_true=X</span><br><span class="line"></span><br><span class="line"><span class="comment">#Define loss and optimizer,minimize the squre error</span></span><br><span class="line">cost=tf.reduce_mean(tf.pow(y_true-y_pred,<span class="number">2</span>))</span><br><span class="line">optimizer=tf.train.AdamOptimizer(learning_rate).minimize(cost)    </span><br><span class="line"></span><br><span class="line"><span class="comment">#Initializing the variables</span></span><br><span class="line">init=tf.initialize_all_variables()</span><br><span class="line"></span><br><span class="line"><span class="comment">#Launch the graph</span></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(init)</span><br><span class="line">    total_batch=int(mnist.train.num_examples/batch_size)</span><br><span class="line">    <span class="comment">#Train cycle</span></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(training_epochs):</span><br><span class="line">        <span class="comment">#Loop overall batches</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(total_batch):</span><br><span class="line">            batch_xs,batch_ys=mnist.train.next_batch(batch_size)<span class="comment">#max(x)=1,min(x)=0,batch_xs已经被normalize正规化过了，最大值是1</span></span><br><span class="line">            <span class="comment">#Run optimization op (backprop) and cost op (to get loss value)</span></span><br><span class="line">            _,c=sess.run([optimizer,cost],feed_dict=&#123;X:batch_xs&#125;)</span><br><span class="line">            <span class="comment">#Display logs per epoch step</span></span><br><span class="line">            <span class="keyword">if</span> epoch% display_step==<span class="number">0</span>:</span><br><span class="line">                print(<span class="string">"Epoch"</span>,<span class="string">'%04d'</span>%(epoch+<span class="number">1</span>),</span><br><span class="line">                      <span class="string">"cost="</span>,<span class="string">"&#123;:9f&#125;"</span>.format(c))</span><br><span class="line">                </span><br><span class="line">    print(<span class="string">"Optimization Finished!"</span>)</span><br><span class="line">            </span><br><span class="line">    <span class="comment">#Applying encode and decode over test set</span></span><br><span class="line">    encode_decode=sess.run(</span><br><span class="line">            y_pred,feed_dict=&#123;X:mnist.test.images[:examples_to_show]&#125;)</span><br><span class="line">    <span class="comment">#Compare original images with their reconstructions</span></span><br><span class="line">    f,a=plt.subplots(<span class="number">2</span>,<span class="number">10</span>,figsize=(<span class="number">10</span>,<span class="number">2</span>))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(examples_to_show):</span><br><span class="line">        <span class="comment">#real data</span></span><br><span class="line">        a[<span class="number">0</span>][i].imshow(np.reshape(mnist.test.images[i],(<span class="number">28</span>,<span class="number">28</span>)))</span><br><span class="line">        <span class="comment">#predict data</span></span><br><span class="line">        a[<span class="number">1</span>][i].imshow(np.reshape(encode_decode[i],(<span class="number">28</span>,<span class="number">28</span>)))</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure>
<h2 id="运行结果："><a href="#运行结果：" class="headerlink" title="运行结果："></a>运行结果：</h2><p><img src="/2018/12/16/Autoencoder自编码/2.gif" alt="autoencoder"></p>
<p><strong>总结</strong>：发现经过压缩过后的MNIST data，在训练的时候明显速度加快了。说明在进行大量数据训练的时候，使用自编码进行encoder-decoder不失为一个好办法。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/12/16/rnn-use-variable/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Victor Drq">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/header.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="drqblog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/12/16/rnn-use-variable/" itemprop="url">rnn_use_variable</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-12-16T15:28:21+08:00">
                2018-12-16
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/categories/TensorFlow/" itemprop="url" rel="index">
                    <span itemprop="name">TensorFlow</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">字数统计&#58;</span>
                
                <span title="字数统计">
                  499
                </span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                
                <span title="阅读时长">
                  3
                </span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="在RNN中use-variable"><a href="#在RNN中use-variable" class="headerlink" title="在RNN中use variable"></a>在RNN中use variable</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#import module</span></span><br><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="comment">#define class</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TrainConfig</span>:</span></span><br><span class="line">    batch_size = <span class="number">20</span></span><br><span class="line">    time_steps = <span class="number">20</span></span><br><span class="line">    input_size = <span class="number">10</span></span><br><span class="line">    output_size = <span class="number">2</span></span><br><span class="line">    cell_size = <span class="number">11</span></span><br><span class="line">    learning_rate = <span class="number">0.01</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TestConfig</span><span class="params">(TrainConfig)</span>:</span></span><br><span class="line">    time_steps = <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#define RNN class</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RNN</span><span class="params">(object)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">#define the init method</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, config)</span>:</span></span><br><span class="line">        self._batch_size = config.batch_size</span><br><span class="line">        self._time_steps = config.time_steps</span><br><span class="line">        self._input_size = config.input_size</span><br><span class="line">        self._output_size = config.output_size</span><br><span class="line">        self._cell_size = config.cell_size</span><br><span class="line">        self._lr = config.learning_rate</span><br><span class="line">        self._built_RNN()</span><br><span class="line"></span><br><span class="line">    <span class="comment">#build the rnn network    </span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_built_RNN</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">with</span> tf.variable_scope(<span class="string">'inputs'</span>):</span><br><span class="line">            self._xs = tf.placeholder(tf.float32, [self._batch_size, self._time_steps, self._input_size], name=<span class="string">'xs'</span>)</span><br><span class="line">            self._ys = tf.placeholder(tf.float32, [self._batch_size, self._time_steps, self._output_size], name=<span class="string">'ys'</span>)</span><br><span class="line">        <span class="keyword">with</span> tf.name_scope(<span class="string">'RNN'</span>):</span><br><span class="line">            <span class="keyword">with</span> tf.variable_scope(<span class="string">'input_layer'</span>):</span><br><span class="line">                l_in_x = tf.reshape(self._xs, [<span class="number">-1</span>, self._input_size], name=<span class="string">'2_2D'</span>)  <span class="comment"># (batch*n_step, in_size)</span></span><br><span class="line">                <span class="comment"># Ws (in_size, cell_size)</span></span><br><span class="line">                Wi = self._weight_variable([self._input_size, self._cell_size])</span><br><span class="line">                print(Wi.name)</span><br><span class="line">                <span class="comment"># bs (cell_size, )</span></span><br><span class="line">                bi = self._bias_variable([self._cell_size, ])</span><br><span class="line">                <span class="comment"># l_in_y = (batch * n_steps, cell_size)</span></span><br><span class="line">                <span class="keyword">with</span> tf.name_scope(<span class="string">'Wx_plus_b'</span>):</span><br><span class="line">                    l_in_y = tf.matmul(l_in_x, Wi) + bi</span><br><span class="line">                l_in_y = tf.reshape(l_in_y, [<span class="number">-1</span>, self._time_steps, self._cell_size], name=<span class="string">'2_3D'</span>)</span><br><span class="line"></span><br><span class="line">            <span class="keyword">with</span> tf.variable_scope(<span class="string">'cell'</span>):</span><br><span class="line">                cell = tf.contrib.rnn.BasicLSTMCell(self._cell_size)</span><br><span class="line">                <span class="keyword">with</span> tf.name_scope(<span class="string">'initial_state'</span>):</span><br><span class="line">                    self._cell_initial_state = cell.zero_state(self._batch_size, dtype=tf.float32)</span><br><span class="line"></span><br><span class="line">                self.cell_outputs = []</span><br><span class="line">                cell_state = self._cell_initial_state</span><br><span class="line">                <span class="keyword">for</span> t <span class="keyword">in</span> range(self._time_steps):</span><br><span class="line">                    <span class="keyword">if</span> t &gt; <span class="number">0</span>: tf.get_variable_scope().reuse_variables()</span><br><span class="line">                    cell_output, cell_state = cell(l_in_y[:, t, :], cell_state)</span><br><span class="line">                    self.cell_outputs.append(cell_output)</span><br><span class="line">                self._cell_final_state = cell_state</span><br><span class="line"></span><br><span class="line">            <span class="keyword">with</span> tf.variable_scope(<span class="string">'output_layer'</span>):</span><br><span class="line">                <span class="comment"># cell_outputs_reshaped (BATCH*TIME_STEP, CELL_SIZE)</span></span><br><span class="line">                cell_outputs_reshaped = tf.reshape(tf.concat(self.cell_outputs, <span class="number">1</span>), [<span class="number">-1</span>, self._cell_size])</span><br><span class="line">                Wo = self._weight_variable((self._cell_size, self._output_size))</span><br><span class="line">                bo = self._bias_variable((self._output_size,))</span><br><span class="line">                product = tf.matmul(cell_outputs_reshaped, Wo) + bo</span><br><span class="line">                <span class="comment"># _pred shape (batch*time_step, output_size)</span></span><br><span class="line">                self._pred = tf.nn.relu(product)    <span class="comment"># for displacement</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">with</span> tf.name_scope(<span class="string">'cost'</span>):</span><br><span class="line">            _pred = tf.reshape(self._pred, [self._batch_size, self._time_steps, self._output_size])</span><br><span class="line">            mse = self.ms_error(_pred, self._ys)</span><br><span class="line">            mse_ave_across_batch = tf.reduce_mean(mse, <span class="number">0</span>)</span><br><span class="line">            mse_sum_across_time = tf.reduce_sum(mse_ave_across_batch, <span class="number">0</span>)</span><br><span class="line">            self._cost = mse_sum_across_time</span><br><span class="line">            self._cost_ave_time = self._cost / self._time_steps</span><br><span class="line"></span><br><span class="line">        <span class="keyword">with</span> tf.variable_scope(<span class="string">'trian'</span>):</span><br><span class="line">            self._lr = tf.convert_to_tensor(self._lr)</span><br><span class="line">            self.train_op = tf.train.AdamOptimizer(self._lr).minimize(self._cost)</span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">ms_error</span><span class="params">(y_target, y_pre)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> tf.square(tf.subtract(y_target, y_pre))</span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_weight_variable</span><span class="params">(shape, name=<span class="string">'weights'</span>)</span>:</span></span><br><span class="line">        initializer = tf.random_normal_initializer(mean=<span class="number">0.</span>, stddev=<span class="number">0.5</span>, )</span><br><span class="line">        <span class="keyword">return</span> tf.get_variable(shape=shape, initializer=initializer, name=name)</span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_bias_variable</span><span class="params">(shape, name=<span class="string">'biases'</span>)</span>:</span></span><br><span class="line">        initializer = tf.constant_initializer(<span class="number">0.1</span>)</span><br><span class="line">        <span class="keyword">return</span> tf.get_variable(name=name, shape=shape, initializer=initializer)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    train_config = TrainConfig()</span><br><span class="line">    test_config = TestConfig()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># the wrong method to reuse parameters in train rnn</span></span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">'train_rnn'</span>):</span><br><span class="line">        train_rnn1 = RNN(train_config)</span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">'test_rnn'</span>):</span><br><span class="line">        test_rnn1 = RNN(test_config)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># the right method to reuse parameters in train rnn</span></span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">'rnn'</span>) <span class="keyword">as</span> scope:</span><br><span class="line">        sess = tf.Session()</span><br><span class="line">        train_rnn2 = RNN(train_config)</span><br><span class="line">        scope.reuse_variables()</span><br><span class="line">        test_rnn2 = RNN(test_config)</span><br><span class="line">        <span class="comment"># tf.initialize_all_variables() no long valid from</span></span><br><span class="line">        <span class="comment"># 2017-03-02 if using tensorflow &gt;= 0.12</span></span><br><span class="line">        <span class="keyword">if</span> int((tf.__version__).split(<span class="string">'.'</span>)[<span class="number">1</span>]) &lt; <span class="number">12</span> <span class="keyword">and</span> int((tf.__version__).split(<span class="string">'.'</span>)[<span class="number">0</span>]) &lt; <span class="number">1</span>:</span><br><span class="line">            init = tf.initialize_all_variables()</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            init = tf.global_variables_initializer()</span><br><span class="line">        sess.run(init)</span><br></pre></td></tr></table></figure>
<h2 id="使用variable-scope的效果："><a href="#使用variable-scope的效果：" class="headerlink" title="使用variable_scope的效果："></a>使用variable_scope的效果：</h2><p><img src="/2018/12/16/rnn-use-variable/1.png" alt="rnn"></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><span class="space">&hellip;</span><a class="page-number" href="/page/7/">7</a><a class="extend next" rel="next" href="/page/2/"><i class="fa fa-angle-right"></i></a>
  </nav>



          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope="" itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image" src="/images/header.jpg" alt="Victor Drq">
            
              <p class="site-author-name" itemprop="name">Victor Drq</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">68</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">8</span>
                  <span class="site-state-item-name">分类</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">1</span>
                  <span class="site-state-item-name">标签</span>
                </a>
              </div>
            

          </nav>

          
            <div class="feed-link motion-element">
              <a href="/atom.xml" rel="alternate">
                <i class="fa fa-rss"></i>
                RSS
              </a>
            </div>
          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="https://github.com/AnonymousDQ" target="_blank" title="GitHub">
                      
                        <i class="fa fa-fw fa-github"></i>GitHub</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="mailto:1397743321@qq.com" target="_blank" title="E-Mail">
                      
                        <i class="fa fa-fw fa-envelope"></i>E-Mail</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="https://www.cnblogs.com/drq1/" target="_blank" title="博客园">
                      
                        <i class="fa fa-fw fa-globe"></i>博客园</a>
                  </span>
                
            </div>
          

          
          

          
          

          

        </div>
      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>
<div class="copyright">&copy; <span itemprop="copyrightYear">2018</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Victor Drq</span>

  
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-area-chart"></i>
    </span>
    
      <span class="post-meta-item-text">Site words total count&#58;</span>
    
    <span title="Site words total count">60.3k</span>
  
</div>
<div class="powered-by">
<i class="fa fa-user-md"></i><span id="busuanzi_container_site_uv">
  本站访客数:<span id="busuanzi_value_site_uv"></span>
</span>
</div>

  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Pisces</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  

  
  
  
  <link rel="stylesheet" href="/lib/needsharebutton/needsharebutton.css">

  
  
  <script src="/lib/needsharebutton/needsharebutton.js"></script>

  <script>
    
      pbOptions = {};
      
          pbOptions.iconStyle = "box";
      
          pbOptions.boxForm = "horizontal";
      
          pbOptions.position = "bottomCenter";
      
          pbOptions.networks = "Weibo,Wechat,Douban,QQZone,Twitter,Facebook";
      
      new needShareButton('#needsharebutton-postbottom', pbOptions);
    
    
  </script>

  

  

  

  

<script src="/live2dw/lib/L2Dwidget.min.js?0c58a1486de42ac6cc1c59c7d98ae887"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>
</html>
