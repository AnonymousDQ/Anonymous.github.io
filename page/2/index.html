<!DOCTYPE html>



  


<html class="theme-next pisces use-motion" lang="zh-Hans">
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT">





  <link rel="alternate" href="/atom.xml" title="drqblog" type="application/atom+xml">






<meta property="og:type" content="website">
<meta property="og:title" content="drqblog">
<meta property="og:url" content="http://yoursite.com/page/2/index.html">
<meta property="og:site_name" content="drqblog">
<meta property="og:locale" content="zh-Hans">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="drqblog">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '5.1.4',
    sidebar: {"position":"right","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/page/2/">





  <title>drqblog</title>
  








</head>

<body itemscope="" itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-right 
  page-home">
    <div class="headband"></div>
	<a href="https://github.com/AnonymousDQ">
	<img style="position: absolute; top: 0; right: 0; border: 0;" src="https://s3.amazonaws.com/github/ribbons/forkme_right_green_007200.png" alt="Fork me on GitHub">
	</a>

    <header id="header" class="header" itemscope="" itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">drqblog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/index" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br>
            
            关于
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br>
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br>
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-schedule">
          <a href="/schedule/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-calendar"></i> <br>
            
            日程表
          </a>
        </li>
      
        
        <li class="menu-item menu-item-sitemap">
          <a href="/sitemap.xml" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-sitemap"></i> <br>
            
            站点地图
          </a>
        </li>
      
        
        <li class="menu-item menu-item-commonweal">
          <a href="/404/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-heartbeat"></i> <br>
            
            公益404
          </a>
        </li>
      
        
        <li class="menu-item menu-item-something">
          <a href="/something" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-question-circle"></i> <br>
            
            有料
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/12/16/Autoencoder自编码/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Victor Drq">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/header.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="drqblog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/12/16/Autoencoder自编码/" itemprop="url">Autoencoder自编码</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-12-16T15:51:25+08:00">
                2018-12-16
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/categories/TensorFlow/" itemprop="url" rel="index">
                    <span itemprop="name">TensorFlow</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">字数统计&#58;</span>
                
                <span title="字数统计">
                  804
                </span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                
                <span title="阅读时长">
                  3
                </span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="Autoencoder自编码"><a href="#Autoencoder自编码" class="headerlink" title="Autoencoder自编码"></a>Autoencoder自编码</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#author:victor</span></span><br><span class="line"><span class="comment">#什么是自编码（Autoencoder）</span></span><br><span class="line"><span class="comment">#What is an Autoencoder</span></span><br><span class="line"><span class="comment">#神经网络的非监督学习</span></span><br><span class="line"><span class="comment">#因为有时候训练的样本数据很大，直接训练会很耗时的，所以把数据的feature压缩一下，然后再解压一下</span></span><br><span class="line"><span class="comment">#Autoencoder是一种数据的压缩算法，其中数据的压缩和解压函数</span></span><br><span class="line"><span class="comment">#数据相关的，有损的，从样本中自动学习的，压缩和解压缩的函数是通过神经网络实现的</span></span><br><span class="line"><span class="comment">#因为自编码不用到训练样本的分类标签，所以是非监督学习的</span></span><br><span class="line"><span class="comment">#比如PCA（principal Component Analysis）：主成分析方法。一种使用最广发的数据压缩算法。</span></span><br><span class="line"><span class="comment">#PCA一种常用的数据降维方法。通过线性变换将原始数据变换成为一组各维度线性无关的表示来提取数据的主要线性分量</span></span><br><span class="line"><span class="comment">#比如分类学习，也是非监督学习的</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#import module</span></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment">#import MNIST data</span></span><br><span class="line"><span class="keyword">from</span> tensorflow.examples.tutorials.mnist <span class="keyword">import</span> input_data</span><br><span class="line">mnist=input_data.read_data_sets(<span class="string">'MNIST_data'</span>,one_hot=<span class="keyword">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#Visualize decoder setting</span></span><br><span class="line"><span class="comment">#Parameters</span></span><br><span class="line">learning_rate=<span class="number">0.01</span></span><br><span class="line">training_epochs=<span class="number">20</span></span><br><span class="line">batch_size=<span class="number">256</span></span><br><span class="line">display_step=<span class="number">1</span></span><br><span class="line">examples_to_show=<span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#Network Parameters</span></span><br><span class="line">n_input=<span class="number">784</span><span class="comment">#MNIST data input(img shape:28*28),也即是784个features</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#tf.Graph input(only pictures)</span></span><br><span class="line">X=tf.placeholder(<span class="string">'float'</span>,[<span class="keyword">None</span>,n_input])</span><br><span class="line"></span><br><span class="line"><span class="comment">#hidden layer settings</span></span><br><span class="line">n_hidden_1=<span class="number">256</span><span class="comment">#first num features(2^8)，先经过一个隐藏层压缩成256个features</span></span><br><span class="line">n_hidden_2=<span class="number">128</span><span class="comment">#second num features(2^7)，在经过一个隐藏层压缩成128个features</span></span><br><span class="line"><span class="comment">#define the weights</span></span><br><span class="line">weights=&#123;</span><br><span class="line">         <span class="string">'encoder_h1'</span>:tf.Variable(tf.random_normal([n_input,n_hidden_1])),</span><br><span class="line">         <span class="string">'encoder_h2'</span>:tf.Variable(tf.random_normal([n_hidden_1,n_hidden_2])),</span><br><span class="line">          <span class="comment">#经过一个隐藏层解压缩把128个features解压成256个features</span></span><br><span class="line">         <span class="string">'decoder_h1'</span>:tf.Variable(tf.random_normal([n_hidden_2,n_hidden_1])),</span><br><span class="line">         <span class="comment">#经过一个隐藏层解压缩把256个features解压成原来784个features</span></span><br><span class="line">         <span class="string">'decoder_h2'</span>:tf.Variable(tf.random_normal([n_hidden_1,n_input])),</span><br><span class="line">         &#125;</span><br><span class="line"><span class="comment">#define the biases</span></span><br><span class="line">biases=&#123;</span><br><span class="line">        <span class="string">'encoder_b1'</span>:tf.Variable(tf.random_normal([n_hidden_1])),</span><br><span class="line">        <span class="string">'encoder_b2'</span>:tf.Variable(tf.random_normal([n_hidden_2])),</span><br><span class="line">        <span class="string">'decoder_b1'</span>:tf.Variable(tf.random_normal([n_hidden_1])),</span><br><span class="line">        <span class="string">'decoder_b2'</span>:tf.Variable(tf.random_normal([n_input])),</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">#building the encoder</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">encoder</span><span class="params">(x)</span>:</span></span><br><span class="line">    layer_1=tf.nn.sigmoid(tf.add(tf.matmul(x,weights[<span class="string">'encoder_h1'</span>]),</span><br><span class="line">                           biases[<span class="string">'encoder_b1'</span>] ))</span><br><span class="line">    <span class="comment">#Decoder hidden layer with sigmoid activation function</span></span><br><span class="line">    layer_2=tf.nn.sigmoid(tf.add(tf.matmul(layer_1,weights[<span class="string">'encoder_h2'</span>]),</span><br><span class="line">                          biases[<span class="string">'encoder_b2'</span>]))</span><br><span class="line">    <span class="keyword">return</span> layer_2</span><br><span class="line">    </span><br><span class="line"><span class="comment">#building the decoder</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">decoder</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="comment">#Encoder hidden layer with sigmoid activation</span></span><br><span class="line">    layer_1=tf.nn.sigmoid(tf.add(tf.matmul(x,weights[<span class="string">'decoder_h1'</span>]),</span><br><span class="line">                           biases[<span class="string">'decoder_b1'</span>] ))</span><br><span class="line">    <span class="comment">#Decoder hidden layer with sigmoid activation function</span></span><br><span class="line">    layer_2=tf.nn.sigmoid(tf.add(tf.matmul(layer_1,weights[<span class="string">'decoder_h2'</span>]),</span><br><span class="line">                          biases[<span class="string">'decoder_b2'</span>]))</span><br><span class="line">    <span class="keyword">return</span> layer_2</span><br><span class="line">    </span><br><span class="line"><span class="comment">#Construct model</span></span><br><span class="line">encoder_op=encoder(X)</span><br><span class="line">decoder_op=decoder(encoder_op)    </span><br><span class="line"></span><br><span class="line"><span class="comment">#Prediction</span></span><br><span class="line">y_pred=decoder_op</span><br><span class="line"><span class="comment">#Targets(Labels) are the input data</span></span><br><span class="line">y_true=X</span><br><span class="line"></span><br><span class="line"><span class="comment">#Define loss and optimizer,minimize the squre error</span></span><br><span class="line">cost=tf.reduce_mean(tf.pow(y_true-y_pred,<span class="number">2</span>))</span><br><span class="line">optimizer=tf.train.AdamOptimizer(learning_rate).minimize(cost)    </span><br><span class="line"></span><br><span class="line"><span class="comment">#Initializing the variables</span></span><br><span class="line">init=tf.initialize_all_variables()</span><br><span class="line"></span><br><span class="line"><span class="comment">#Launch the graph</span></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(init)</span><br><span class="line">    total_batch=int(mnist.train.num_examples/batch_size)</span><br><span class="line">    <span class="comment">#Train cycle</span></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(training_epochs):</span><br><span class="line">        <span class="comment">#Loop overall batches</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(total_batch):</span><br><span class="line">            batch_xs,batch_ys=mnist.train.next_batch(batch_size)<span class="comment">#max(x)=1,min(x)=0,batch_xs已经被normalize正规化过了，最大值是1</span></span><br><span class="line">            <span class="comment">#Run optimization op (backprop) and cost op (to get loss value)</span></span><br><span class="line">            _,c=sess.run([optimizer,cost],feed_dict=&#123;X:batch_xs&#125;)</span><br><span class="line">            <span class="comment">#Display logs per epoch step</span></span><br><span class="line">            <span class="keyword">if</span> epoch% display_step==<span class="number">0</span>:</span><br><span class="line">                print(<span class="string">"Epoch"</span>,<span class="string">'%04d'</span>%(epoch+<span class="number">1</span>),</span><br><span class="line">                      <span class="string">"cost="</span>,<span class="string">"&#123;:9f&#125;"</span>.format(c))</span><br><span class="line">                </span><br><span class="line">    print(<span class="string">"Optimization Finished!"</span>)</span><br><span class="line">            </span><br><span class="line">    <span class="comment">#Applying encode and decode over test set</span></span><br><span class="line">    encode_decode=sess.run(</span><br><span class="line">            y_pred,feed_dict=&#123;X:mnist.test.images[:examples_to_show]&#125;)</span><br><span class="line">    <span class="comment">#Compare original images with their reconstructions</span></span><br><span class="line">    f,a=plt.subplots(<span class="number">2</span>,<span class="number">10</span>,figsize=(<span class="number">10</span>,<span class="number">2</span>))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(examples_to_show):</span><br><span class="line">        <span class="comment">#real data</span></span><br><span class="line">        a[<span class="number">0</span>][i].imshow(np.reshape(mnist.test.images[i],(<span class="number">28</span>,<span class="number">28</span>)))</span><br><span class="line">        <span class="comment">#predict data</span></span><br><span class="line">        a[<span class="number">1</span>][i].imshow(np.reshape(encode_decode[i],(<span class="number">28</span>,<span class="number">28</span>)))</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure>
<h2 id="运行结果："><a href="#运行结果：" class="headerlink" title="运行结果："></a>运行结果：</h2><p><img src="/2018/12/16/Autoencoder自编码/2.gif" alt="autoencoder"></p>
<p><strong>总结</strong>：发现经过压缩过后的MNIST data，在训练的时候明显速度加快了。说明在进行大量数据训练的时候，使用自编码进行encoder-decoder不失为一个好办法。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/12/16/rnn-use-variable/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Victor Drq">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/header.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="drqblog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/12/16/rnn-use-variable/" itemprop="url">rnn_use_variable</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-12-16T15:28:21+08:00">
                2018-12-16
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/categories/TensorFlow/" itemprop="url" rel="index">
                    <span itemprop="name">TensorFlow</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">字数统计&#58;</span>
                
                <span title="字数统计">
                  499
                </span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                
                <span title="阅读时长">
                  3
                </span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="在RNN中use-variable"><a href="#在RNN中use-variable" class="headerlink" title="在RNN中use variable"></a>在RNN中use variable</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#import module</span></span><br><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="comment">#define class</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TrainConfig</span>:</span></span><br><span class="line">    batch_size = <span class="number">20</span></span><br><span class="line">    time_steps = <span class="number">20</span></span><br><span class="line">    input_size = <span class="number">10</span></span><br><span class="line">    output_size = <span class="number">2</span></span><br><span class="line">    cell_size = <span class="number">11</span></span><br><span class="line">    learning_rate = <span class="number">0.01</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TestConfig</span><span class="params">(TrainConfig)</span>:</span></span><br><span class="line">    time_steps = <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#define RNN class</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RNN</span><span class="params">(object)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">#define the init method</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, config)</span>:</span></span><br><span class="line">        self._batch_size = config.batch_size</span><br><span class="line">        self._time_steps = config.time_steps</span><br><span class="line">        self._input_size = config.input_size</span><br><span class="line">        self._output_size = config.output_size</span><br><span class="line">        self._cell_size = config.cell_size</span><br><span class="line">        self._lr = config.learning_rate</span><br><span class="line">        self._built_RNN()</span><br><span class="line"></span><br><span class="line">    <span class="comment">#build the rnn network    </span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_built_RNN</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">with</span> tf.variable_scope(<span class="string">'inputs'</span>):</span><br><span class="line">            self._xs = tf.placeholder(tf.float32, [self._batch_size, self._time_steps, self._input_size], name=<span class="string">'xs'</span>)</span><br><span class="line">            self._ys = tf.placeholder(tf.float32, [self._batch_size, self._time_steps, self._output_size], name=<span class="string">'ys'</span>)</span><br><span class="line">        <span class="keyword">with</span> tf.name_scope(<span class="string">'RNN'</span>):</span><br><span class="line">            <span class="keyword">with</span> tf.variable_scope(<span class="string">'input_layer'</span>):</span><br><span class="line">                l_in_x = tf.reshape(self._xs, [<span class="number">-1</span>, self._input_size], name=<span class="string">'2_2D'</span>)  <span class="comment"># (batch*n_step, in_size)</span></span><br><span class="line">                <span class="comment"># Ws (in_size, cell_size)</span></span><br><span class="line">                Wi = self._weight_variable([self._input_size, self._cell_size])</span><br><span class="line">                print(Wi.name)</span><br><span class="line">                <span class="comment"># bs (cell_size, )</span></span><br><span class="line">                bi = self._bias_variable([self._cell_size, ])</span><br><span class="line">                <span class="comment"># l_in_y = (batch * n_steps, cell_size)</span></span><br><span class="line">                <span class="keyword">with</span> tf.name_scope(<span class="string">'Wx_plus_b'</span>):</span><br><span class="line">                    l_in_y = tf.matmul(l_in_x, Wi) + bi</span><br><span class="line">                l_in_y = tf.reshape(l_in_y, [<span class="number">-1</span>, self._time_steps, self._cell_size], name=<span class="string">'2_3D'</span>)</span><br><span class="line"></span><br><span class="line">            <span class="keyword">with</span> tf.variable_scope(<span class="string">'cell'</span>):</span><br><span class="line">                cell = tf.contrib.rnn.BasicLSTMCell(self._cell_size)</span><br><span class="line">                <span class="keyword">with</span> tf.name_scope(<span class="string">'initial_state'</span>):</span><br><span class="line">                    self._cell_initial_state = cell.zero_state(self._batch_size, dtype=tf.float32)</span><br><span class="line"></span><br><span class="line">                self.cell_outputs = []</span><br><span class="line">                cell_state = self._cell_initial_state</span><br><span class="line">                <span class="keyword">for</span> t <span class="keyword">in</span> range(self._time_steps):</span><br><span class="line">                    <span class="keyword">if</span> t &gt; <span class="number">0</span>: tf.get_variable_scope().reuse_variables()</span><br><span class="line">                    cell_output, cell_state = cell(l_in_y[:, t, :], cell_state)</span><br><span class="line">                    self.cell_outputs.append(cell_output)</span><br><span class="line">                self._cell_final_state = cell_state</span><br><span class="line"></span><br><span class="line">            <span class="keyword">with</span> tf.variable_scope(<span class="string">'output_layer'</span>):</span><br><span class="line">                <span class="comment"># cell_outputs_reshaped (BATCH*TIME_STEP, CELL_SIZE)</span></span><br><span class="line">                cell_outputs_reshaped = tf.reshape(tf.concat(self.cell_outputs, <span class="number">1</span>), [<span class="number">-1</span>, self._cell_size])</span><br><span class="line">                Wo = self._weight_variable((self._cell_size, self._output_size))</span><br><span class="line">                bo = self._bias_variable((self._output_size,))</span><br><span class="line">                product = tf.matmul(cell_outputs_reshaped, Wo) + bo</span><br><span class="line">                <span class="comment"># _pred shape (batch*time_step, output_size)</span></span><br><span class="line">                self._pred = tf.nn.relu(product)    <span class="comment"># for displacement</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">with</span> tf.name_scope(<span class="string">'cost'</span>):</span><br><span class="line">            _pred = tf.reshape(self._pred, [self._batch_size, self._time_steps, self._output_size])</span><br><span class="line">            mse = self.ms_error(_pred, self._ys)</span><br><span class="line">            mse_ave_across_batch = tf.reduce_mean(mse, <span class="number">0</span>)</span><br><span class="line">            mse_sum_across_time = tf.reduce_sum(mse_ave_across_batch, <span class="number">0</span>)</span><br><span class="line">            self._cost = mse_sum_across_time</span><br><span class="line">            self._cost_ave_time = self._cost / self._time_steps</span><br><span class="line"></span><br><span class="line">        <span class="keyword">with</span> tf.variable_scope(<span class="string">'trian'</span>):</span><br><span class="line">            self._lr = tf.convert_to_tensor(self._lr)</span><br><span class="line">            self.train_op = tf.train.AdamOptimizer(self._lr).minimize(self._cost)</span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">ms_error</span><span class="params">(y_target, y_pre)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> tf.square(tf.subtract(y_target, y_pre))</span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_weight_variable</span><span class="params">(shape, name=<span class="string">'weights'</span>)</span>:</span></span><br><span class="line">        initializer = tf.random_normal_initializer(mean=<span class="number">0.</span>, stddev=<span class="number">0.5</span>, )</span><br><span class="line">        <span class="keyword">return</span> tf.get_variable(shape=shape, initializer=initializer, name=name)</span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_bias_variable</span><span class="params">(shape, name=<span class="string">'biases'</span>)</span>:</span></span><br><span class="line">        initializer = tf.constant_initializer(<span class="number">0.1</span>)</span><br><span class="line">        <span class="keyword">return</span> tf.get_variable(name=name, shape=shape, initializer=initializer)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    train_config = TrainConfig()</span><br><span class="line">    test_config = TestConfig()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># the wrong method to reuse parameters in train rnn</span></span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">'train_rnn'</span>):</span><br><span class="line">        train_rnn1 = RNN(train_config)</span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">'test_rnn'</span>):</span><br><span class="line">        test_rnn1 = RNN(test_config)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># the right method to reuse parameters in train rnn</span></span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">'rnn'</span>) <span class="keyword">as</span> scope:</span><br><span class="line">        sess = tf.Session()</span><br><span class="line">        train_rnn2 = RNN(train_config)</span><br><span class="line">        scope.reuse_variables()</span><br><span class="line">        test_rnn2 = RNN(test_config)</span><br><span class="line">        <span class="comment"># tf.initialize_all_variables() no long valid from</span></span><br><span class="line">        <span class="comment"># 2017-03-02 if using tensorflow &gt;= 0.12</span></span><br><span class="line">        <span class="keyword">if</span> int((tf.__version__).split(<span class="string">'.'</span>)[<span class="number">1</span>]) &lt; <span class="number">12</span> <span class="keyword">and</span> int((tf.__version__).split(<span class="string">'.'</span>)[<span class="number">0</span>]) &lt; <span class="number">1</span>:</span><br><span class="line">            init = tf.initialize_all_variables()</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            init = tf.global_variables_initializer()</span><br><span class="line">        sess.run(init)</span><br></pre></td></tr></table></figure>
<h2 id="使用variable-scope的效果："><a href="#使用variable-scope的效果：" class="headerlink" title="使用variable_scope的效果："></a>使用variable_scope的效果：</h2><p><img src="/2018/12/16/rnn-use-variable/1.png" alt="rnn"></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/12/16/name-scope的用法/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Victor Drq">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/header.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="drqblog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/12/16/name-scope的用法/" itemprop="url">name_scope的用法</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-12-16T15:28:02+08:00">
                2018-12-16
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/categories/TensorFlow/" itemprop="url" rel="index">
                    <span itemprop="name">TensorFlow</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">字数统计&#58;</span>
                
                <span title="字数统计">
                  169
                </span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                
                <span title="阅读时长">
                  1
                </span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="name-scope的用法"><a href="#name-scope的用法" class="headerlink" title="name_scope的用法"></a>name_scope的用法</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#author:victor</span></span><br><span class="line"><span class="comment">#name_scope和variable_scope的区别</span></span><br><span class="line"><span class="comment">#from _future_ import print_function</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#import module</span></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">tf.set_random_seed(<span class="number">1</span>)<span class="comment">#reproducible</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">'a_name_scope'</span>):</span><br><span class="line">    <span class="comment">#常量初始化</span></span><br><span class="line">    initializer=tf.constant_initializer(value=<span class="number">1</span>)</span><br><span class="line">    var1=tf.get_variable(name=<span class="string">'var1'</span>,shape=[<span class="number">1</span>],dtype=tf.float32,initializer=initializer)</span><br><span class="line">    var2=tf.Variable(name=<span class="string">'var2'</span>,initial_value=[<span class="number">2</span>],dtype=tf.float32)</span><br><span class="line">    var21=tf.Variable(name=<span class="string">'var2'</span>,initial_value=[<span class="number">2</span>,<span class="number">1</span>],dtype=tf.float32)</span><br><span class="line">    var22=tf.Variable(name=<span class="string">'var2'</span>,initial_value=[<span class="number">2</span>,<span class="number">2</span>],dtype=tf.float32)</span><br><span class="line">    </span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(tf.initialize_all_variables())</span><br><span class="line">    print(var1.name)<span class="comment">#var1:0,用tf.get_vriable创建变量，name_scope不会加上，也就是无效的</span></span><br><span class="line">    print(sess.run(var1))</span><br><span class="line">    print(var2.name)<span class="comment">#a_name_scope/var2:0，用tf.Variable创建变量，会先检查一下有没有name_scope，有就会加上name_scope</span></span><br><span class="line">    print(sess.run(var2))</span><br><span class="line">    print(var1.name)</span><br><span class="line">    print(sess.run(var21))</span><br><span class="line">    print(var2.name)</span><br><span class="line">    print(sess.run(var22))</span><br></pre></td></tr></table></figure>
<h2 id="使用name-scope的效果："><a href="#使用name-scope的效果：" class="headerlink" title="使用name_scope的效果："></a>使用name_scope的效果：</h2><p><img src="/2018/12/16/name-scope的用法/1.png" alt="name_scope"></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/12/16/variable-scope的用法/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Victor Drq">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/header.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="drqblog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/12/16/variable-scope的用法/" itemprop="url">variable_scope的用法</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-12-16T15:27:40+08:00">
                2018-12-16
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/categories/TensorFlow/" itemprop="url" rel="index">
                    <span itemprop="name">TensorFlow</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">字数统计&#58;</span>
                
                <span title="字数统计">
                  353
                </span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                
                <span title="阅读时长">
                  1
                </span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="variable-scope的用法"><a href="#variable-scope的用法" class="headerlink" title="variable_scope的用法"></a>variable_scope的用法</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#author:victor</span></span><br><span class="line"><span class="comment">#import module</span></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">tf.reset_default_graph()</span><br><span class="line"></span><br><span class="line">tf.set_random_seed(<span class="number">1</span>)<span class="comment">#reproducible</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">'a_variable_scope'</span>) <span class="keyword">as</span> scope:</span><br><span class="line">    <span class="comment">#常量初始化</span></span><br><span class="line">    initializer=tf.constant_initializer(value=<span class="number">3</span>)</span><br><span class="line">    var3=tf.get_variable(name=<span class="string">'var3'</span>,shape=[<span class="number">1</span>],dtype=tf.float32,initializer=initializer)</span><br><span class="line">    var4=tf.Variable(name=<span class="string">'var4'</span>,initial_value=[<span class="number">4</span>],dtype=tf.float32)</span><br><span class="line">    var4_reuse=tf.Variable(name=<span class="string">'var4'</span>,initial_value=[<span class="number">4</span>],dtype=tf.float32)</span><br><span class="line">    <span class="comment">#想要重复利用必须这么写：</span></span><br><span class="line">    scope.reuse_variables()<span class="comment">#如果不加这一句会报错，报Variable a_variable_scope/var3 already exists, disallowed. Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope</span></span><br><span class="line">    var3_reuse=tf.get_variable(name=<span class="string">'var3'</span>)</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(tf.initialize_all_variables())</span><br><span class="line">    print(var3.name)<span class="comment">#a_variable_scope/var3:0,variable_scope加上</span></span><br><span class="line">    print(sess.run(var4))</span><br><span class="line">    </span><br><span class="line">    print(var4.name)<span class="comment">#a_variable_scope/var4:0，variable_scope加上</span></span><br><span class="line">    print(sess.run(var4))</span><br><span class="line">    </span><br><span class="line">    print(var4_reuse.name)<span class="comment">#a_variable_scope/var4_1:0,variable_scope加上,重复利用的时候，并不是打印出var4:0,而是创建了一个新的var4_1</span></span><br><span class="line">    print(sess.run(var4_reuse))</span><br><span class="line">    </span><br><span class="line">    print(var3_reuse.name)<span class="comment">#a_variable_scope/var3:0，可以看到和var3一样了，为什么要重复利用呢？</span></span><br><span class="line">    <span class="comment">#因为同一个variable可能需要在不同的地方使用，</span></span><br><span class="line">    <span class="comment">#通常在RNN循环神经网络中，有一个重复循环的机制。通常在RNN中用到reuse_variable，通常Train RNN和Test RNN的神经网络结构不一样，但是其中的参数可能一样的，所以就会用到reuse variable...</span></span><br><span class="line">    print(sess.run(var3_reuse))</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">比如RNN结构</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TrainConfig</span>:</span></span><br><span class="line">    batch_size=<span class="number">20</span></span><br><span class="line">    time_steps=<span class="number">20</span></span><br><span class="line">    input_size=<span class="number">10</span></span><br><span class="line">    output_size=<span class="number">2</span></span><br><span class="line">    cell_size=<span class="number">11</span></span><br><span class="line">    learning_rate=<span class="number">0.01</span></span><br><span class="line"> <span class="comment">#但是RNN的test，有可能的time_steps改成1，但是不会用Train里的time_steps</span></span><br><span class="line"><span class="comment">#所以就会用到reuse variable的time_steps   </span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TestConfig</span><span class="params">(TrainConfig)</span>:</span></span><br><span class="line">    time_steps=<span class="number">1</span></span><br></pre></td></tr></table></figure>
<h2 id="使用variable-scope的效果："><a href="#使用variable-scope的效果：" class="headerlink" title="使用variable_scope的效果："></a>使用variable_scope的效果：</h2><p><img src="/2018/12/16/variable-scope的用法/1.png" alt="variable_scope"></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/12/16/用RNN来预测学习Sinx曲线/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Victor Drq">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/header.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="drqblog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/12/16/用RNN来预测学习Sinx曲线/" itemprop="url">用RNN来预测学习Sinx曲线</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-12-16T13:29:19+08:00">
                2018-12-16
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/categories/TensorFlow/" itemprop="url" rel="index">
                    <span itemprop="name">TensorFlow</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">字数统计&#58;</span>
                
                <span title="字数统计">
                  995
                </span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                
                <span title="阅读时长">
                  5
                </span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="用RNN来预测学习sinx曲线"><a href="#用RNN来预测学习sinx曲线" class="headerlink" title="用RNN来预测学习sinx曲线"></a>用RNN来预测学习sinx曲线</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#author:victor</span></span><br><span class="line"><span class="comment">#使用RNN进行回归训练，会用到自己创建对sin曲线，预测一条cos曲线，</span></span><br><span class="line"><span class="comment">#设置RNN各种参数</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="comment">#清除图形推展并重置全局默认图形</span></span><br><span class="line">tf.reset_default_graph() </span><br><span class="line">BATCH_START = <span class="number">0</span> <span class="comment">#建立batch data时候对index</span></span><br><span class="line">TIME_STEPS = <span class="number">20</span> <span class="comment">#backpropagation through time 的 time_steps</span></span><br><span class="line">BATCH_SIZE = <span class="number">50</span></span><br><span class="line">INPUT_SIZE = <span class="number">1</span> <span class="comment">#sim 数据输入size</span></span><br><span class="line">OUTPUT_SIZE = <span class="number">1</span> <span class="comment">#cos数据输出size</span></span><br><span class="line">CELL_SIZE = <span class="number">10</span> <span class="comment">#RNN的hidden unit size</span></span><br><span class="line">LR = <span class="number">0.006</span></span><br><span class="line">state = tf.Variable(<span class="number">0.0</span>,dtype=tf.float32)</span><br><span class="line"><span class="comment">#生成数据的get_batch function:</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_batch</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">global</span> BATCH_START, TIME_STEPS</span><br><span class="line">    <span class="comment"># xs shape (50batch, 20steps)</span></span><br><span class="line">    xs = np.arange(BATCH_START, BATCH_START + TIME_STEPS*BATCH_SIZE).reshape((BATCH_SIZE, TIME_STEPS)) / (<span class="number">10</span>*np.pi)</span><br><span class="line">    seq = np.sin(xs)</span><br><span class="line">    res = np.cos(xs)</span><br><span class="line">    BATCH_START += TIME_STEPS</span><br><span class="line">    <span class="comment"># returned seq, res and xs; shape(batch, step, input)</span></span><br><span class="line">    <span class="keyword">return</span> [seq[:, :, np.newaxis], res[:, :, np.newaxis], xs]</span><br><span class="line"></span><br><span class="line"><span class="comment">#定义LSTMRNN的主体结构</span></span><br><span class="line"><span class="comment">#使用一个 class 来定义这次的 LSTMRNN 会更加方便. 第一步定义 class 中的 __init__ 传入各种参数:</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LSTMRNN</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, n_steps, input_size, output_size, cell_size, batch_size)</span>:</span></span><br><span class="line">        self.n_steps = n_steps</span><br><span class="line">        self.input_size = input_size</span><br><span class="line">        self.output_size = output_size</span><br><span class="line">        self.cell_size = cell_size</span><br><span class="line">        self.batch_size = batch_size</span><br><span class="line">        <span class="keyword">with</span> tf.name_scope(<span class="string">'inputs'</span>):</span><br><span class="line">            self.xs = tf.placeholder(tf.float32, [<span class="keyword">None</span>, n_steps, input_size], name=<span class="string">'xs'</span>)</span><br><span class="line">            self.ys = tf.placeholder(tf.float32, [<span class="keyword">None</span>, n_steps, output_size], name=<span class="string">'ys'</span>)</span><br><span class="line">        <span class="keyword">with</span> tf.variable_scope(<span class="string">'in_hidden'</span>):</span><br><span class="line">            self.add_input_layer()</span><br><span class="line">        <span class="keyword">with</span> tf.variable_scope(<span class="string">'LSTM_cell'</span>):</span><br><span class="line">            self.add_cell()</span><br><span class="line">        <span class="keyword">with</span> tf.variable_scope(<span class="string">'out_hidden'</span>):</span><br><span class="line">            self.add_output_layer()</span><br><span class="line">        <span class="keyword">with</span> tf.name_scope(<span class="string">'cost'</span>):</span><br><span class="line">            self.compute_cost()</span><br><span class="line">        <span class="keyword">with</span> tf.name_scope(<span class="string">'train'</span>):</span><br><span class="line">            self.train_op = tf.train.AdamOptimizer(LR).minimize(self.cost)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">#设置add_input_layer()函数，添加input_layer()</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">add_input_layer</span><span class="params">(self,)</span>:</span></span><br><span class="line">        l_in_x = tf.reshape(self.xs, [<span class="number">-1</span>, self.input_size], name = <span class="string">'2_2D'</span>) <span class="comment">#(batch*n_step, in_size)</span></span><br><span class="line">        <span class="comment">#Ws (in_size, cell_size)</span></span><br><span class="line">        Ws_in = self._weight_variable([self.input_size, self.cell_size])</span><br><span class="line">        <span class="comment">#bs (cell_size)</span></span><br><span class="line">        bs_in = self._bias_variable([self.cell_size,])</span><br><span class="line">        <span class="comment">#l_in_y = (batch * n_steps, cell_size)</span></span><br><span class="line">        <span class="keyword">with</span> tf.name_scope(<span class="string">'Wx_plus_b'</span>):</span><br><span class="line">            l_in_y = tf.matmul(l_in_x, Ws_in) +bs_in</span><br><span class="line">        <span class="comment">#reshape l_in_y ==&gt; (batch, n_steps, cell_size)</span></span><br><span class="line">        self.l_in_y = tf.reshape(l_in_y, [<span class="number">-1</span>, self.n_steps, self.cell_size], name=<span class="string">'2_3D'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">#设置add_cell功能，添加cell， 注意此处的self.cell_init_state, 因为我们在 training 的时候, 这个地方要特别说明.</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">add_cell</span><span class="params">(self)</span>:</span></span><br><span class="line">        lstm_cell = tf.contrib.rnn.BasicLSTMCell(self.cell_size, forget_bias = <span class="number">1.0</span>, state_is_tuple = <span class="keyword">True</span>)</span><br><span class="line">        <span class="keyword">with</span> tf.name_scope(<span class="string">'initial_state'</span>):</span><br><span class="line">            self.cell_init_state = lstm_cell.zero_state(self.batch_size, dtype= tf.float32)</span><br><span class="line">        self.cell_outputs, self.cell_final_state = tf.nn.dynamic_rnn(lstm_cell, self.l_in_y, initial_state=self.cell_init_state, time_major=<span class="keyword">False</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">#设置add_output_layer功能， 添加output_layer:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">add_output_layer</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="comment"># shape= (batch * steps, cell_size)</span></span><br><span class="line">        l_out_x = tf.reshape(self.cell_outputs, [<span class="number">-1</span>, self.cell_size], name= <span class="string">'2_2D'</span>)</span><br><span class="line">        Ws_out = self._weight_variable([self.cell_size, self.output_size])</span><br><span class="line">        bs_out = self._bias_variable([self.output_size, ])</span><br><span class="line">        <span class="comment"># shape = (batch * steps, output_size)</span></span><br><span class="line">        <span class="keyword">with</span> tf.name_scope(<span class="string">'Wx_plus_b'</span>):</span><br><span class="line">            self.pred = tf.matmul(l_out_x, Ws_out) + bs_out</span><br><span class="line"></span><br><span class="line">    <span class="comment">#添加RNN 剩余部分</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">compute_cost</span><span class="params">(self)</span>:</span></span><br><span class="line">        losses = tf.contrib.legacy_seq2seq.sequence_loss_by_example(</span><br><span class="line">            [tf.reshape(self.pred, [<span class="number">-1</span>], name=<span class="string">'reshape_pred'</span>)],</span><br><span class="line">            [tf.reshape(self.ys, [<span class="number">-1</span>], name= <span class="string">'reshape_target'</span>)],</span><br><span class="line">            [tf.ones([self.batch_size * self.n_steps], dtype = tf.float32)],</span><br><span class="line">            average_across_timesteps = <span class="keyword">True</span>,</span><br><span class="line">            softmax_loss_function = self.ms_error,</span><br><span class="line">            name= <span class="string">'losses'</span></span><br><span class="line">        )</span><br><span class="line">        <span class="keyword">with</span> tf.name_scope(<span class="string">'average_cost'</span>):</span><br><span class="line">            self.cost = tf.div(</span><br><span class="line">                tf.reduce_sum(losses, name=<span class="string">'losses_sum'</span>),</span><br><span class="line">                tf.cast(self.batch_size, tf.float32),</span><br><span class="line">                name = <span class="string">'average_cost'</span>)</span><br><span class="line">            tf.summary.scalar(<span class="string">'cost'</span>, self.cost)</span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">ms_error</span><span class="params">(labels, logits)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> tf.square(tf.subtract(labels, logits))</span><br><span class="line">    <span class="comment">#没有加@staticmethod时候报错， TypeError: ms_error() got multiple values for argument 'labels'</span></span><br><span class="line">    <span class="comment">#解决办法：https://stackoverflow.com/questions/18950054/class-method-generates-typeerror-got-multiple-values-for-keyword-argument</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">#define weight</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_weight_variable</span><span class="params">(self, shape, name=<span class="string">'weights'</span>)</span>:</span></span><br><span class="line">        initializer = tf.random_normal_initializer(mean=<span class="number">0.</span>, stddev=<span class="number">1.</span>, )</span><br><span class="line">        <span class="keyword">return</span> tf.get_variable(shape=shape, initializer=initializer, name=name)</span><br><span class="line"></span><br><span class="line">    <span class="comment">#define biases</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_bias_variable</span><span class="params">(self, shape, name=<span class="string">'biases'</span>)</span>:</span></span><br><span class="line">        initializer = tf.constant_initializer(<span class="number">0.1</span>)</span><br><span class="line">        <span class="keyword">return</span> tf.get_variable(name=name, shape = shape, initializer=initializer)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#训练LSTMRNN</span></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    model = LSTMRNN(TIME_STEPS, INPUT_SIZE, OUTPUT_SIZE, CELL_SIZE, BATCH_SIZE)</span><br><span class="line">    sess = tf.Session()</span><br><span class="line">    merged = tf.summary.merge_all()</span><br><span class="line">    writer = tf.summary.FileWriter(<span class="string">"logs"</span>, sess.graph)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">#tensorflow的版本小于12的，变量初始化方法</span></span><br><span class="line">    <span class="keyword">if</span> int((tf.__version__).split(<span class="string">'.'</span>)[<span class="number">1</span>]) &lt; <span class="number">12</span> <span class="keyword">and</span> int((tf.__version__).split(<span class="string">'.'</span>)[<span class="number">0</span>]) &lt; <span class="number">1</span>:</span><br><span class="line">        init = tf.initialize_all_variables()</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        init = tf.global_variables_initializer()</span><br><span class="line">    sess.run(init)</span><br><span class="line">    <span class="comment"># relocate to the local dir and run this line to view it on Chrome (http://0.0.0.0:6006/):</span></span><br><span class="line">    <span class="comment"># $ tensorboard --logdir='logs'</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">#可视化库matplotlib的显示模式默认是block阻塞模式。</span></span><br><span class="line">    <span class="comment">#就是plt.show()之后，程序回暂停在哪儿，并不会继续执行下去。</span></span><br><span class="line">    <span class="comment">#展示动态图就需要plt.ion()函数，把matplotlib的显示模式转换为交互interactive模式</span></span><br><span class="line">    plt.ion()</span><br><span class="line">    plt.show()</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">200</span>):</span><br><span class="line">        seq, res, xs = get_batch()</span><br><span class="line">        <span class="keyword">if</span> i == <span class="number">0</span>:</span><br><span class="line">            feed_dict = &#123;</span><br><span class="line">                    model.xs: seq,</span><br><span class="line">                    model.ys: res,</span><br><span class="line">                    <span class="comment"># create initial state</span></span><br><span class="line">            &#125;</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            feed_dict = &#123;</span><br><span class="line">                model.xs: seq,</span><br><span class="line">                model.ys: res,</span><br><span class="line">                model.cell_init_state: state    <span class="comment"># use last state as the initial state for this run</span></span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">        _, cost, state, pred = sess.run(</span><br><span class="line">            [model.train_op, model.cost, model.cell_final_state, model.pred],</span><br><span class="line">            feed_dict=feed_dict)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># plotting</span></span><br><span class="line">        plt.plot(xs[<span class="number">0</span>, :], res[<span class="number">0</span>].flatten(), <span class="string">'r'</span>, xs[<span class="number">0</span>, :], pred.flatten()[:TIME_STEPS], <span class="string">'b--'</span>)</span><br><span class="line">        plt.ylim((<span class="number">-1.2</span>, <span class="number">1.2</span>))</span><br><span class="line">        plt.draw()</span><br><span class="line">        plt.pause(<span class="number">0.3</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> i % <span class="number">20</span> == <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">'cost: '</span>, round(cost, <span class="number">4</span>))</span><br><span class="line">            result = sess.run(merged, feed_dict)</span><br><span class="line">            writer.add_summary(result, i)</span><br></pre></td></tr></table></figure>
<h2 id="运行效果"><a href="#运行效果" class="headerlink" title="运行效果"></a>运行效果</h2><p><img src="/2018/12/16/用RNN来预测学习Sinx曲线/4.gif" alt="rnn predict sinx"></p>
<p>发现刚开始RNN蓝色曲线为预测去年，并不是很重合，随着慢慢的RNN训练，到60以后，预测曲线和实际曲线基本吻合。</p>
<h2 id="用Tensorboard查看loss和graph"><a href="#用Tensorboard查看loss和graph" class="headerlink" title="用Tensorboard查看loss和graph"></a>用Tensorboard查看loss和graph</h2><ul>
<li><p>使用tensorboard</p>
<p><img src="/2018/12/16/用RNN来预测学习Sinx曲线/1.gif" alt="console"></p>
</li>
<li><p>查看loss和graph</p>
</li>
</ul>
<p><img src="/2018/12/16/用RNN来预测学习Sinx曲线/2.gif" alt="tensorboard"></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/12/16/RNN入门demo/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Victor Drq">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/header.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="drqblog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/12/16/RNN入门demo/" itemprop="url">RNN入门demo</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-12-16T13:11:21+08:00">
                2018-12-16
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/categories/TensorFlow/" itemprop="url" rel="index">
                    <span itemprop="name">TensorFlow</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">字数统计&#58;</span>
                
                <span title="字数统计">
                  715
                </span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                
                <span title="阅读时长">
                  3
                </span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="RNN入门Demo"><a href="#RNN入门Demo" class="headerlink" title="RNN入门Demo"></a>RNN入门Demo</h2><p>​    耗费了大量时间来讲解RNN和LSTM的原理，并且这一块确实有点难以理解。实践是检验真理的唯一标准，废话不多说，直接上基于TensorFlow平台的RNN加上LSTM优化后的代码和运行效果。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment">#author:victor</span></span><br><span class="line"><span class="comment">#什么是循环神经网络RNN</span></span><br><span class="line"><span class="comment">#What is Recurrent Neural Networks?(RNN)</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.examples.tutorials.mnist <span class="keyword">import</span> input_data</span><br><span class="line">tf.reset_default_graph() </span><br><span class="line"></span><br><span class="line"><span class="comment">#Mnist data</span></span><br><span class="line">mnist=input_data.read_data_sets(<span class="string">'MNIST_data'</span>,one_hot=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#networks parameters</span></span><br><span class="line">lr=<span class="number">0.001</span><span class="comment">#learning rate</span></span><br><span class="line">training_iters=<span class="number">1000000</span><span class="comment">#iteration也就是循环多少次</span></span><br><span class="line">batch_size=<span class="number">128</span></span><br><span class="line">display_step=<span class="number">10</span></span><br><span class="line"></span><br><span class="line">n_inputs=<span class="number">28</span><span class="comment">#MNIST data input(image shape:28*28)</span></span><br><span class="line">n_steps=<span class="number">28</span><span class="comment">#time steps,inut的28行，作为28列输出</span></span><br><span class="line">n_hidden_units=<span class="number">128</span><span class="comment">#neurons in hidden layer</span></span><br><span class="line">n_classes=<span class="number">10</span><span class="comment">#MNIST classes(0-9digits)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#tf.Graph input</span></span><br><span class="line">x=tf.placeholder(tf.float32,[<span class="keyword">None</span>,n_steps,n_inputs])</span><br><span class="line">y=tf.placeholder(tf.float32,[<span class="keyword">None</span>,n_classes])</span><br><span class="line"></span><br><span class="line"><span class="comment">#Define weights</span></span><br><span class="line">weights=&#123;</span><br><span class="line"><span class="comment">#(28,128)</span></span><br><span class="line"><span class="string">'in'</span>:tf.Variable(tf.random_normal([n_inputs,n_hidden_units])),</span><br><span class="line"><span class="comment">#(128,10)                                 </span></span><br><span class="line"><span class="string">'out'</span>:tf.Variable(tf.random_normal([n_hidden_units,n_classes]))  </span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line"><span class="comment">#Define biases</span></span><br><span class="line">biases=&#123;</span><br><span class="line"><span class="comment">#(128,) </span></span><br><span class="line"><span class="string">'in'</span>:tf.Variable(tf.constant(<span class="number">0.1</span>,shape=[n_hidden_units,])),</span><br><span class="line"><span class="comment">#(10,)                             </span></span><br><span class="line"><span class="string">'out'</span>:tf.Variable(tf.constant(<span class="number">0.1</span>,shape=[n_classes,]))        </span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#Define RNN</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">RNN</span><span class="params">(X,weights,biases)</span>:</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">#hidden layer for input to cell</span></span><br><span class="line">    <span class="comment">#X(128 batch,28 steps,28 inputs)</span></span><br><span class="line">    <span class="comment">#把X转换成(128*28,28 inputs)</span></span><br><span class="line">    X=tf.reshape(X,[<span class="number">-1</span>,n_inputs])</span><br><span class="line">    <span class="comment">#把X转换成(128batch,28 steps,128 hidden)</span></span><br><span class="line">    X_in=tf.matmul(X,weights[<span class="string">'in'</span>]+biases[<span class="string">'in'</span>])</span><br><span class="line">    <span class="comment">#把X转换成(128batch,28steps,128hidden)</span></span><br><span class="line">    X_in=tf.reshape(X_in,[<span class="number">-1</span>,n_steps,n_hidden_units])</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    <span class="comment">#cell</span></span><br><span class="line">    <span class="comment">#使用lstm(long-short Term Memory),因为使用RNN可能会存在梯度爆炸，用LSTM优化</span></span><br><span class="line">    <span class="comment">#RNN中一般会用tanh()函数作为激活函数</span></span><br><span class="line">    <span class="comment">#在迭代后期，会逐渐收敛，导致梯度趋于0，于是，出现了“梯度下降”的问题。</span></span><br><span class="line">    lstm_cell=tf.nn.rnn_cell.BasicLSTMCell(n_hidden_units,</span><br><span class="line">                                           forget_bias=<span class="number">1.0</span>,</span><br><span class="line">                                           state_is_tuple=<span class="keyword">True</span>)<span class="comment">#state_is_tuple，生成的是不是一个元组</span></span><br><span class="line">    <span class="comment">#lstm cell is divided into two parts(c_state,m_state),主线的state是c_state,副线的state是m_state</span></span><br><span class="line">    _init_state=lstm_cell.zero_state(batch_size,dtype=tf.float32)    </span><br><span class="line">    <span class="comment">#使用dynamic_rnn比rnn更好，优点在于对尺度不同的数据的处理上，会减少计算量</span></span><br><span class="line">    <span class="comment">#time_major,上面的28 steps是它，</span></span><br><span class="line">    outputs,states=tf.nn.dynamic_rnn(lstm_cell,X_in,initial_state=_init_state,time_major=<span class="keyword">False</span>)</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    <span class="comment">#hidden layer for output as the final results</span></span><br><span class="line">    <span class="comment">#method1：</span></span><br><span class="line">    results=tf.matmul(states[<span class="number">1</span>],weights[<span class="string">'out'</span>])+biases[<span class="string">'out'</span>]</span><br><span class="line">    <span class="comment">#method2:                 </span></span><br><span class="line">    <span class="comment">#or use unpack to list[(batch,outputs)..]*steps,就是把tensor解包成list</span></span><br><span class="line">    <span class="comment">#outputs=tf.unstack(tf.transpose(outputs,[1,0,2])) #states is the last outputs</span></span><br><span class="line">    <span class="comment">#选择最后一步的outputs,也就是-1</span></span><br><span class="line">    <span class="comment">#results=tf.matmul(outputs[-1],weights['out'])+biases['out']                              </span></span><br><span class="line">    <span class="keyword">return</span> results</span><br><span class="line">    </span><br><span class="line"><span class="comment">#prediction    </span></span><br><span class="line">pred=RNN(x,weights,biases)</span><br><span class="line"><span class="comment">#cost</span></span><br><span class="line">cost=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred,labels=y))</span><br><span class="line"><span class="comment">#train_op</span></span><br><span class="line">train_op=tf.train.AdamOptimizer(lr).minimize(cost)</span><br><span class="line"></span><br><span class="line">correct_pred=tf.equal(tf.argmax(pred,<span class="number">1</span>),tf.argmax(y,<span class="number">1</span>))</span><br><span class="line">accuracy=tf.reduce_mean(tf.cast(correct_pred,tf.float32))</span><br><span class="line"></span><br><span class="line"><span class="comment">#important step</span></span><br><span class="line">init=tf.initialize_all_variables()</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(init)</span><br><span class="line">    step=<span class="number">0</span></span><br><span class="line">    <span class="keyword">while</span> step*batch_size&lt;training_iters:</span><br><span class="line">        batch_xs,batch_ys=mnist.train.next_batch(batch_size)</span><br><span class="line">        batch_xs=batch_xs.reshape([batch_size,n_steps,n_inputs])<span class="comment">#28行，28列，在加上要一个batch_size</span></span><br><span class="line">        sess.run([train_op],feed_dict=&#123;x:batch_xs,y:batch_ys&#125;)</span><br><span class="line">        <span class="keyword">if</span> step%<span class="number">20</span>==<span class="number">0</span>:</span><br><span class="line">            print(sess.run(accuracy,</span><br><span class="line">                           feed_dict=&#123;x:batch_xs,</span><br><span class="line">                                      y:batch_ys</span><br><span class="line">                                      &#125;</span><br><span class="line">                           )</span><br><span class="line">                 )</span><br><span class="line">        step+=<span class="number">1</span></span><br></pre></td></tr></table></figure>
<h2 id="运行效果"><a href="#运行效果" class="headerlink" title="运行效果"></a>运行效果</h2><ul>
<li>由于自己是CPU版本的TensorFlow，运行起来比较慢，只能慢慢等待咯</li>
</ul>
<p><img src="/2018/12/16/RNN入门demo/1.gif" alt="RNN"></p>
<ul>
<li><p>随着训练次数的增加，精确度也渐渐上升</p>
<p><img src="/2018/12/16/RNN入门demo/2.gif" alt="RNN"></p>
</li>
<li><p>设置的训练100w次，每20步输出一次结果，由于时间太久，我就不一一截图了。训练结束后，精确度99%</p>
</li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/12/16/LSTM长短期记忆网络/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Victor Drq">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/header.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="drqblog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/12/16/LSTM长短期记忆网络/" itemprop="url">LSTM(Long-Short Term Memory)长短期记忆网络</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-12-16T12:57:42+08:00">
                2018-12-16
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/categories/TensorFlow/" itemprop="url" rel="index">
                    <span itemprop="name">TensorFlow</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">字数统计&#58;</span>
                
                <span title="字数统计">
                  897
                </span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                
                <span title="阅读时长">
                  3
                </span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="LSTM-Long-Short-Term-Memory-长短期记忆网络"><a href="#LSTM-Long-Short-Term-Memory-长短期记忆网络" class="headerlink" title="LSTM(Long-Short Term Memory)长短期记忆网络"></a>LSTM(Long-Short Term Memory)长短期记忆网络</h2><p><strong>一、LSTM结构</strong></p>
<p>​        <strong>LSTM(Long-Short Term Memory)，长短期记忆网络</strong>是RNN的一种变形结构。RNN因为梯度消失的原因只有短期记忆，LSTM网络通过精妙的Cell门控制将短期记忆与长期记忆结合起来，并在一定程度上解决了梯度消失的问题。</p>
<p>所有RNN都具有一种重复神经网络模块的链式形式。在标准的RNN中，重复的模块只有一个非常简单的结构，比如一个tanh层。</p>
<ul>
<li><strong>标准的RNN：</strong></li>
</ul>
<p><img src="https://mmbiz.qpic.cn/mmbiz_png/ZWiblltaIVJqHqcPibzEJzDL6xtC58zFJzuKaicpf0NcJlc3o2MuJLmou6lHUukuXNZ1hSwtZgy85iat5zBwXoribfQ/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="img"></p>
<ul>
<li><p><strong>LSTM的结构：</strong></p>
<p>LSTM同样的结构，但是重复的模块拥有一个不同的结构，不同于单一层，这里是由四个，以一种非常特殊的方式进行交互</p>
</li>
</ul>
<p><img src="https://mmbiz.qpic.cn/mmbiz_png/ZWiblltaIVJqHqcPibzEJzDL6xtC58zFJz4iau12ibDOmPcfibkw3ibguhfpI0O3pK80R8QdTicHWrs5L2ia6eA6loDgfQ/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="img"></p>
<ul>
<li><strong>LSTM解析图标：</strong></li>
</ul>
<p><img src="https://mmbiz.qpic.cn/mmbiz_png/ZWiblltaIVJqHqcPibzEJzDL6xtC58zFJzbfbQ5RcgAVqDG2PTIePC5V8Jt510kIvD81gulmSe08zb5D4WiacamDw/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="img"></p>
<ul>
<li>橙黄色矩形：学习得到的神经网络层</li>
<li>粉色圆形：代表一些运算，比如加法，乘法</li>
<li>黑色单箭头：向量的传输</li>
<li>两个箭头合并：表示向量的连接</li>
<li>一个箭头分开：表示向量的复制</li>
</ul>
<p><strong>二、LSTM的核心思想</strong></p>
<ul>
<li>LSTM的关键就是细胞Cell状态，水平线在图上方贯穿运行。</li>
<li>Cell细胞状态像传送带一样，直接在整个链上运行，只有一些少量的线性交互，信息在上线流传保持不变很容易。</li>
</ul>
<p><img src="https://mmbiz.qpic.cn/mmbiz_png/ZWiblltaIVJqHqcPibzEJzDL6xtC58zFJzibU8dqKfcb80AJhcTe2kpEbA3oFkvoqNK2Yczuqbckd99AnHOPbhNMQ/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="img"></p>
<ul>
<li><p>LSTM有通过精心设计的称作为“门”的结构来去除或者增加信息到细胞状态的能力。门是一种让信息选择通过的方法。包含一个sigmoid神经网络层和一个按位的乘法操作</p>
<p><img src="https://mmbiz.qpic.cn/mmbiz_png/ZWiblltaIVJqHqcPibzEJzDL6xtC58zFJzcmmIT6flW2INcSCbGSaxt2aIARSwDYOkFicOv5LzIMMt6jgP7OEqV2A/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="img"></p>
</li>
</ul>
<p>sigmoid层输出0~1之间的数值，描述每个部分有多少量可以通过。0代表不允许</p>
<p>任何量通过，1代表允许任意量通过。LSTM拥有三个门，来保护和控制细胞状态。</p>
<p><strong>三、LSTM的推理</strong></p>
<ul>
<li><strong>LSTM中的第一步是决定我们会从细胞状态中丢弃什么信息</strong>。第一步的决定四通过一个叫做<strong>忘记门层</strong>完成。忘记门层会读ht-1和xt，输出一个0~1之间的数值给每个在细胞状态Ct-1中的数字，1表示完全保留，0表示完全舍弃<img src="https://mmbiz.qpic.cn/mmbiz_png/ZWiblltaIVJqHqcPibzEJzDL6xtC58zFJzF7sg4qWOzv480f9vJrSNffHR9UGjxEiag23UZZQI42ZCWPbHyREYNPw/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="img"></li>
<li><strong>下一步是确定什么样的新信息被存放在细胞状态**</strong>中**，这里包含两个部分：</li>
</ul>
<p>​       sigmoid层叫做输入门层，决定将要更新的值</p>
<p>​       tanh层创建一个新的候选值向量Ct，会被假如到状态中。</p>
<p><img src="https://mmbiz.qpic.cn/mmbiz_png/ZWiblltaIVJqHqcPibzEJzDL6xtC58zFJzzLhSKFCeib4VXiab2kIXIyiaEvVKkdq67M7eicxibjXBdgghia67MjcyeVTw/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="img"></p>
<ul>
<li><strong>更新旧细胞状态，把Ct-1更新为Ct</strong>。就是把旧状态与ft相乘，丢弃掉确定需要丢弃的信息，然后加上it*Ct，变成新的候选值。</li>
</ul>
<p><img src="https://mmbiz.qpic.cn/mmbiz_png/ZWiblltaIVJqHqcPibzEJzDL6xtC58zFJzlpl41y2DKdCFPtRE4sl5ZX0Fq6RQRHdJ3lEAXCNoDxJjHoNiceIHZbw/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="img"></p>
<p><strong>最后一步，确定需要输出什么值</strong>。这个输出将会基于我们的细胞状态，但是也是一个过滤后的版本。首先是，运行一个sigmoid层来确定细胞状态的哪个部分将被输出。然后，把细胞状态通过tanh进行处理，得到一个-1~1之间的值，将它和sigmoid门的输出相乘。</p>
<p><img src="https://mmbiz.qpic.cn/mmbiz_png/ZWiblltaIVJqHqcPibzEJzDL6xtC58zFJz0w811ujoicJszFKicnp3bxIwbm9p1mhEBuicWYRouLIDZGRqvfswmw6Kg/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="img"></p>
<p><strong>四、LSTM的变体</strong></p>
<ul>
<li><p><strong>让门层也接受细胞状态的输入</strong></p>
<p><img src="https://mmbiz.qpic.cn/mmbiz_png/ZWiblltaIVJqHqcPibzEJzDL6xtC58zFJzLDfZCqTAp0pcKgRB5l1RBASld7Mc6rKthmdsWfnHZTxcTiaMXx93eGg/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="img"></p>
</li>
<li><p><strong>使用coupled忘记和输入门</strong></p>
<p>区别于标准LSTM的分开确定忘记什么和需要添加什么新的信息，变体的LSTM是两者一同做出决定。</p>
</li>
</ul>
<p><img src="https://mmbiz.qpic.cn/mmbiz_png/ZWiblltaIVJqHqcPibzEJzDL6xtC58zFJzl2HXYUQWFDa4WTZ5mCaLtV5bibpUzibgsm10RKFeBh1bq9K40jXTjgqA/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="img"></p>
<ul>
<li><strong>Gated Recurrent Unit(GRU)变体</strong>，是将忘记门和输入门合成了一个单一的更新门。还混合了细胞状态和隐藏状态的，也是非常流行的变体</li>
</ul>
<p><img src="https://mmbiz.qpic.cn/mmbiz_png/ZWiblltaIVJqHqcPibzEJzDL6xtC58zFJz2mpfWXDSDAgLRauNTLkPDd0E89OlvzbXShtcELbJibrN7CgKFKJo6Yw/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="img"></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/12/16/RNN-Recurrent-Neural-Network-循环神经网络/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Victor Drq">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/header.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="drqblog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/12/16/RNN-Recurrent-Neural-Network-循环神经网络/" itemprop="url">RNN(Recurrent Neural Network)循环神经网络</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-12-16T12:50:26+08:00">
                2018-12-16
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/categories/TensorFlow/" itemprop="url" rel="index">
                    <span itemprop="name">TensorFlow</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">字数统计&#58;</span>
                
                <span title="字数统计">
                  1.4k
                </span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                
                <span title="阅读时长">
                  4
                </span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="RNN-Recurrent-Neural-Network-循环神经网络"><a href="#RNN-Recurrent-Neural-Network-循环神经网络" class="headerlink" title="RNN(Recurrent Neural Network)循环神经网络"></a>RNN(Recurrent Neural Network)循环神经网络</h2><p><strong>RNN(Recurrent Neural Network)是一类用于处理序列数据的神经网络</strong>。序列数据：时间序列数据，也就是在不同时间点上收集到的数据，这类数据反映了某一事物、现象等随着时间变化的状态或者程度。也不一定是时间序列，也可以是文本序列。总之：后面的数据跟前面的数据是有关系，可以将RNN看做全连接网络</p>
<p><strong>一、RNN的结构</strong></p>
<p>​        普通神经网络包含input layer，hidden layer，output layer，通过Activation Function来控制输出。layer与layer通过weights连接。</p>
<ul>
<li><strong>RNN的标准结构</strong>：</li>
</ul>
<p><img src="https://mmbiz.qpic.cn/mmbiz_png/ZWiblltaIVJqHqcPibzEJzDL6xtC58zFJz2ARrr4xTL3gSbQbhia3UvglxuY7FYwibzS6W7mjibbcfTfibdGZmITkK1Q/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="img"></p>
<ul>
<li><strong>普通神经网络结构</strong>：</li>
</ul>
<p><img src="https://mmbiz.qpic.cn/mmbiz_png/ZWiblltaIVJrkjx6u84WjxLvC7yJLxrSeypZvBiaCCJkz1RaFYTmXiciczD6EFwURtpLkVhXs15icGwPWEwJib5smp9Q/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="img"></p>
<ul>
<li><p><strong>两者区别</strong>：</p>
<p>基础的神经网络只在层与层之间建立了权Weights连接，RNN在层之间的神经元之间也建立了权连接</p>
</li>
<li><p><strong>实际中，RNN标准结构并不能解决所有问题：</strong></p>
</li>
</ul>
<ol>
<li><strong>输入为一串文字，输出为分类类别，那么输出就不需要一个序列，只要单个输出：</strong></li>
</ol>
<p>   <img src="https://mmbiz.qpic.cn/mmbiz_jpg/ZWiblltaIVJqHqcPibzEJzDL6xtC58zFJzbxE36EJWlxvFOEiafZKx7zBIpU90D5kFwAOxU8c5QG6U50tv5BqHbUA/640?wx_fmt=jpeg&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="img"></p>
<ol start="2">
<li><strong>有时候需要单个输入但是输出为序列的情况的RNN结构：</strong></li>
</ol>
<p>   <img src="https://mmbiz.qpic.cn/mmbiz_jpg/ZWiblltaIVJqHqcPibzEJzDL6xtC58zFJzoxpv5hQGSbWm8dne0gaqqINicdcpnDjY4mibacJPVPOibFuoJpIWw1N2w/640?wx_fmt=jpeg&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="img"></p>
<ol start="3">
<li><p><strong>有时候输入时序列，但是不随着序列变化：</strong></p>
<p><img src="https://mmbiz.qpic.cn/mmbiz_jpg/ZWiblltaIVJqHqcPibzEJzDL6xtC58zFJzbHGayQHWEvX3AuDVcwQia6LX9S4GaQCkw4oJ5n4goYZnqKdjVCVM9cg/640?wx_fmt=jpeg&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="img"></p>
</li>
<li><p><strong>实际中，大部分问题序列都是不等长的。</strong>比如：自然语言处理中，源语言和目标语言的句子往往长度是不同的。就需要Encoder-Decoder模型，也叫Seq2Seq模型。</p>
<p><img src="https://mmbiz.qpic.cn/mmbiz_jpg/ZWiblltaIVJqHqcPibzEJzDL6xtC58zFJz9EuMk7iclkYo0zkvuGiaK8Q8nqTchcicbgNPqBHAaeOLRwrRnX2D5CQaA/640?wx_fmt=jpeg&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="img"></p>
<p>Encoder-Decoder模型结构原理：先编码后解码。左侧的RNN用来编码得到c，然后再用右侧的RNN把c解码。</p>
</li>
</ol>
<p><strong>二、标准RNN的流程</strong></p>
<p>标准的RNN采用的是前向传播过程：</p>
<p><img src="https://mmbiz.qpic.cn/mmbiz_png/ZWiblltaIVJqHqcPibzEJzDL6xtC58zFJzhGInmNumyib3KbPLKzAicGLcLxur0yT0HPJWMuBR8so7YuNdiaBrf3icfA/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="img"></p>
<p>图中的：x为输入input，h为hidden layer单元，o为输出output，L为loss损失函数，y为training set训练集，右上角小标号代表t时刻状态。W，U，V代表权值Weights。</p>
<ul>
<li><strong>前向传播算法公式：</strong><img src="https://mmbiz.qpic.cn/mmbiz_png/ZWiblltaIVJqHqcPibzEJzDL6xtC58zFJzYMAoaGDy7OeibFhz5LEOam7JpB1z30zmLibNHZdARfWC7icIHLpGmaic9w/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="img"></li>
</ul>
<p>​       φ为激活函数，一般选择是tanh函数，b为biases偏置</p>
<ul>
<li><p><strong>t时刻的输出o：</strong><img src="https://mmbiz.qpic.cn/mmbiz_png/ZWiblltaIVJqHqcPibzEJzDL6xtC58zFJz8NQkCgAdAwqy4VKra14dnBEw9eISgp5LeAyEBiaBcULTzGUgicn9x9UQ/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="img"></p>
</li>
<li><p><strong>预测输出为：</strong></p>
<p><img src="https://mmbiz.qpic.cn/mmbiz_png/ZWiblltaIVJqHqcPibzEJzDL6xtC58zFJzWXQHNYU2AzVzz3bArpHyF7Zn0keYJYOicicjjxBBBlD3DpwZLDMjHXug/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="img"></p>
</li>
</ul>
<p>​      其中δ为激活函数，RNN常用语分类，这里一般用softmax函数</p>
<p><strong>三、RNN的训练方法(Back Propagation Through Time,BPTT)</strong></p>
<p>BPTT用来RNN训练。它的本质是BP算法，只是加上了时间。因为RNN处理时间序列数据，要基于时间反向传播。</p>
<p><strong>核心思想</strong>：BPTT和BP算法相同，都沿着需要优化参数的负梯度方向不断寻找更优的点直到收敛。也就是梯度下降法。</p>
<p><img src="https://mmbiz.qpic.cn/mmbiz_png/ZWiblltaIVJqHqcPibzEJzDL6xtC58zFJzhGInmNumyib3KbPLKzAicGLcLxur0yT0HPJWMuBR8so7YuNdiaBrf3icfA/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="img"></p>
<p>需要寻找最优的有三个参数：U，V，W。</p>
<p>U,W两个参数的寻找最优的过程需要用到历史数据。而，V只关注当前h（hidden layer）的数据。</p>
<ul>
<li><p><strong>求V的偏导数**</strong>：**</p>
<p>也即是L(Loss)对V求偏导，V到L还经过o(output)，里面有激活函数。所以是复合函数求导过程。</p>
<p><img src="https://mmbiz.qpic.cn/mmbiz_png/ZWiblltaIVJqHqcPibzEJzDL6xtC58zFJzcsUM3dRbxkYPz27JpLWhRmydyNnkibkegYUxLYbBOSicr39a7qaOl4SA/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="img"></p>
<p>因为RNN的L(Loss)的损失是随着时间累加的，所以叠加后的结果如上图。</p>
</li>
<li><p><strong>求W的偏导数：</strong></p>
<p>W偏导求解需要用到历史数据，假设我们只有三个时刻，假设第三个时刻L对W的偏导数为：</p>
<p><img src="https://mmbiz.qpic.cn/mmbiz_jpg/ZWiblltaIVJqHqcPibzEJzDL6xtC58zFJzdnIDWhxhAnPo8W1VFT2ianmJdFZU4fvhj2jia4GhUkLoP0y9k5bexHSg/640?wx_fmt=jpeg&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="img"></p>
</li>
<li><p><strong>求U的偏导数：</strong></p>
<p>U偏导求解需要用到历史数据，假设我们只有三个时刻，假设第三个时刻L对U的偏导数为：</p>
<p><img src="https://mmbiz.qpic.cn/mmbiz_jpg/ZWiblltaIVJqHqcPibzEJzDL6xtC58zFJzAgib52SUbfeibySwjyqP6hrzhibUajefazLb56uJAXb1a5EnlXibMrxxdQ/640?wx_fmt=jpeg&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="img"></p>
</li>
<li><p><strong>上面只是对某个时刻的W和U求的偏导数</strong>，但是RNN的L(Loss)损失是随着时间累加的，要追溯到历史数据，那么整个损失函数L对W，U的偏导数十分复杂的。通过找规律发现：</p>
<p><img src="https://mmbiz.qpic.cn/mmbiz_png/ZWiblltaIVJqHqcPibzEJzDL6xtC58zFJzhlmxorq4HrDum3NAJL7jByQmSzHQqib7ScZum0d1ABiaPL7eSHzDIz5A/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="img"></p>
</li>
<li><p><strong>根据RNN图得知，Activation Function激活函数</strong>是嵌套在对h(hidden)的偏导里面的，把中间累乘部分替换为tanh或者sigmoid写法为：</p>
<p><img src="https://mmbiz.qpic.cn/mmbiz_jpg/ZWiblltaIVJqHqcPibzEJzDL6xtC58zFJzib0d7RbiaIXw5hQEV7No50ibaezBCenG7MoMZkpnl5t5elnoiceejJDR0A/640?wx_fmt=jpeg&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="img"></p>
</li>
<li><p>上面的累乘会导致激活函数的累乘，会导致“<strong>梯度消失</strong>”和”<strong>梯度爆炸</strong>“现象。</p>
<p><strong>1、sigmoid函数（logistics函数）和导数：</strong></p>
<p><img src="https://mmbiz.qpic.cn/mmbiz_png/ZWiblltaIVJqHqcPibzEJzDL6xtC58zFJzLsCsLwO114f2ibjtxzBU6JFsN2D4WlWHafHeeycP6IdX6v0vZpDzGTA/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="img"></p>
<p><strong>结论</strong>：使用sigmoid函数作为激活函数，肯定是累乘的时候结果越来越小，随着时间推移小数的累乘导致梯度小到接近于0，这就是“梯度消失”。梯度消失会导致，那一层的参数再也不会更新，那么那一层隐藏层就变成了单纯的映射层，就毫无意义了。</p>
<p><img src="https://mmbiz.qpic.cn/mmbiz_png/ZWiblltaIVJqHqcPibzEJzDL6xtC58zFJzUtkwBlHzgE4UVqJRpH6xic4nnPqMAJScpAYibZicWTEvoIk9DH5Lloqvg/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="img"></p>
<p><strong>总结</strong>：sigmoid函数(logisitic函数)不是0中心对称</p>
<p><strong>2、tanh函数和导数：</strong></p>
<p><img src="https://mmbiz.qpic.cn/mmbiz_png/ZWiblltaIVJqHqcPibzEJzDL6xtC58zFJzAgAAcl0BPC4THTIsMnTPVIfZVhdz9feTsHZFXZ7H1QUDV3eq8zYibfQ/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="img"></p>
<p><strong>总结</strong>：tanh函数是中心对称，会导致神经网络的收敛性更好，是tanh函数相对于sigmoid函数来说梯度较大，收敛速度更快且引起梯度消失更慢</p>
</li>
<li><p>解决梯度消失的方法：</p>
<p><strong>1、选取其他激活函数</strong></p>
<p>一般选用ReLU作为激活函数</p>
<p><img src="https://mmbiz.qpic.cn/mmbiz_png/ZWiblltaIVJqHqcPibzEJzDL6xtC58zFJzKgAhFEfBZ2U5Ean6crG1VSCboPnO69JnkHqm4ia7Un4eX2uic1gibQ7UA/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="img"></p>
<p><strong>ReLU的导数：</strong></p>
<p><img src="https://mmbiz.qpic.cn/mmbiz_png/ZWiblltaIVJqHqcPibzEJzDL6xtC58zFJzic1NovT9A91arI5ovvoVj2icL1UJsJNYySfCfTiatkCqJJ7zic8zIiapMzg/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="img"></p>
<p><strong>总结</strong>：ReLU函数的左导数为0，右导数为1，避免了“梯度消失”，然而右导数为1，会导致“梯度爆炸”，但是可以设定合适的阈值可以解决“梯度爆炸”问题。还有一点就是，左导数恒为0，有可能导致把神经元学死，同样设置合适的步长（training_step）可以避免问题发生。</p>
<p><strong>2、改变传播结构</strong></p>
<p>改变传播结构，也就是引入<strong>LSTM(Long-Short Term Memory)，长短期记忆网络</strong>。是一种时间递归神经网络。适合处理和预测时间序列中间隔和延迟相对较长的重要时间。LSTM区别于RNN的地方，就是在算法中加了一个判断信息有用与否的处理器Cell。</p>
</li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/12/15/利用CNN识别图片/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Victor Drq">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/header.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="drqblog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/12/15/利用CNN识别图片/" itemprop="url">利用CNN识别图片</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-12-15T19:53:24+08:00">
                2018-12-15
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/categories/TensorFlow/" itemprop="url" rel="index">
                    <span itemprop="name">TensorFlow</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">字数统计&#58;</span>
                
                <span title="字数统计">
                  676
                </span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                
                <span title="阅读时长">
                  4
                </span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>具体CNN的原理以及应用请看上一篇文章。废话不多说，直接上代码，看运行效果</p>
<h2 id="利用CNN识别image的源代码"><a href="#利用CNN识别image的源代码" class="headerlink" title="利用CNN识别image的源代码"></a>利用CNN识别image的源代码</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -- coding: utf-8 --</span></span><br><span class="line"></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">@author: victor</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Convolutional Neural Network Example</span></span><br><span class="line"><span class="string">Build a convolutional neural network with Tensorflow</span></span><br><span class="line"><span class="string">This example is using TensorFlow layers API</span></span><br><span class="line"><span class="string">see 'convolutional_network_raw' example for a raw TensorFlow</span></span><br><span class="line"><span class="string">implementation with variables</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#from future import division,print_function,absolute_import</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#Import MNIST data</span></span><br><span class="line"><span class="keyword">from</span> tensorflow.examples.tutorials.mnist <span class="keyword">import</span> input_data</span><br><span class="line">mnist=input_data.read_data_sets(<span class="string">"MNIST_data/"</span>,one_hot=<span class="keyword">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#import module</span></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment">#Training parameters</span></span><br><span class="line">learning_rate=<span class="number">0.001</span></span><br><span class="line">num_steps=<span class="number">1000</span></span><br><span class="line">batch_size=<span class="number">128</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#Network parameters</span></span><br><span class="line">num_input=<span class="number">784</span><span class="comment">#MNIST data input(img shape:28*28)</span></span><br><span class="line">num_classes=<span class="number">10</span><span class="comment">#MNIST total classes(0-9 digits)</span></span><br><span class="line">dropout=<span class="number">0.25</span><span class="comment">#Dropout,probability to drop a unit</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#Create the neural network</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">conv_net</span><span class="params">(x_dict,n_classes,dropout,reuse,is_training)</span>:</span></span><br><span class="line">  </span><br><span class="line">    <span class="comment">#Define a scope for reusing the variables</span></span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">'ConvNet'</span>,reuse=reuse):</span><br><span class="line">        <span class="comment">#tf Estimator input is a dict,in case of multiple inputs</span></span><br><span class="line">        x=x_dict[<span class="string">'images'</span>]</span><br><span class="line"></span><br><span class="line">        </span><br><span class="line">        <span class="comment">#MNIST data input is a 1-D vector of 784 features(28*28 pixels)</span></span><br><span class="line">        <span class="comment">#Reshape to match picture format [Height x Width x Channel]</span></span><br><span class="line">        <span class="comment">#Tensor input become 4-D:[Batch Size,Height,Width,Channel]</span></span><br><span class="line">        x=tf.reshape(x,shape=[<span class="number">-1</span>,<span class="number">28</span>,<span class="number">28</span>,<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">      </span><br><span class="line">        <span class="comment">#Convolution Layer with 32 filters and a kernel size of 5</span></span><br><span class="line">        conv1=tf.layers.conv2d(x,<span class="number">32</span>,<span class="number">5</span>,activation=tf.nn.relu)</span><br><span class="line"></span><br><span class="line">        <span class="comment">#Max Pooling(down-sampling) with strides of 2 and kernel size of 2</span></span><br><span class="line">        conv1=tf.layers.max_pooling2d(conv1,<span class="number">2</span>,<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        </span><br><span class="line"></span><br><span class="line">        <span class="comment">#Convolution Layer with 64 filters and a kernel size of 3</span></span><br><span class="line">        conv2=tf.layers.conv2d(conv1,<span class="number">64</span>,<span class="number">3</span>,activation=tf.nn.relu)</span><br><span class="line"></span><br><span class="line">        <span class="comment">#Max Pooling(down-sampling) with strides of 2 and kernel size of 2</span></span><br><span class="line">        conv2=tf.layers.average_pooling2d(conv2,<span class="number">2</span>,<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        </span><br><span class="line">        <span class="comment">#Flatten the data to a 1-D vector for the fully connected layer</span></span><br><span class="line">        fc1=tf.contrib.layers.flatten(conv2)</span><br><span class="line"></span><br><span class="line">        </span><br><span class="line"></span><br><span class="line">        <span class="comment">#Fully connected layer(in tf contrib folder for now)</span></span><br><span class="line">        fc1=tf.layers.dense(fc1,<span class="number">1024</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment">#Apply Dropout(if is_training is False,dropout is not applied)</span></span><br><span class="line">        fc1=tf.layers.dropout(fc1,rate=dropout,training=is_training)</span><br><span class="line"></span><br><span class="line">        <span class="comment">#Output layer,class prediction</span></span><br><span class="line">        out=tf.layers.dense(fc1,n_classes)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line"><span class="comment">#Define the model function(following Tf Estimator Template)</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">model_fn</span><span class="params">(features,labels,mode)</span>:</span></span><br><span class="line">  </span><br><span class="line">    <span class="comment">#Build the neural network</span></span><br><span class="line">    <span class="comment">#Because Dropout have different behavior at training and prediction time</span></span><br><span class="line">    <span class="comment">#we need to create 2 distinct computation graphs that still share the same weights</span></span><br><span class="line">    logits_train=conv_net(features,num_classes,dropout,reuse=<span class="keyword">False</span>,is_training=<span class="keyword">True</span>)</span><br><span class="line">    logits_test=conv_net(features,num_classes,dropout,reuse=<span class="keyword">True</span>,is_training=<span class="keyword">False</span>)</span><br><span class="line"></span><br><span class="line">    </span><br><span class="line"></span><br><span class="line">    <span class="comment">#Predictions</span></span><br><span class="line">    pred_classes=tf.argmax(logits_test,axis=<span class="number">1</span>)</span><br><span class="line">    pred_probas=tf.nn.softmax(logits_test)</span><br><span class="line"></span><br><span class="line">   </span><br><span class="line">    <span class="comment">#If prediction mode,early return </span></span><br><span class="line">    <span class="keyword">if</span> mode==tf.estimator.ModeKeys.PREDICT:</span><br><span class="line">        <span class="keyword">return</span> tf.estimator.EstimatorSpec(mode,predictions=pred_classes)</span><br><span class="line"></span><br><span class="line">      </span><br><span class="line">    <span class="comment">#Define loss and optimizer</span></span><br><span class="line">    loss_op=tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(</span><br><span class="line">         logits=logits_train,labels=tf.cast(labels,dtype=tf.int32)))</span><br><span class="line"></span><br><span class="line">    optimizer=tf.train.AdamOptimizer(learning_rate=learning_rate)</span><br><span class="line">    train_op=optimizer.minimize(loss_op,global_step=tf.train.get_global_step())</span><br><span class="line"></span><br><span class="line">    </span><br><span class="line">    <span class="comment">#Evaluate the accuracy of the model</span></span><br><span class="line">    acc_op=tf.metrics.accuracy(labels=labels,predictions=pred_classes)</span><br><span class="line"></span><br><span class="line">    </span><br><span class="line">    <span class="comment">#TF Estimators requires to return a EstimatorSpec,that specify</span></span><br><span class="line">    <span class="comment">#the different ops for training,evaluating,...</span></span><br><span class="line">    estim_specs=tf.estimator.EstimatorSpec(</span><br><span class="line">                mode=mode,</span><br><span class="line">                predictions=pred_classes,</span><br><span class="line">                loss=loss_op,</span><br><span class="line">                train_op=train_op,</span><br><span class="line">                eval_metric_ops=&#123;<span class="string">'accuracy'</span>:acc_op&#125;)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> estim_specs </span><br><span class="line"></span><br><span class="line">   </span><br><span class="line"><span class="comment">#Build the Estimator</span></span><br><span class="line">model=tf.estimator.Estimator(model_fn)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#Define the input function for training</span></span><br><span class="line">input_fn=tf.estimator.inputs.numpy_input_fn(</span><br><span class="line">         x=&#123;<span class="string">'images'</span>:mnist.train.images&#125;,</span><br><span class="line">         y=mnist.train.labels,</span><br><span class="line">         batch_size=batch_size,</span><br><span class="line">         num_epochs=<span class="keyword">None</span>,</span><br><span class="line">         shuffle=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#Train the Model</span></span><br><span class="line">model.train(input_fn,steps=num_steps)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#Predict single images</span></span><br><span class="line">n_images=<span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#Get images from test set</span></span><br><span class="line">test_images=mnist.test.images[:n_images]</span><br><span class="line"></span><br><span class="line"><span class="comment">#Prepare the input data</span></span><br><span class="line">input_fn=tf.estimator.inputs.numpy_input_fn(</span><br><span class="line">         x=&#123;<span class="string">'images'</span>:test_images&#125;,shuffle=<span class="keyword">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#Use the model to predict the images class</span></span><br><span class="line">preds=list(model.predict(input_fn))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#Display</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(n_images):</span><br><span class="line">    plt.imshow(np.reshape(test_images[i],[<span class="number">28</span>,<span class="number">28</span>]),cmap=<span class="string">'gray'</span>)</span><br><span class="line">    plt.show()</span><br><span class="line">    print(<span class="string">'Model prediction:'</span>,preds[i])</span><br><span class="line">    plt.xlabel(<span class="string">'Model prediction:'</span>+str(preds[i]),fontsize=<span class="number">14</span>)</span><br><span class="line">    plt.pause(<span class="number">0.5</span>)</span><br></pre></td></tr></table></figure>
<h2 id="运行效果"><a href="#运行效果" class="headerlink" title="运行效果"></a>运行效果</h2><p><img src="/2018/12/15/利用CNN识别图片/1.gif" alt="cnn识别图片"></p>
<h2 id="查看Tensor-board上的效果"><a href="#查看Tensor-board上的效果" class="headerlink" title="查看Tensor board上的效果"></a>查看Tensor board上的效果</h2><ul>
<li><p>Tensorboard的操作</p>
<p><img src="/2018/12/15/利用CNN识别图片/3.gif" alt="tensorboard"></p>
</li>
<li><p>利用Google Chrome查看图形化差异</p>
</li>
</ul>
<p><img src="/2018/12/15/利用CNN识别图片/2.gif" alt="loss"></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/12/15/CNN的入门demo/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Victor Drq">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/header.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="drqblog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/12/15/CNN的入门demo/" itemprop="url">CNN的入门demo</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-12-15T19:31:32+08:00">
                2018-12-15
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/categories/TensorFlow/" itemprop="url" rel="index">
                    <span itemprop="name">TensorFlow</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">字数统计&#58;</span>
                
                <span title="字数统计">
                  1.1k
                </span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                
                <span title="阅读时长">
                  5
                </span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="一、CNN的源代码"><a href="#一、CNN的源代码" class="headerlink" title="一、CNN的源代码"></a>一、CNN的源代码</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#author:victor</span></span><br><span class="line"><span class="comment">#import module</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.examples.tutorials.mnist <span class="keyword">import</span> input_data</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#input MNIST_data</span></span><br><span class="line"></span><br><span class="line">mnist = input_data.read_data_sets(<span class="string">"MNIST_data/"</span>, one_hot=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line">sess = tf.InteractiveSession()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#define placeholder for inputs to network</span></span><br><span class="line">x = tf.placeholder(tf.float32, shape=[<span class="keyword">None</span>, <span class="number">784</span>])<span class="comment">#28*28</span></span><br><span class="line">y_ = tf.placeholder(tf.float32, shape=[<span class="keyword">None</span>, <span class="number">10</span>])<span class="comment">#0~9共10个数字</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#define weights(神经元的权重)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">weight_variable</span><span class="params">(shape)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">#tf.truncated_normal(shape,mean,stddev)</span></span><br><span class="line">    <span class="comment">#shape:表示生成张量的维度</span></span><br><span class="line">    <span class="comment">#mean:表示均值</span></span><br><span class="line">    <span class="comment">#stddev：表示标准差</span></span><br><span class="line">    <span class="comment">#这是一个截断产生正太分布的函数</span></span><br><span class="line">    <span class="comment">#tf.truncated_normal与tf.random_normal的区别是：</span></span><br><span class="line">    <span class="comment">#这两个输入参数几乎完全一致，都是正态分布产生函数</span></span><br><span class="line">    <span class="comment">#tf.truncated_normal截断的标准差是2倍的stddev</span></span><br><span class="line">    initial = tf.truncated_normal(shape, stddev=<span class="number">0.1</span>)</span><br><span class="line">    <span class="keyword">return</span> tf.Variable(initial)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#define biases(神经元的偏置常量)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">bias_variable</span><span class="params">(shape)</span>:</span></span><br><span class="line">    initial = tf.constant(<span class="number">0.1</span>, shape=shape)</span><br><span class="line">    <span class="keyword">return</span> tf.Variable(initial)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#define conv（卷积层）</span></span><br><span class="line"><span class="comment">#padding有两个值，一个SAME,一个VALID</span></span><br><span class="line"><span class="comment">#padding设置为SAME：说明输入图片和输出图片大小一致</span></span><br><span class="line"><span class="comment">#padding设置为VALID：说明图片经过滤波器filter后可能会变小</span></span><br><span class="line"><span class="comment">#设置conv的滑动步长strides为1，1，1，1</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#define convolutional layer</span></span><br><span class="line"><span class="comment">#x:x为image的所有信息</span></span><br><span class="line"><span class="comment">#W:Weight</span></span><br><span class="line"><span class="comment">#strides的前后都为1，然后第二个，第三个，表示，x方向，y方向都为1</span></span><br><span class="line"><span class="comment">#strides=[1,x_movement,y_movement,1]</span></span><br><span class="line"><span class="comment">#must have strides[0]=strides[3]=1</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">conv2d</span><span class="params">(x, W)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> tf.nn.conv2d(x, W, strides=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], padding=<span class="string">"SAME"</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#define pooling（池化层）</span></span><br><span class="line"><span class="comment">##pooling的方式是max pooling</span></span><br><span class="line"><span class="comment">#设置它的滑动步长strides为1，2，2，1</span></span><br><span class="line"><span class="comment">#use max_pool method</span></span><br><span class="line"><span class="comment">#ksize：也就是kernel size</span></span><br><span class="line"><span class="comment">#strides=[1,x_movement,y_movement,1]</span></span><br><span class="line"><span class="comment">#strides[1]=strides[2]=2,也就是隔2个像素移动一下</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">max_pool_2x2</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> tf.nn.max_pool(x, ksize=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>], strides=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>], padding=<span class="string">"SAME"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">##conv1 layer卷积层##</span></span><br><span class="line"><span class="comment">#W_conv1就是Weights</span></span><br><span class="line"><span class="comment">#patch 5*5,in size=1是image的厚度,out size=32</span></span><br><span class="line">W_conv1 = weight_variable([<span class="number">5</span>, <span class="number">5</span>, <span class="number">1</span>, <span class="number">32</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment">#b_conv1就是biases，32就是卷积核的个数，按照经验取值</span></span><br><span class="line">b_conv1 = bias_variable([<span class="number">32</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment">#-1：是把数据扁平化，28*28就是所有像素点784，1由于这个MNIST里的图片全都是黑白的所有只有1，如果是彩色的就可以有其他的值</span></span><br><span class="line">x_image = tf.reshape(x, [<span class="number">-1</span>, <span class="number">28</span>, <span class="number">28</span>, <span class="number">1</span>])</span><br><span class="line"><span class="comment">#print(x_image,shape)#[n_samples,28,28,1]</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#非线性化处理</span></span><br><span class="line">h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)<span class="comment">#output size 282832之前的步长是1，1</span></span><br><span class="line"></span><br><span class="line"><span class="comment">##pooling layer池化层##</span></span><br><span class="line">h_pool1 = max_pool_2x2(h_conv1)<span class="comment">#output size 141432，pooling的步长是2，2，则就是原来基础上除以2</span></span><br><span class="line"><span class="comment">##conv1 layer##</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#conv2 layer卷积层##</span></span><br><span class="line">W_conv2 = weight_variable([<span class="number">5</span>, <span class="number">5</span>, <span class="number">32</span>, <span class="number">64</span>])<span class="comment">#patch 5*5,in size 32,out size 64</span></span><br><span class="line">b_conv2 = bias_variable([<span class="number">64</span>])</span><br><span class="line">h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)<span class="comment">#output size 141464,第二层的输入是第一层的输出，输出的pooling是14*14，步长是1，1</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">##pooling layer池化层##</span></span><br><span class="line">h_pool2 = max_pool_2x2(h_conv2)<span class="comment">#output size然后步长是2，2，所以就是7764</span></span><br><span class="line"><span class="comment">##conv2 layer##</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">##func1 layer全连接层##</span></span><br><span class="line">W_fc1 = weight_variable([<span class="number">7</span> * <span class="number">7</span> * <span class="number">64</span>, <span class="number">1024</span>])</span><br><span class="line">b_fc1 = bias_variable([<span class="number">1024</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment">##转换##</span></span><br><span class="line">h_pool2_flat = tf.reshape(h_pool2, [<span class="number">-1</span>, <span class="number">7</span> * <span class="number">7</span> * <span class="number">64</span>])<span class="comment">#[n_samples,7,7,64]转换为[n_samples,7764]</span></span><br><span class="line"></span><br><span class="line">h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)</span><br><span class="line"></span><br><span class="line"><span class="comment">#define keep_prob</span></span><br><span class="line">keep_prob = tf.placeholder(<span class="string">"float"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#use dropout solve overfitting(使用dropout防止过拟合)keep_prob为0~1之间数</span></span><br><span class="line">h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)<span class="comment">#考虑到有overfitting，加一个dropout处理</span></span><br><span class="line"></span><br><span class="line"><span class="comment">##func1 layer##</span></span><br><span class="line"><span class="comment">##func2 layer全连接层##</span></span><br><span class="line"><span class="comment">#第二层的input=1024是第一层的输出1024，第二层的输出为10因为是有0~9，10个数字</span></span><br><span class="line"></span><br><span class="line">W_fc2 = weight_variable([<span class="number">1024</span>, <span class="number">10</span>])</span><br><span class="line">b_fc2 = bias_variable([<span class="number">10</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment">#prediction</span></span><br><span class="line">y_conv = tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)</span><br><span class="line"></span><br><span class="line"><span class="comment">#use cross_entropy交叉熵</span></span><br><span class="line"><span class="comment">#the error between prediction and real data</span></span><br><span class="line"></span><br><span class="line">cross_entropy = -tf.reduce_sum(y_ * tf.log(y_conv))</span><br><span class="line"></span><br><span class="line"><span class="comment">#对于庞大的神经网络使用AdamOptimizer不适用GradientDescentOptimizer了train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)</span></span><br><span class="line">correct_prediction = tf.equal(tf.argmax(y_conv, <span class="number">1</span>), tf.argmax(y_, <span class="number">1</span>))</span><br><span class="line">accuracy = tf.reduce_mean(tf.cast(correct_prediction, <span class="string">"float"</span>))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#import step</span></span><br><span class="line">sess.run(tf.global_variables_initializer())</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">20000</span>):</span><br><span class="line">    batch = mnist.train.next_batch(<span class="number">50</span>)</span><br><span class="line">    <span class="keyword">if</span> i % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line"></span><br><span class="line">        <span class="comment">#控制它的keep_prob为1.0也就是所有元素全部保留</span></span><br><span class="line">        train_accuracy = accuracy.eval(feed_dict=&#123;</span><br><span class="line">            x: batch[<span class="number">0</span>], y: batch[<span class="number">1</span>], keep_prob: <span class="number">1.0</span>&#125;)</span><br><span class="line">        print(<span class="string">"step %d, training accuracy %g"</span> % (i, train_accuracy))</span><br><span class="line">    train_step.run(feed_dict=&#123;x: batch[<span class="number">0</span>], y: batch[<span class="number">1</span>], keep_prob: <span class="number">0.5</span>&#125;)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"test accuracy %g"</span> % accuracy.eval(feed_dict=&#123;</span><br><span class="line">    x: mnist.test.images, y_: mnist.test.labels, keep_prob: <span class="number">1.0</span>&#125;))</span><br></pre></td></tr></table></figure>
<p><strong>二、运行效果</strong></p>
<ul>
<li>刚开始训练精度并不高，在90%左右</li>
</ul>
<p><img src="/2018/12/15/CNN的入门demo/1.png" alt="start training"></p>
<ul>
<li>随着慢慢的训练5000步左右的时候，精度逐渐增加到99%左右</li>
</ul>
<p><img src="/2018/12/15/CNN的入门demo/2.png" alt="cnn"></p>
<ul>
<li>等过万的时候，精确度已经很高了，接近于100%</li>
</ul>
<p><img src="/2018/12/15/CNN的入门demo/3.png" alt="cnn"></p>
<ul>
<li><p>训练到15000步，精确度已经很高，几乎100%</p>
<p><img src="/2018/12/15/CNN的入门demo/4.png" alt="cnn"></p>
</li>
<li><p>训练结束后的结果</p>
<p><img src="/2018/12/15/CNN的入门demo/5.png" alt="cnn"></p>
<p>对比上一节的MNIST入门的Demo利用GradientDescentOptimizer直接进行训练，利用CNN训练，精确度基本上99%</p>
<p>ps：由于设置的循环range为20000，训练次数比较大，跑起来比较耗时，我安装的是CPU版本的Tensorflow，跑了大概1个小时训练结束。</p>
</li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/"><i class="fa fa-angle-left"></i></a><a class="page-number" href="/">1</a><span class="page-number current">2</span><a class="page-number" href="/page/3/">3</a><span class="space">&hellip;</span><a class="page-number" href="/page/7/">7</a><a class="extend next" rel="next" href="/page/3/"><i class="fa fa-angle-right"></i></a>
  </nav>



          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope="" itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image" src="/images/header.jpg" alt="Victor Drq">
            
              <p class="site-author-name" itemprop="name">Victor Drq</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">70</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">8</span>
                  <span class="site-state-item-name">分类</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">1</span>
                  <span class="site-state-item-name">标签</span>
                </a>
              </div>
            

          </nav>

          
            <div class="feed-link motion-element">
              <a href="/atom.xml" rel="alternate">
                <i class="fa fa-rss"></i>
                RSS
              </a>
            </div>
          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="https://github.com/AnonymousDQ" target="_blank" title="GitHub">
                      
                        <i class="fa fa-fw fa-github"></i>GitHub</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="mailto:1397743321@qq.com" target="_blank" title="E-Mail">
                      
                        <i class="fa fa-fw fa-envelope"></i>E-Mail</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="https://www.cnblogs.com/drq1/" target="_blank" title="博客园">
                      
                        <i class="fa fa-fw fa-globe"></i>博客园</a>
                  </span>
                
            </div>
          

          
          

          
          

          

        </div>
      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>
<div class="copyright">&copy; <span itemprop="copyrightYear">2018</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Victor Drq</span>

  
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-area-chart"></i>
    </span>
    
      <span class="post-meta-item-text">Site words total count&#58;</span>
    
    <span title="Site words total count">62k</span>
  
</div>
<div class="powered-by">
<i class="fa fa-user-md"></i><span id="busuanzi_container_site_uv">
  本站访客数:<span id="busuanzi_value_site_uv"></span>
</span>
</div>

  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Pisces</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  

  
  
  
  <link rel="stylesheet" href="/lib/needsharebutton/needsharebutton.css">

  
  
  <script src="/lib/needsharebutton/needsharebutton.js"></script>

  <script>
    
      pbOptions = {};
      
          pbOptions.iconStyle = "box";
      
          pbOptions.boxForm = "horizontal";
      
          pbOptions.position = "bottomCenter";
      
          pbOptions.networks = "Weibo,Wechat,Douban,QQZone,Twitter,Facebook";
      
      new needShareButton('#needsharebutton-postbottom', pbOptions);
    
    
  </script>

  

  

  

  

<script src="/live2dw/lib/L2Dwidget.min.js?0c58a1486de42ac6cc1c59c7d98ae887"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>
</html>
