<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>drqblog</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2018-12-15T06:14:10.837Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>Victor Drq</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>MNIST数据集入门Demo</title>
    <link href="http://yoursite.com/2018/12/15/MNIST%E6%95%B0%E6%8D%AE%E9%9B%86%E5%85%A5%E9%97%A8Demo/"/>
    <id>http://yoursite.com/2018/12/15/MNIST数据集入门Demo/</id>
    <published>2018-12-15T06:09:13.000Z</published>
    <updated>2018-12-15T06:14:10.837Z</updated>
    
    <content type="html"><![CDATA[<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#Classficiation分类学习</span></span><br><span class="line"><span class="comment">#author:victor</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#MNIST数据集入门</span></span><br><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="comment"># number 1 to 10 data</span></span><br><span class="line"><span class="comment">#use mnist data</span></span><br><span class="line"><span class="comment">#使用这两句，会在程序储存的位置出现文件夹MINIST_data</span></span><br><span class="line"><span class="comment">#下载MNIST数据集中的四个压缩包，并放在MINIST_data文件夹中，不要解压</span></span><br><span class="line"><span class="comment">#官网下载地址：http://yann.lecun.com/exdb/mnist/</span></span><br><span class="line"><span class="keyword">from</span> tensorflow.examples.tutorials.mnist <span class="keyword">import</span> input_data</span><br><span class="line">mnist = input_data.read_data_sets(<span class="string">'MNIST_data/'</span>, one_hot=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#define add_layer function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">add_layer</span><span class="params">(inputs, in_size, out_size, activation_function=None,)</span>:</span></span><br><span class="line">    <span class="comment"># add one more layer and return the output of this layer</span></span><br><span class="line">    Weights = tf.Variable(tf.random_normal([in_size, out_size]))</span><br><span class="line">    biases = tf.Variable(tf.zeros([<span class="number">1</span>, out_size]) + <span class="number">0.1</span>,)</span><br><span class="line">    Wx_plus_b = tf.matmul(inputs, Weights) + biases</span><br><span class="line">    <span class="keyword">if</span> activation_function <span class="keyword">is</span> <span class="keyword">None</span>:</span><br><span class="line">        outputs = Wx_plus_b</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        outputs = activation_function(Wx_plus_b,)</span><br><span class="line">    <span class="keyword">return</span> outputs</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#define compute_accuracy function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_accuracy</span><span class="params">(v_xs, v_ys)</span>:</span></span><br><span class="line">    <span class="keyword">global</span> prediction</span><br><span class="line">    y_pre = sess.run(prediction, feed_dict=&#123;xs: v_xs&#125;)</span><br><span class="line">    correct_prediction = tf.equal(tf.argmax(y_pre,<span class="number">1</span>), tf.argmax(v_ys,<span class="number">1</span>))</span><br><span class="line">    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))</span><br><span class="line">    result = sess.run(accuracy, feed_dict=&#123;xs: v_xs, ys: v_ys&#125;)</span><br><span class="line">    <span class="comment">#output result which is the percent，this percent too high,the prediction too accurate</span></span><br><span class="line">    <span class="keyword">return</span> result</span><br><span class="line"></span><br><span class="line"><span class="comment"># define placeholder for inputs to network</span></span><br><span class="line">xs = tf.placeholder(tf.float32, [<span class="keyword">None</span>, <span class="number">784</span>]) <span class="comment"># 28x28,也就是有784个数据点</span></span><br><span class="line">ys = tf.placeholder(tf.float32, [<span class="keyword">None</span>, <span class="number">10</span>])<span class="comment">#有10个输出</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># add output layer</span></span><br><span class="line">prediction = add_layer(xs, <span class="number">784</span>, <span class="number">10</span>,  activation_function=tf.nn.softmax)</span><br><span class="line"></span><br><span class="line"><span class="comment"># the error between prediction and real data</span></span><br><span class="line"><span class="comment">#分类的话，用softmax配上cross_entropy(交叉熵)</span></span><br><span class="line">cross_entropy = tf.reduce_mean(-tf.reduce_sum(ys * tf.log(prediction),</span><br><span class="line">                                              reduction_indices=[<span class="number">1</span>]))<span class="comment"># loss</span></span><br><span class="line">train_step = tf.train.GradientDescentOptimizer(<span class="number">0.5</span>).minimize(cross_entropy)</span><br><span class="line"></span><br><span class="line">sess = tf.Session()</span><br><span class="line"><span class="comment"># important step</span></span><br><span class="line"><span class="comment"># tf.initialize_all_variables() no long valid from</span></span><br><span class="line"><span class="comment"># 2017-03-02 if using tensorflow &gt;= 0.12</span></span><br><span class="line"><span class="keyword">if</span> int((tf.__version__).split(<span class="string">'.'</span>)[<span class="number">1</span>]) &lt; <span class="number">12</span> <span class="keyword">and</span> int((tf.__version__).split(<span class="string">'.'</span>)[<span class="number">0</span>]) &lt; <span class="number">1</span>:</span><br><span class="line">    init = tf.initialize_all_variables()</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    init = tf.global_variables_initializer()</span><br><span class="line">sess.run(init)</span><br><span class="line"></span><br><span class="line"><span class="comment">#display the graph</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1000</span>):</span><br><span class="line">    batch_xs, batch_ys = mnist.train.next_batch(<span class="number">100</span>)</span><br><span class="line">    sess.run(train_step, feed_dict=&#123;xs: batch_xs, ys: batch_ys&#125;)</span><br><span class="line">    <span class="keyword">if</span> i % <span class="number">50</span> == <span class="number">0</span>:</span><br><span class="line">        print(compute_accuracy(</span><br><span class="line">            mnist.test.images, mnist.test.labels))</span><br></pre></td></tr></table></figure><h2 id="运行效果"><a href="#运行效果" class="headerlink" title="运行效果"></a>运行效果</h2><p><img src="/2018/12/15/MNIST数据集入门Demo/MNIST训练结果.png" alt="MNIST data result"></p><p>可以看出图片识别分类的精确度并不是很高，后续用到了CNN卷积神经网络，精确度可以达到99%</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span clas
      
    
    </summary>
    
      <category term="TensorFlow" scheme="http://yoursite.com/categories/TensorFlow/"/>
    
    
  </entry>
  
  <entry>
    <title>MNIST数据集简介</title>
    <link href="http://yoursite.com/2018/12/15/MNIST%E6%95%B0%E6%8D%AE%E9%9B%86%E7%AE%80%E4%BB%8B/"/>
    <id>http://yoursite.com/2018/12/15/MNIST数据集简介/</id>
    <published>2018-12-15T06:02:35.000Z</published>
    <updated>2018-12-15T06:08:54.215Z</updated>
    
    <content type="html"><![CDATA[<h2 id="MNIST数据集简介"><a href="#MNIST数据集简介" class="headerlink" title="MNIST数据集简介"></a>MNIST数据集简介</h2><p>MNIST(Mixed National Institute of Standards and Technology database)是一个计算机视觉数据集，它包含70000张手写数字的灰度图片，其中每张图片包含28*28个像素点。可以用一个数字数组来表示这张图片。</p><p><img src="/2018/12/15/MNIST数据集简介/MNIST-Matrix.png" alt="MNIST Matrix"></p><p>每张图片都有对应的标签，也就是图片对应的数字，例如上图的标签是1</p><p>数据集被分成两部分：<strong>60000行的训练数据集(mnist.train)和10000行的测试数据集(mnist.test)</strong></p><p>其中：<strong>60000行的训练集拆分为55000行的训练集和5000行的验证集</strong></p><p>60000行的训练数据集是一个形状为[60000,784]的张量，第一个维度数字用来表示图片索引，第二个维度的数字用来索引每张图片中的像素点。在[60000,784]的张量里的每一个元素，都表示每张图片里的某个像素的强度值，值在0，1之间。</p><p><img src="/2018/12/15/MNIST数据集简介/mnist-train-xs.png" alt="mnist-train-xs"></p><p>60000行的训练数据集标签是在0到9的数字，用来描述给定图片里表示的数字，叫做“one-hot vectors”。一个one-hot向量除了某一位的数字是1以外，其余各维度数字都是0，也就是数字n将表示成一个只有在第n维度(从0开始)数字为1的10维度向量。比如，标签0将表示成[1,0,0,0,0,0,0,0,0,0]，而标签0是一个[60000,10]的数字矩阵</p><p><img src="/2018/12/15/MNIST数据集简介/mnist-train-ys.png" alt="mnist-train-ys"></p><p>在TensorFlow里面可以用如下代码导入MNIST数据集：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow.examples.tutorials.mnist <span class="keyword">import</span> input_data</span><br><span class="line"></span><br><span class="line">mnist=input_data.read_data_sets(<span class="string">"MNIST_data/"</span>,one-hot=<span class="keyword">True</span>)</span><br></pre></td></tr></table></figure><p>引入这两句，会自动在代码文件的路径上创建一个MNIST_data文件夹。</p><p>比较烦人的是使用tensorflow.examples.tutorials.mnist import input_data读取数据的时候，经常出现网络连接超时。</p><p><strong>解决方法：查看input_data.py</strong></p><p><img src="/2018/12/15/MNIST数据集简介/input_data.png" alt="input_data"></p><p>这段代码，会先检查文件是否存在，如果不存在才进行下载，我们可以自己动手下载MNIST数据。</p><p>官网下载地址：<a href="http://yann.lecun.com/exdb/mnist/" target="_blank" rel="noopener">MNIST data</a></p><p>注：下载好数据集不要解压。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;MNIST数据集简介&quot;&gt;&lt;a href=&quot;#MNIST数据集简介&quot; class=&quot;headerlink&quot; title=&quot;MNIST数据集简介&quot;&gt;&lt;/a&gt;MNIST数据集简介&lt;/h2&gt;&lt;p&gt;MNIST(Mixed National Institute of Stan
      
    
    </summary>
    
      <category term="TensorFlow" scheme="http://yoursite.com/categories/TensorFlow/"/>
    
    
  </entry>
  
  <entry>
    <title>Visualize Gradient Descent可视化梯度下降</title>
    <link href="http://yoursite.com/2018/12/15/Visualize-Gradient-Descent%E5%8F%AF%E8%A7%86%E5%8C%96%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/"/>
    <id>http://yoursite.com/2018/12/15/Visualize-Gradient-Descent可视化梯度下降/</id>
    <published>2018-12-15T05:57:32.000Z</published>
    <updated>2018-12-15T06:00:19.344Z</updated>
    
    <content type="html"><![CDATA[<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#author:victor</span></span><br><span class="line"><span class="comment">#import module</span></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> mpl_toolkits.mplot3d <span class="keyword">import</span> Axes3D</span><br><span class="line">tf.reset_default_graph()</span><br><span class="line"></span><br><span class="line"><span class="comment">#当你的模型没办法收敛的时候，可以调低学习率</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">Tensorflow的用途，为模型，公式调参，神经网络就是用梯度下降，而</span></span><br><span class="line"><span class="string">梯度下降就是一种优化模式，可以利用梯度下降机制来调参。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">一般在初始化神经网络的参数时，我们可以用到Normal distribution正太分布等方式</span></span><br><span class="line"><span class="string">并且多做几次初始化实验，看看效果如何，运气好的时候很成功</span></span><br><span class="line"><span class="string">可以带来比较好的局部最优解。</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="comment">#initialize parameters</span></span><br><span class="line">LR = <span class="number">0.1</span></span><br><span class="line">REAL_PARAMS = [<span class="number">1.2</span>, <span class="number">2.5</span>]</span><br><span class="line">INIT_PARAMS = [[<span class="number">5</span>, <span class="number">4</span>],</span><br><span class="line">               [<span class="number">5</span>, <span class="number">1</span>],</span><br><span class="line">               [<span class="number">2</span>, <span class="number">4.5</span>]][<span class="number">2</span>]</span><br><span class="line"></span><br><span class="line">x = np.linspace(<span class="number">-1</span>, <span class="number">1</span>, <span class="number">200</span>, dtype=np.float32)   <span class="comment"># x data</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Test (1): Visualize a simple linear function with two parameters,</span></span><br><span class="line"><span class="comment"># you can change LR to 1 to see the different pattern in gradient descent.</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#y_fun = lambda a, b: a * x + b</span></span><br><span class="line"><span class="comment">#tf_y_fun = lambda a, b: a * x + b</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Test (2): Using Tensorflow as a calibrating tool for empirical formula like following.</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#y_fun = lambda a, b: a * x**3 + b * x**2</span></span><br><span class="line"><span class="comment">#tf_y_fun = lambda a, b: a * x**3 + b * x**2</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Test (3): Most simplest two parameters and two layers Neural Net, and their local &amp; global minimum,</span></span><br><span class="line"><span class="comment"># you can try different INIT_PARAMS set to visualize the gradient descent.</span></span><br><span class="line"></span><br><span class="line">y_fun = <span class="keyword">lambda</span> a, b: np.sin(b*np.cos(a*x))</span><br><span class="line">tf_y_fun = <span class="keyword">lambda</span> a, b: tf.sin(b*tf.cos(a*x))</span><br><span class="line"></span><br><span class="line">noise = np.random.randn(<span class="number">200</span>)/<span class="number">10</span></span><br><span class="line">y = y_fun(*REAL_PARAMS) + noise         <span class="comment"># target</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># tensorflow graph</span></span><br><span class="line">a, b = [tf.Variable(initial_value=p, dtype=tf.float32) <span class="keyword">for</span> p <span class="keyword">in</span> INIT_PARAMS]</span><br><span class="line">pred = tf_y_fun(a, b)</span><br><span class="line">mse = tf.reduce_mean(tf.square(y-pred))</span><br><span class="line">train_op = tf.train.GradientDescentOptimizer(LR).minimize(mse)</span><br><span class="line"></span><br><span class="line">a_list, b_list, cost_list = [], [], []</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(tf.global_variables_initializer())</span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> range(<span class="number">400</span>):</span><br><span class="line">        a_, b_, mse_ = sess.run([a, b, mse])</span><br><span class="line">        a_list.append(a_); b_list.append(b_); cost_list.append(mse_)    <span class="comment"># record parameter changes</span></span><br><span class="line">        result, _ = sess.run([pred, train_op])                          <span class="comment"># training</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># visualization codes:</span></span><br><span class="line">print(<span class="string">'a='</span>, a_, <span class="string">'b='</span>, b_)</span><br><span class="line">plt.figure(<span class="number">1</span>)</span><br><span class="line">plt.scatter(x, y, c=<span class="string">'b'</span>)    <span class="comment"># plot data</span></span><br><span class="line">plt.plot(x, result, <span class="string">'r-'</span>, lw=<span class="number">2</span>)   <span class="comment"># plot line fitting</span></span><br><span class="line"><span class="comment"># 3D cost figure</span></span><br><span class="line">fig = plt.figure(<span class="number">2</span>); ax = Axes3D(fig)</span><br><span class="line">a3D, b3D = np.meshgrid(np.linspace(<span class="number">-2</span>, <span class="number">7</span>, <span class="number">30</span>), np.linspace(<span class="number">-2</span>, <span class="number">7</span>, <span class="number">30</span>))  <span class="comment"># parameter space</span></span><br><span class="line">cost3D = np.array([np.mean(np.square(y_fun(a_, b_) - y)) <span class="keyword">for</span> a_, b_ <span class="keyword">in</span> zip(a3D.flatten(), b3D.flatten())]).reshape(a3D.shape)</span><br><span class="line">ax.plot_surface(a3D, b3D, cost3D, rstride=<span class="number">1</span>, cstride=<span class="number">1</span>, cmap=plt.get_cmap(<span class="string">'rainbow'</span>), alpha=<span class="number">0.5</span>)</span><br><span class="line">ax.scatter(a_list[<span class="number">0</span>], b_list[<span class="number">0</span>], zs=cost_list[<span class="number">0</span>], s=<span class="number">300</span>, c=<span class="string">'r'</span>)  <span class="comment"># initial parameter place</span></span><br><span class="line">ax.set_xlabel(<span class="string">'a'</span>); ax.set_ylabel(<span class="string">'b'</span>)</span><br><span class="line">ax.plot(a_list, b_list, zs=cost_list, zdir=<span class="string">'z'</span>, c=<span class="string">'r'</span>, lw=<span class="number">3</span>)    <span class="comment"># plot 3D gradient descent</span></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><h2 id="运行效果"><a href="#运行效果" class="headerlink" title="运行效果"></a>运行效果</h2><p><img src="/2018/12/15/Visualize-Gradient-Descent可视化梯度下降/1.gif" alt="Visualize-Gradient-Descent"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span clas
      
    
    </summary>
    
      <category term="TensorFlow" scheme="http://yoursite.com/categories/TensorFlow/"/>
    
    
  </entry>
  
  <entry>
    <title>用Batch Gradient Descent来拟合sinx</title>
    <link href="http://yoursite.com/2018/12/15/%E7%94%A8Batch-Gradient-Descent%E6%9D%A5%E6%8B%9F%E5%90%88sinx/"/>
    <id>http://yoursite.com/2018/12/15/用Batch-Gradient-Descent来拟合sinx/</id>
    <published>2018-12-15T05:48:05.000Z</published>
    <updated>2018-12-15T05:54:50.920Z</updated>
    
    <content type="html"><![CDATA[<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#author:victor</span></span><br><span class="line"><span class="comment">#import module</span></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"></span><br><span class="line"><span class="comment">#define a add_layer function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">add_layer</span><span class="params">(inputs, in_size, out_size, activation_function=None)</span>:</span></span><br><span class="line">    <span class="comment"># add one more layer and return the output of this layer</span></span><br><span class="line">    Weights = tf.Variable(tf.random_normal([in_size, out_size]))</span><br><span class="line">    biases = tf.Variable(tf.zeros([<span class="number">1</span>, out_size]))</span><br><span class="line">    Wx_plus_b = tf.matmul(inputs, Weights) + biases</span><br><span class="line">    <span class="keyword">if</span> activation_function <span class="keyword">is</span> <span class="keyword">None</span>:</span><br><span class="line">        outputs = Wx_plus_b</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        outputs = activation_function(Wx_plus_b)</span><br><span class="line">    <span class="keyword">return</span> outputs</span><br><span class="line"> </span><br><span class="line"><span class="comment"># Make up some real data</span></span><br><span class="line">x_data = np.linspace(-np.pi,np.pi,<span class="number">300</span>)[:, np.newaxis]</span><br><span class="line">noise = np.random.normal(<span class="number">0</span>, <span class="number">0.05</span>, x_data.shape)</span><br><span class="line">y_data = np.sin(x_data) + noise</span><br><span class="line"> </span><br><span class="line"><span class="comment"># define placeholder for inputs to network</span></span><br><span class="line">xs = tf.placeholder(tf.float32, [<span class="keyword">None</span>, <span class="number">1</span>])</span><br><span class="line">ys = tf.placeholder(tf.float32, [<span class="keyword">None</span>, <span class="number">1</span>])</span><br><span class="line"><span class="comment"># add hidden layer</span></span><br><span class="line">l1 = add_layer(xs, <span class="number">1</span>, <span class="number">10</span>, activation_function=tf.nn.relu)</span><br><span class="line"><span class="comment"># add output layer</span></span><br><span class="line">prediction = add_layer(l1, <span class="number">10</span>, <span class="number">1</span>, activation_function=tf.nn.tanh)</span><br><span class="line"> </span><br><span class="line">loss = tf.reduce_mean(tf.square(ys - prediction))</span><br><span class="line">train_step = tf.train.GradientDescentOptimizer(<span class="number">0.05</span>).minimize(loss)</span><br><span class="line"> </span><br><span class="line">init = tf.initialize_all_variables()</span><br><span class="line">sess = tf.Session()</span><br><span class="line">sess.run(init)</span><br><span class="line"> </span><br><span class="line"><span class="comment"># plot the real data</span></span><br><span class="line">fig = plt.figure()</span><br><span class="line">ax = fig.add_subplot(<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>)</span><br><span class="line">ax.scatter(x_data, y_data)</span><br><span class="line"><span class="comment"># Interactive mode on</span></span><br><span class="line">plt.ion()</span><br><span class="line">plt.show()</span><br><span class="line"> </span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">5000</span>):</span><br><span class="line">    <span class="comment"># training</span></span><br><span class="line">    sess.run(train_step, feed_dict=&#123;xs: x_data, ys: y_data&#125;)</span><br><span class="line">    <span class="keyword">if</span> i % <span class="number">50</span> == <span class="number">0</span>:</span><br><span class="line">        <span class="comment"># to visualize the result and remove the previous line </span></span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            <span class="comment">#ax.lines.remove(lines[0])             </span></span><br><span class="line">            <span class="comment">#每次抹除线，先暂停0.1秒</span></span><br><span class="line">             plt.pause(<span class="number">0.1</span>)</span><br><span class="line">             ax.lines.remove(lines[<span class="number">0</span>])  <span class="comment">#在图片中，去除掉第一个线段</span></span><br><span class="line">        <span class="keyword">except</span> Exception:</span><br><span class="line">            <span class="keyword">pass</span></span><br><span class="line">        prediction_value = sess.run(prediction, feed_dict=&#123;xs: x_data&#125;)</span><br><span class="line">        <span class="comment"># plot the prediction</span></span><br><span class="line">        lines = ax.plot(x_data, prediction_value, <span class="string">'r-'</span>, lw=<span class="number">5</span>)</span><br><span class="line">        plt.pause(<span class="number">0.1</span>)</span><br></pre></td></tr></table></figure><h2 id="运行效果"><a href="#运行效果" class="headerlink" title="运行效果"></a>运行效果</h2><p><img src="/2018/12/15/用Batch-Gradient-Descent来拟合sinx/1.gif" alt="batch gradient descent sinx"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span clas
      
    
    </summary>
    
      <category term="TensorFlow" scheme="http://yoursite.com/categories/TensorFlow/"/>
    
    
  </entry>
  
  <entry>
    <title>用Batch Gradient Descent来拟合二次函数</title>
    <link href="http://yoursite.com/2018/12/15/%E7%94%A8Batch-Gradient-Descent%E6%9D%A5%E6%8B%9F%E5%90%88%E4%BA%8C%E6%AC%A1%E5%87%BD%E6%95%B0/"/>
    <id>http://yoursite.com/2018/12/15/用Batch-Gradient-Descent来拟合二次函数/</id>
    <published>2018-12-15T05:35:45.000Z</published>
    <updated>2018-12-15T05:43:00.394Z</updated>
    
    <content type="html"><![CDATA[<h2 id="用Batch-Gradient-Descent来拟合二次函数"><a href="#用Batch-Gradient-Descent来拟合二次函数" class="headerlink" title="用Batch Gradient Descent来拟合二次函数"></a>用Batch Gradient Descent来拟合二次函数</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#批量梯度下降算法</span></span><br><span class="line"><span class="comment">#author:victor</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#import module</span></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment">#输出结果可视化模块</span></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="comment">#定义一个添加神经层的函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">add_layer</span><span class="params">(inputs,in_size,out_size,activation_function=None)</span>:</span></span><br><span class="line">    Weights=tf.Variable(tf.random_normal([in_size,out_size]))</span><br><span class="line">    biases=tf.Variable(tf.zeros([<span class="number">1</span>,out_size])+<span class="number">0.1</span>)<span class="comment">#因为biases（偏差）推荐值不能为0，所以加上一个0.1</span></span><br><span class="line">    <span class="comment">#Wx_plus_b=tf.matmul(inputs,Weights)+biases#inputs+Weights+biases</span></span><br><span class="line">    Wx_plus_b=tf.add(tf.matmul(inputs,Weights),biases)</span><br><span class="line">    <span class="keyword">if</span> activation_function <span class="keyword">is</span> <span class="keyword">None</span>:</span><br><span class="line">        outputs=Wx_plus_b</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        outputs=activation_function(Wx_plus_b)</span><br><span class="line">    <span class="keyword">return</span> outputs</span><br><span class="line"></span><br><span class="line">x_data=np.linspace(<span class="number">-1</span>,<span class="number">1</span>,<span class="number">300</span>)[:,np.newaxis]<span class="comment">#（-1,1）区间的生成300个数等差数列，np.newaxis定义格式为300个行</span></span><br><span class="line"><span class="comment">#define a noise，定义一个噪声，来让它不是正规的二次函数。</span></span><br><span class="line">noise=np.random.normal(<span class="number">0</span>,<span class="number">0.05</span>,x_data.shape)<span class="comment">#定义跟x_data数据一样的格式</span></span><br><span class="line"><span class="comment">#真实值</span></span><br><span class="line">y_data=np.square(x_data)<span class="number">-0.5</span>+noise<span class="comment">#这个是拟合函数y=x^2+nosie</span></span><br><span class="line"><span class="comment">#placeholder用来参数</span></span><br><span class="line">xs=tf.placeholder(tf.float32,[<span class="keyword">None</span>,<span class="number">1</span>])</span><br><span class="line">ys=tf.placeholder(tf.float32,[<span class="keyword">None</span>,<span class="number">1</span>])</span><br><span class="line"><span class="comment">#隐藏层随便有多少个神经元，越少，越差，先定义一个</span></span><br><span class="line">l1=add_layer(xs,<span class="number">1</span>,<span class="number">10</span>,activation_function=tf.nn.relu)</span><br><span class="line"><span class="comment">#预测值</span></span><br><span class="line">prediction=add_layer(l1,<span class="number">10</span>,<span class="number">1</span>,activation_function=<span class="keyword">None</span>)</span><br><span class="line"><span class="comment">#误差</span></span><br><span class="line">loss=tf.reduce_mean(tf.reduce_sum(tf.square(ys-prediction),</span><br><span class="line">                   reduction_indices=[<span class="number">1</span>]))</span><br><span class="line">train_step=tf.train.GradientDescentOptimizer(<span class="number">0.1</span>).minimize(loss)<span class="comment">#GradientDescentOptimizer给定一个learning rate,通常是小于1</span></span><br><span class="line"><span class="comment">#must step,初始化变量</span></span><br><span class="line">init=tf.initialize_all_variables()</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(init)</span><br><span class="line">    <span class="comment">#显示图形</span></span><br><span class="line">    fig=plt.figure()</span><br><span class="line">    ax=fig.add_subplot(<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>)</span><br><span class="line">    ax.scatter(x_data,y_data)</span><br><span class="line">    plt.ion()<span class="comment">#就是让图片show了后不用暂停</span></span><br><span class="line">    plt.show()</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1000</span>):<span class="comment">#执行1000次</span></span><br><span class="line">        sess.run(train_step,feed_dict=&#123;xs:x_data,ys:y_data&#125;)<span class="comment">#只用通过placeholder的，都要用feed_dict传参数</span></span><br><span class="line">        <span class="keyword">if</span> i%<span class="number">50</span>:                  </span><br><span class="line">            <span class="comment">#to see the step improvement控制台看，每一步的误差减少</span></span><br><span class="line">            <span class="comment">#print(sess.run(loss,feed_dict=&#123;xs:x_data,ys:y_data&#125;))            </span></span><br><span class="line">            <span class="keyword">try</span>:  </span><br><span class="line">                <span class="comment">#预测值</span></span><br><span class="line">                prediction_value=sess.run(prediction,feed_dict=&#123;xs:x_data&#125;)</span><br><span class="line">                lines=ax.plot(x_data,prediction_value,<span class="string">'r-'</span>,lw=<span class="number">5</span>)             </span><br><span class="line">                <span class="comment">#每次抹除线，先暂停0.1秒</span></span><br><span class="line">                plt.pause(<span class="number">0.1</span>)</span><br><span class="line">                ax.lines.remove(lines[<span class="number">0</span>])  <span class="comment">#在图片中，去除掉第一个线段</span></span><br><span class="line">            <span class="keyword">except</span> Exception:</span><br><span class="line">                <span class="keyword">pass</span></span><br></pre></td></tr></table></figure><h2 id="运行效果"><a href="#运行效果" class="headerlink" title="运行效果"></a>运行效果</h2><p><img src="/2018/12/15/用Batch-Gradient-Descent来拟合二次函数/1.gif" alt="batch gradient descent"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;用Batch-Gradient-Descent来拟合二次函数&quot;&gt;&lt;a href=&quot;#用Batch-Gradient-Descent来拟合二次函数&quot; class=&quot;headerlink&quot; title=&quot;用Batch Gradient Descent来拟合二次函数&quot;&gt;
      
    
    </summary>
    
      <category term="TensorFlow" scheme="http://yoursite.com/categories/TensorFlow/"/>
    
    
  </entry>
  
  <entry>
    <title>Activation Function</title>
    <link href="http://yoursite.com/2018/12/15/Activation-Function/"/>
    <id>http://yoursite.com/2018/12/15/Activation-Function/</id>
    <published>2018-12-15T05:30:18.000Z</published>
    <updated>2018-12-15T05:32:37.803Z</updated>
    
    <content type="html"><![CDATA[<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#为什么需要激励函数？Why need activation function?</span></span><br><span class="line"><span class="comment">#激励函数就是为了解决不能用线性方程(Linear)解决的问题，y=w*x+b。也就是非线性方程问题(Nonlinear)</span></span><br><span class="line"><span class="comment">#y=Wx--&gt;y=AF(Wx)</span></span><br><span class="line"><span class="comment">#一些常用的AF非线性函数,比如relu，sigmoid，tanh，也就是激励函数</span></span><br><span class="line"><span class="comment">#relu为，x&gt;0,f(x)=1,x&lt;=0,f(x=0)</span></span><br><span class="line"><span class="comment">#你也可以自己创建自己的激励函数，但是函数必须保证是可微分的</span></span><br><span class="line"><span class="comment">#在卷积神经网络中，推荐使用relu激励函数</span></span><br><span class="line"><span class="comment">#在循环神经网络中(Recurrent Nerual Network)推荐适用relu or tanh</span></span><br><span class="line"><span class="comment">#sigmoid函数，也叫Logistics会出现梯度消失</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid</span><span class="params">(x)</span>:</span></span><br><span class="line">    y = <span class="number">1.0</span> / (<span class="number">1.0</span> + np.exp(-x))</span><br><span class="line">    <span class="keyword">return</span> y</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">elu</span><span class="params">(x, a)</span>:</span></span><br><span class="line">    y = x.copy()</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(y.shape[<span class="number">0</span>]):</span><br><span class="line">        <span class="keyword">if</span> y[i] &lt; <span class="number">0</span>:</span><br><span class="line">            y[i] = a * (np.exp(y[i]) - <span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> y</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">lrelu</span><span class="params">(x, a)</span>:</span></span><br><span class="line">    y = x.copy()</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(y.shape[<span class="number">0</span>]):</span><br><span class="line">        <span class="keyword">if</span> y[i] &lt; <span class="number">0</span>:</span><br><span class="line">            y[i] = a * y[i]</span><br><span class="line">    <span class="keyword">return</span> y</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">relu</span><span class="params">(x)</span>:</span></span><br><span class="line">    y = x.copy()</span><br><span class="line">    y[y &lt; <span class="number">0</span>] = <span class="number">0</span></span><br><span class="line">    <span class="keyword">return</span> y</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">softplus</span><span class="params">(x)</span>:</span></span><br><span class="line">    y = np.log(np.exp(x) + <span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> y</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">softsign</span><span class="params">(x)</span>:</span></span><br><span class="line">    y = x / (np.abs(x) + <span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> y</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">tanh</span><span class="params">(x)</span>:</span></span><br><span class="line">    y = (<span class="number">1.0</span> - np.exp(<span class="number">-2</span> * x)) / (<span class="number">1.0</span> + np.exp(<span class="number">-2</span> * x))</span><br><span class="line">    <span class="keyword">return</span> y</span><br><span class="line"></span><br><span class="line"><span class="comment">#invoke this function</span></span><br><span class="line">x = np.linspace(start=<span class="number">-10</span>, stop=<span class="number">10</span>, num=<span class="number">100</span>)</span><br><span class="line">y_sigmoid = sigmoid(x)</span><br><span class="line">y_elu = elu(x, <span class="number">0.25</span>)</span><br><span class="line">y_lrelu = lrelu(x, <span class="number">0.25</span>)</span><br><span class="line">y_relu = relu(x)</span><br><span class="line">y_softplus = softplus(x)</span><br><span class="line">y_softsign = softsign(x)</span><br><span class="line">y_tanh = tanh(x)</span><br><span class="line"></span><br><span class="line">tx = <span class="number">6</span></span><br><span class="line">ty = <span class="number">0.9</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#display the graph</span></span><br><span class="line">plt.subplot(<span class="number">331</span>)</span><br><span class="line">plt.title(<span class="string">'sigmoid'</span>)</span><br><span class="line">plt.plot(x, y_sigmoid)</span><br><span class="line">plt.grid(<span class="keyword">True</span>)</span><br><span class="line">plt.subplot(<span class="number">332</span>)</span><br><span class="line">plt.title(<span class="string">'elu'</span>)</span><br><span class="line">plt.plot(x, y_elu)</span><br><span class="line">plt.grid(<span class="keyword">True</span>)</span><br><span class="line">plt.subplot(<span class="number">333</span>)</span><br><span class="line">plt.title(<span class="string">'lrelu'</span>)</span><br><span class="line">plt.plot(x, y_lrelu)</span><br><span class="line">plt.grid(<span class="keyword">True</span>)</span><br><span class="line">plt.subplot(<span class="number">334</span>)</span><br><span class="line">plt.title(<span class="string">'relu'</span>)</span><br><span class="line">plt.plot(x, y_relu)</span><br><span class="line">plt.grid(<span class="keyword">True</span>)</span><br><span class="line">plt.subplot(<span class="number">335</span>)</span><br><span class="line">plt.title(<span class="string">'softplus'</span>)</span><br><span class="line">plt.plot(x, y_softplus)</span><br><span class="line">plt.grid(<span class="keyword">True</span>)</span><br><span class="line">plt.subplot(<span class="number">336</span>)</span><br><span class="line">plt.title(<span class="string">'softsign'</span>)</span><br><span class="line">plt.plot(x, y_softsign)</span><br><span class="line">plt.grid(<span class="keyword">True</span>)</span><br><span class="line">plt.subplot(<span class="number">337</span>)</span><br><span class="line">plt.title(<span class="string">'tanh'</span>)</span><br><span class="line">plt.plot(x, y_tanh)</span><br><span class="line">plt.grid(<span class="keyword">True</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="/2018/12/15/Activation-Function/1.jpg" alt="Activatoin Function"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span clas
      
    
    </summary>
    
      <category term="TensorFlow" scheme="http://yoursite.com/categories/TensorFlow/"/>
    
    
  </entry>
  
  <entry>
    <title>利用TensorFlow模拟线性回归(Linear Regression)</title>
    <link href="http://yoursite.com/2018/12/15/%E5%88%A9%E7%94%A8TensorFlow%E6%A8%A1%E6%8B%9F%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92-Linear-Regression/"/>
    <id>http://yoursite.com/2018/12/15/利用TensorFlow模拟线性回归-Linear-Regression/</id>
    <published>2018-12-15T03:38:06.000Z</published>
    <updated>2018-12-15T04:00:49.904Z</updated>
    
    <content type="html"><![CDATA[<p><strong>线性回归(Linear Regression）</strong></p><ul><li>线性回归是利用数理统计中回归分析，来确定两种或两种以上变量间相互依赖的定量关系的一种统计分析方法，运用十分广泛。其表达形式为<strong>y = w’x+e</strong>，e为误差服从均值为0的正态分布</li><li>回归分析中，只包括一个自变量和一个因变量，且二者的关系可用一条直线近似表示，这种回归分析称为一元线性回归分析。如果回归分析中包括两个或两个以上的自变量，且因变量和自变量之间是线性关系，则称为多元线性回归分析。</li><li><strong>梯度下降</strong>是用于找到函数最小值的一阶迭代优化算法。为了使用梯度下降找到函数的局部最小值，需要采用与当前点处函数的梯度（或近似梯度）的负值成比例的步长。相反，如果采用与梯度的正值成比例的步长，则接近该函数的局部最大值 ; 然后将该过程称为梯度上升。</li></ul><p><strong>废话不多说，直接上代码，展示运行效果</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#Linear Regression线性回归</span></span><br><span class="line"><span class="comment">#author:victor</span></span><br><span class="line"><span class="keyword">from</span> future <span class="keyword">import</span> print_function</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment">#generate number</span></span><br><span class="line">rng = numpy.random</span><br><span class="line"></span><br><span class="line"><span class="comment"># Parameters</span></span><br><span class="line">learning_rate = <span class="number">0.01</span></span><br><span class="line">training_epochs = <span class="number">1000</span></span><br><span class="line">display_step = <span class="number">50</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Training Data</span></span><br><span class="line"><span class="comment">#numpy.asarray和array都可以讲结构数据转化为ndarray</span></span><br><span class="line"><span class="comment">#区别：当数据源是ndarray时，array仍然会copy出一个副本，占用新的内存，#但是asarray不会</span></span><br><span class="line">train_X = numpy.asarray([<span class="number">3.3</span>,<span class="number">4.4</span>,<span class="number">5.5</span>,<span class="number">6.71</span>,<span class="number">6.93</span>,<span class="number">4.168</span>,<span class="number">9.779</span>,<span class="number">6.182</span>,<span class="number">7.59</span>,<span class="number">2.167</span>,</span><br><span class="line">                         <span class="number">7.042</span>,<span class="number">10.791</span>,<span class="number">5.313</span>,<span class="number">7.997</span>,<span class="number">5.654</span>,<span class="number">9.27</span>,<span class="number">3.1</span>])</span><br><span class="line">train_Y = numpy.asarray([<span class="number">1.7</span>,<span class="number">2.76</span>,<span class="number">2.09</span>,<span class="number">3.19</span>,<span class="number">1.694</span>,<span class="number">1.573</span>,<span class="number">3.366</span>,<span class="number">2.596</span>,<span class="number">2.53</span>,<span class="number">1.221</span>,</span><br><span class="line">                         <span class="number">2.827</span>,<span class="number">3.465</span>,<span class="number">1.65</span>,<span class="number">2.904</span>,<span class="number">2.42</span>,<span class="number">2.94</span>,<span class="number">1.3</span>])</span><br><span class="line"></span><br><span class="line">n_samples = train_X.shape[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># tf Graph Input</span></span><br><span class="line">X = tf.placeholder(<span class="string">"float"</span>)</span><br><span class="line">Y = tf.placeholder(<span class="string">"float"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Set model weights</span></span><br><span class="line">W = tf.Variable(rng.randn(), name=<span class="string">"weight"</span>)</span><br><span class="line">b = tf.Variable(rng.randn(), name=<span class="string">"bias"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Construct a linear model</span></span><br><span class="line">pred = tf.add(tf.multiply(X, W), b)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Mean squared error</span></span><br><span class="line">cost = tf.reduce_sum(tf.pow(pred-Y, <span class="number">2</span>))/(<span class="number">2</span>*n_samples)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Gradient descent</span></span><br><span class="line"><span class="comment">#  Note, minimize() knows to modify W and b because Variable objects are #trainable=True by default</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#优化器，采用梯度下降方法来训练学习</span></span><br><span class="line">optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Initialize the variables (i.e. assign their default value)</span></span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Start training</span></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    <span class="comment"># Run the initializer</span></span><br><span class="line">    sess.run(init)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Fit all training data</span></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(training_epochs):</span><br><span class="line">        <span class="keyword">for</span> (x, y) <span class="keyword">in</span> zip(train_X, train_Y):</span><br><span class="line">            sess.run(optimizer, feed_dict=&#123;X: x, Y: y&#125;)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Display logs per epoch step(控制台显示每次步骤)</span></span><br><span class="line">        <span class="keyword">if</span> (epoch+<span class="number">1</span>) % display_step == <span class="number">0</span>:</span><br><span class="line">            c = sess.run(cost, feed_dict=&#123;X: train_X, Y:train_Y&#125;)</span><br><span class="line">            print(<span class="string">"Epoch:"</span>, <span class="string">'%04d'</span> % (epoch+<span class="number">1</span>), <span class="string">"cost="</span>, <span class="string">"&#123;:.9f&#125;"</span>.format(c), \</span><br><span class="line">                <span class="string">"W="</span>, sess.run(W), <span class="string">"b="</span>, sess.run(b))</span><br><span class="line"></span><br><span class="line">    print(<span class="string">"Optimization Finished!"</span>)</span><br><span class="line">    training_cost = sess.run(cost, feed_dict=&#123;X: train_X, Y: train_Y&#125;)</span><br><span class="line">    print(<span class="string">"Training cost="</span>, training_cost, <span class="string">"W="</span>, sess.run(W), <span class="string">"b="</span>, sess.run(b), <span class="string">'\n'</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Graphic display(图像展示）the original data on the graph</span></span><br><span class="line">    plt.plot(train_X, train_Y, <span class="string">'ro'</span>, label=<span class="string">'Original data'</span>)</span><br><span class="line">    plt.plot(train_X, sess.run(W) * train_X + sess.run(b), label=<span class="string">'Fitted line'</span>)</span><br><span class="line">    plt.legend()</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Testing example, as requested (Issue #2)</span></span><br><span class="line">    test_X = numpy.asarray([<span class="number">6.83</span>, <span class="number">4.668</span>, <span class="number">8.9</span>, <span class="number">7.91</span>, <span class="number">5.7</span>, <span class="number">8.7</span>, <span class="number">3.1</span>, <span class="number">2.1</span>])</span><br><span class="line">    test_Y = numpy.asarray([<span class="number">1.84</span>, <span class="number">2.273</span>, <span class="number">3.2</span>, <span class="number">2.831</span>, <span class="number">2.92</span>, <span class="number">3.24</span>, <span class="number">1.35</span>, <span class="number">1.03</span>])</span><br><span class="line"></span><br><span class="line">    print(<span class="string">"Testing... (Mean square loss Comparison)"</span>)</span><br><span class="line"></span><br><span class="line">    testing_cost = sess.run(</span><br><span class="line">        tf.reduce_sum(tf.pow(pred - Y, <span class="number">2</span>)) / (<span class="number">2</span> * test_X.shape[<span class="number">0</span>]),</span><br><span class="line">        feed_dict=&#123;X: test_X, Y: test_Y&#125;)  <span class="comment"># same function as cost above</span></span><br><span class="line"></span><br><span class="line">    print(<span class="string">"Testing cost="</span>, testing_cost)</span><br><span class="line">    print(<span class="string">"Absolute mean square loss difference:"</span>, abs(</span><br><span class="line">        training_cost - testing_cost))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#display the Test data on the graph</span></span><br><span class="line">    plt.plot(test_X, test_Y, <span class="string">'bo'</span>, label=<span class="string">'Testing data'</span>)</span><br><span class="line">    plt.plot(train_X, sess.run(W) * train_X + sess.run(b), label=<span class="string">'Fitted line'</span>)</span><br><span class="line">    plt.legend()</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure><p><strong>控制台的每一次梯度下降后的误差值：</strong></p><p><img src="/2018/12/15/利用TensorFlow模拟线性回归-Linear-Regression/线性回归控制台步骤.png" alt="Linear Regression"></p><p><strong>线性回归后的结果图展示：</strong></p><p><img src="/2018/12/15/利用TensorFlow模拟线性回归-Linear-Regression/线性回归图像.png" alt="Linear Regression"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;strong&gt;线性回归(Linear Regression）&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;线性回归是利用数理统计中回归分析，来确定两种或两种以上变量间相互依赖的定量关系的一种统计分析方法，运用十分广泛。其表达形式为&lt;strong&gt;y = w’x+e&lt;/st
      
    
    </summary>
    
      <category term="TensorFlow" scheme="http://yoursite.com/categories/TensorFlow/"/>
    
    
  </entry>
  
  <entry>
    <title>用matplotlib包画函数图像</title>
    <link href="http://yoursite.com/2018/12/15/%E7%94%A8matplotlib%E5%8C%85%E7%94%BB%E5%87%BD%E6%95%B0%E5%9B%BE%E5%83%8F/"/>
    <id>http://yoursite.com/2018/12/15/用matplotlib包画函数图像/</id>
    <published>2018-12-15T03:25:31.000Z</published>
    <updated>2018-12-15T04:15:45.845Z</updated>
    
    <content type="html"><![CDATA[<h2 id="画Sinx函数图像"><a href="#画Sinx函数图像" class="headerlink" title="画Sinx函数图像"></a>画Sinx函数图像</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#正弦函数图像</span></span><br><span class="line"><span class="comment">#author:victor</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#import module</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment">#generate number</span></span><br><span class="line">x=np.arange(<span class="number">0</span>,<span class="number">2</span>*np.pi,<span class="number">0.00001</span>)</span><br><span class="line">y=np.sin(x)</span><br><span class="line"></span><br><span class="line"><span class="comment">#display the graph</span></span><br><span class="line">plt.plot(x,y)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="/2018/12/15/用matplotlib包画函数图像/sinx的图像.png" alt="sinx"></p><h2 id="画cosx-x的图像"><a href="#画cosx-x的图像" class="headerlink" title="画cosx/x的图像"></a>画cosx/x的图像</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#画出cosx/x的图像</span></span><br><span class="line"><span class="comment">#author:victor</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#import module</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment">#define function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> np.cos(x*<span class="number">30</span>)/x</span><br><span class="line"></span><br><span class="line"><span class="comment">#generate number</span></span><br><span class="line">g=np.frompyfunc(f,<span class="number">1</span>,<span class="number">1</span>)</span><br><span class="line">a=np.arange(<span class="number">0.1</span>,<span class="number">2</span>*np.pi,<span class="number">0.00001</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#use the function</span></span><br><span class="line">d=g(a)</span><br><span class="line"></span><br><span class="line"><span class="comment">#set maxsize</span></span><br><span class="line">d_max=np.max(d)</span><br><span class="line"><span class="comment">#set minsize</span></span><br><span class="line">d_min=np.min(d)</span><br><span class="line"></span><br><span class="line"><span class="comment">#display the graph</span></span><br><span class="line">plt.figure(figsize=(<span class="number">52</span>,<span class="number">23.65</span>))</span><br><span class="line">plt.xlim((<span class="number">-0.1</span>,<span class="number">2</span>*np.pi+<span class="number">0.1</span>))</span><br><span class="line">plt.ylim((<span class="number">-5</span>,<span class="number">5</span>))</span><br><span class="line">plt.plot(a,d,<span class="string">'-'</span>,c=<span class="string">'g'</span>,lw=<span class="number">2</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="/2018/12/15/用matplotlib包画函数图像/cosx除以x的图像.png" alt="cosx/x"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;画Sinx函数图像&quot;&gt;&lt;a href=&quot;#画Sinx函数图像&quot; class=&quot;headerlink&quot; title=&quot;画Sinx函数图像&quot;&gt;&lt;/a&gt;画Sinx函数图像&lt;/h2&gt;&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;
      
    
    </summary>
    
      <category term="TensorFlow" scheme="http://yoursite.com/categories/TensorFlow/"/>
    
    
  </entry>
  
  <entry>
    <title>用matplotlib包画箱型图</title>
    <link href="http://yoursite.com/2018/12/15/%E7%94%A8matplotlib%E5%8C%85%E7%94%BB%E7%AE%B1%E5%9E%8B%E5%9B%BE/"/>
    <id>http://yoursite.com/2018/12/15/用matplotlib包画箱型图/</id>
    <published>2018-12-15T03:22:05.000Z</published>
    <updated>2018-12-15T04:11:09.299Z</updated>
    
    <content type="html"><![CDATA[<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#箱型图</span></span><br><span class="line"><span class="comment">#author:victor</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#import module</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment">#normal distribution</span></span><br><span class="line">data=np.random.normal(loc=<span class="number">0</span>,scale=<span class="number">1</span>,size=<span class="number">1000</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#sym点的形状，whis虚线的长度</span></span><br><span class="line">plt.boxplot(data,sym=<span class="string">'o'</span>,whis=<span class="number">1.5</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#display the graph</span></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="/2018/12/15/用matplotlib包画箱型图/箱型图.png" alt="box"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span clas
      
    
    </summary>
    
      <category term="TensorFlow" scheme="http://yoursite.com/categories/TensorFlow/"/>
    
    
  </entry>
  
  <entry>
    <title>用matplotlib包画饼状图</title>
    <link href="http://yoursite.com/2018/12/15/%E7%94%A8matplotlib%E5%8C%85%E7%94%BB%E9%A5%BC%E7%8A%B6%E5%9B%BE/"/>
    <id>http://yoursite.com/2018/12/15/用matplotlib包画饼状图/</id>
    <published>2018-12-15T03:19:44.000Z</published>
    <updated>2018-12-15T04:06:13.186Z</updated>
    
    <content type="html"><![CDATA[<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#饼状图</span></span><br><span class="line"><span class="comment">#author：victor</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#导入模块</span></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment">#设置labels标签参数，x是对应的数据列表，autopct显示每一个区域占的比例</span></span><br><span class="line"><span class="comment">#explode突出显示某一块，shadow阴影</span></span><br><span class="line">labels=[<span class="string">'A'</span>,<span class="string">'B'</span>,<span class="string">'C'</span>,<span class="string">'D'</span>]</span><br><span class="line">fracs=[<span class="number">15</span>,<span class="number">30</span>,<span class="number">45</span>,<span class="number">10</span>]</span><br><span class="line">explode=[<span class="number">0</span>,<span class="number">0.1</span>,<span class="number">0.05</span>,<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment">#设置x,y轴比例1:1，从而得到一个正的圆</span></span><br><span class="line">plt.axes(aspect=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#labels标签参数，x是对应的数据列表，autopct显示每一个区域占的比例，</span></span><br><span class="line"><span class="comment">#explode突出显示某一块，shadow阴影</span></span><br><span class="line">plt.pie(x=fracs,labels=labels,autopct=<span class="string">"%.0f%%"</span>,explode=explode,shadow=<span class="keyword">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#display the graph</span></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="/2018/12/15/用matplotlib包画饼状图/饼状图.png" alt="pie graph"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span clas
      
    
    </summary>
    
      <category term="TensorFlow" scheme="http://yoursite.com/categories/TensorFlow/"/>
    
    
  </entry>
  
  <entry>
    <title>用matplotlib包画条形图</title>
    <link href="http://yoursite.com/2018/12/15/%E7%94%A8matplotlib%E5%8C%85%E7%94%BB%E6%9D%A1%E5%BD%A2%E5%9B%BE/"/>
    <id>http://yoursite.com/2018/12/15/用matplotlib包画条形图/</id>
    <published>2018-12-15T03:17:43.000Z</published>
    <updated>2018-12-15T04:09:08.539Z</updated>
    
    <content type="html"><![CDATA[<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#条形图</span></span><br><span class="line"><span class="comment">#author:victor</span></span><br><span class="line"><span class="comment">#导入模块</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment">#产生随机数</span></span><br><span class="line">data = np.random.normal(<span class="number">0</span>, <span class="number">20</span>, <span class="number">1000</span>)</span><br><span class="line">bins = np.arange(<span class="number">-100</span>, <span class="number">100</span>, <span class="number">5</span>) <span class="comment"># fixed bin size</span></span><br><span class="line"></span><br><span class="line">plt.xlim([min(data)<span class="number">-5</span>, max(data)+<span class="number">5</span>])</span><br><span class="line">plt.hist(data, bins=bins, alpha=<span class="number">0.5</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#display the graph</span></span><br><span class="line">plt.title(<span class="string">'Random Gaussian data (fixed bin size)'</span>)</span><br><span class="line">plt.xlabel(<span class="string">'variable X (bin size = 5)'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'count'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="/2018/12/15/用matplotlib包画条形图/条形图.png" alt="gaussian graph"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span clas
      
    
    </summary>
    
      <category term="TensorFlow" scheme="http://yoursite.com/categories/TensorFlow/"/>
    
    
  </entry>
  
  <entry>
    <title>用matplotlib包画折线图</title>
    <link href="http://yoursite.com/2018/12/15/%E7%94%A8matplotlib%E5%8C%85%E7%94%BB%E6%8A%98%E7%BA%BF%E5%9B%BE/"/>
    <id>http://yoursite.com/2018/12/15/用matplotlib包画折线图/</id>
    <published>2018-12-15T03:14:30.000Z</published>
    <updated>2018-12-15T06:27:18.104Z</updated>
    
    <content type="html"><![CDATA[<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#折线图</span></span><br><span class="line"><span class="comment">#author:victor</span></span><br><span class="line"><span class="comment">#导入模块</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment">#生成数据</span></span><br><span class="line">x=np.linspace(<span class="number">-10000</span>,<span class="number">10000</span>,<span class="number">100</span>) <span class="comment">#将-10000到10000等区间分成100份</span></span><br><span class="line">y=x2+x3+x**<span class="number">7</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#显示图像</span></span><br><span class="line">plt.plot(x,y)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="/2018/12/15/用matplotlib包画折线图/折线图.png" alt="line"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span clas
      
    
    </summary>
    
      <category term="TensorFlow" scheme="http://yoursite.com/categories/TensorFlow/"/>
    
    
  </entry>
  
  <entry>
    <title>用matplotlib包画散点图</title>
    <link href="http://yoursite.com/2018/12/15/%E7%94%A8matplotlib%E5%8C%85%E7%94%BB%E6%95%A3%E7%82%B9%E5%9B%BE/"/>
    <id>http://yoursite.com/2018/12/15/用matplotlib包画散点图/</id>
    <published>2018-12-15T03:08:48.000Z</published>
    <updated>2018-12-15T04:08:28.924Z</updated>
    
    <content type="html"><![CDATA[<p><strong>Scatter函数是一个强大的画散点图函数：</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">matplotlib.pyplot.scatter(x, y, s=<span class="keyword">None</span>, c=<span class="keyword">None</span>, marker=<span class="keyword">None</span>, cmap=<span class="keyword">None</span>, norm=<span class="keyword">None</span>, vmin=<span class="keyword">None</span>, vmax=<span class="keyword">None</span>, alpha=<span class="keyword">None</span>, linewidths=<span class="keyword">None</span>, verts=<span class="keyword">None</span>, edgecolors=<span class="keyword">None</span>, *, data=<span class="keyword">None</span>, **kwargs)</span><br><span class="line"></span><br><span class="line"><span class="comment">#x,y：表示数据的位置，也就是x,y轴</span></span><br><span class="line"><span class="comment">#s：表示图形的大小</span></span><br><span class="line"><span class="comment">#c：表示颜色或颜色序列，可能的情况如下： </span></span><br><span class="line"></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">  1. 单一颜色</span></span><br><span class="line"><span class="string">  2. 颜色序列</span></span><br><span class="line"><span class="string">  3. 使用cmap映射到颜色的序列数</span></span><br><span class="line"><span class="string">  4. 一个行为RGB的2-D数组</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">marker：绘出的图形的形状，具有多种风格 </span></span><br><span class="line"><span class="string">'''</span></span><br></pre></td></tr></table></figure><p><strong>一、画散点图(一)</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#散点图</span></span><br><span class="line"><span class="comment">#导入必要的模块</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment">#参数随机数</span></span><br><span class="line">N=<span class="number">50</span></span><br><span class="line">x=np.random.randn(N)</span><br><span class="line">y=np.random.randn(N)</span><br><span class="line"></span><br><span class="line"><span class="comment">#画散点图</span></span><br><span class="line">plt.scatter(x,y,s=<span class="number">50</span>,c=<span class="string">'r'</span>,marker=<span class="string">'o'</span>,alpha=<span class="number">0.5</span>)</span><br><span class="line"><span class="comment">#x---&gt;x轴，y---&gt;y轴</span></span><br><span class="line"><span class="comment">#s---&gt;散点面积</span></span><br><span class="line"><span class="comment">#c---&gt;散点颜色</span></span><br><span class="line"><span class="comment">#颜色参数c：b--&gt;blue  c--&gt;cyan g--&gt;green  k--&gt;black  m--&gt;magenta  r--&gt;red </span></span><br><span class="line"><span class="comment">#w--&gt;white  y--&gt;yellow</span></span><br><span class="line"><span class="comment">#marker---&gt;散点形状</span></span><br><span class="line"><span class="comment">#marker='o'为圆点，marker='x'为×号，marker='s'显示为小正方形</span></span><br><span class="line"><span class="comment">#alpha---&gt;散点透明度</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#显示所画的图</span></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="/2018/12/15/用matplotlib包画散点图/散点图.png" alt="scatter"></p><p><strong>二、画散点图(二）</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#导入必要的模块 </span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np </span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt </span><br><span class="line"></span><br><span class="line"><span class="comment">#产生测试数据 </span></span><br><span class="line">x = np.arange(<span class="number">1</span>,<span class="number">10</span>) </span><br><span class="line">y = x </span><br><span class="line">fig = plt.figure() </span><br><span class="line">ax1 = fig.add_subplot(<span class="number">111</span>) </span><br><span class="line"></span><br><span class="line"><span class="comment">#设置标题 </span></span><br><span class="line">ax1.set_title(<span class="string">'Scatter Plot'</span>) </span><br><span class="line"></span><br><span class="line"><span class="comment">#设置X轴标签 </span></span><br><span class="line">plt.xlabel(<span class="string">'X'</span>) </span><br><span class="line"></span><br><span class="line"><span class="comment">#设置Y轴标签 </span></span><br><span class="line">plt.ylabel(<span class="string">'Y'</span>) </span><br><span class="line"></span><br><span class="line"><span class="comment">#画散点图 </span></span><br><span class="line">ax1.scatter(x,y,c = <span class="string">'r'</span>,marker = <span class="string">'o'</span>) </span><br><span class="line"></span><br><span class="line"><span class="comment">#设置图标 </span></span><br><span class="line">plt.legend(<span class="string">'x1'</span>) </span><br><span class="line"></span><br><span class="line"><span class="comment">#显示所画的图 </span></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="/2018/12/15/用matplotlib包画散点图/散点图1.png" alt="scatter"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;strong&gt;Scatter函数是一个强大的画散点图函数：&lt;/strong&gt;&lt;/p&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/spa
      
    
    </summary>
    
      <category term="TensorFlow" scheme="http://yoursite.com/categories/TensorFlow/"/>
    
    
  </entry>
  
  <entry>
    <title>用matplotlib包画图</title>
    <link href="http://yoursite.com/2018/12/15/%E7%94%A8matplotlib%E5%8C%85%E7%94%BB%E5%9B%BE/"/>
    <id>http://yoursite.com/2018/12/15/用matplotlib包画图/</id>
    <published>2018-12-15T03:06:50.000Z</published>
    <updated>2018-12-15T04:10:31.295Z</updated>
    
    <content type="html"><![CDATA[<p>​    最近深度学习发展非常迅猛，大有一统江湖的趋势。经过一段时间学习，发现自己对这种神奇的玄学非常感兴趣，希望能够进一步的研究。而这种研究性学科单纯地看论文比较难以明白，所以希望能够跟进大牛们写的代码深入学习。我发现很多大牛给的源码是基于python写的，于是就打算学习python。 </p><p><strong>一、Spyder的用法</strong></p><ul><li>Spyder是一个简单的集成开发环境，它模拟MATLAB的‘工作空间’的功能，可以很方便的观察和修改数组的值。</li><li>推荐您使用IPython Console，因为它比标准的Python Console的功能更多，并且建议您将它设置为默认控制台。</li><li>命名空间（在任何给定时间内在console中定义的对象集合）在IPython中可以使用%reset命令清除。输入<strong>%reset</strong>然后按下<strong>enter键</strong>，用y确认。</li><li>首先要进行工具栏设置，调出QT窗体来显示图形。</li><li>第一步：选择工具栏的<strong>Tools–&gt;Preferences</strong></li></ul><p><img src="/2018/12/15/用matplotlib包画图/设置1.png" alt="setting1"></p><ul><li>第二步：点击IPython console选项</li></ul><p><img src="/2018/12/15/用matplotlib包画图/设置2.png" alt="setting2"></p><ul><li>第三步：选择Graphics选项，在Backend选择Qt5，然后保存重启就行</li></ul><p><img src="/2018/12/15/用matplotlib包画图/设置3.png" alt="setting3"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;​    最近深度学习发展非常迅猛，大有一统江湖的趋势。经过一段时间学习，发现自己对这种神奇的玄学非常感兴趣，希望能够进一步的研究。而这种研究性学科单纯地看论文比较难以明白，所以希望能够跟进大牛们写的代码深入学习。我发现很多大牛给的源码是基于python写的，于是就打算学习
      
    
    </summary>
    
      <category term="TensorFlow" scheme="http://yoursite.com/categories/TensorFlow/"/>
    
    
  </entry>
  
  <entry>
    <title>TensorFlow入门demo</title>
    <link href="http://yoursite.com/2018/12/15/TensorFlow%E5%85%A5%E9%97%A8demo/"/>
    <id>http://yoursite.com/2018/12/15/TensorFlow入门demo/</id>
    <published>2018-12-15T03:01:13.000Z</published>
    <updated>2018-12-15T04:32:03.796Z</updated>
    
    <content type="html"><![CDATA[<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#引入tensorflow模块</span></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="comment">#创建两个常量节点</span></span><br><span class="line">node1=tf.constant(<span class="number">3.2</span>)</span><br><span class="line">node2=tf.constant(<span class="number">4.8</span>)</span><br><span class="line"><span class="comment">#创建一个adder节点，，对上面节点执行+操作</span></span><br><span class="line">adder = node1 + node2</span><br><span class="line"><span class="comment">#打印一些节点</span></span><br><span class="line">print(adder)</span><br><span class="line"><span class="comment">#打印adder运行后的结果</span></span><br><span class="line">sess=tf.Session()</span><br><span class="line">print(sess.run(adder))</span><br><span class="line"><span class="comment">#创建两个占位Tensor节点</span></span><br><span class="line">a=tf.placeholder(tf.float32)</span><br><span class="line">b=tf.placeholder(tf.float32)</span><br><span class="line"><span class="comment">#创建一个adder节点，对上面两个节点执行+操作</span></span><br><span class="line">adder_node=a+b</span><br><span class="line"><span class="comment">#打印三个节点</span></span><br><span class="line">print(a)</span><br><span class="line">print(b)</span><br><span class="line">print(adder_node)</span><br><span class="line"><span class="comment">#运行一下，后面的dict参数是为占位tensor提供输入数据</span></span><br><span class="line">sess=tf.Session()</span><br><span class="line">print(sess.run(adder_node,&#123;a:<span class="number">3</span>,b:<span class="number">4.5</span>&#125;))</span><br><span class="line">print(sess.run(adder_node,&#123;a:[<span class="number">1</span>,<span class="number">3</span>],b:[<span class="number">2</span>,<span class="number">4</span>]&#125;))</span><br><span class="line"><span class="comment">#添加乘法操作</span></span><br><span class="line">add_and_triple=adder*<span class="number">3.</span></span><br><span class="line">print(sess.run(add_and_triple,&#123;a:<span class="number">3</span>,b:<span class="number">4.5</span>&#125;))</span><br><span class="line"><span class="comment">#创建变量w和b节点，并设置初始值</span></span><br><span class="line">w=tf.Variable([<span class="number">.1</span>],dtype=tf.float32)</span><br><span class="line">b=tf.Variable([<span class="number">-.1</span>],dtype=tf.float32)</span><br><span class="line"><span class="comment">#创建x节点，用来输入实验中的输入数据</span></span><br><span class="line">x=tf.placeholder(tf.float32)</span><br><span class="line"><span class="comment">#创建线性模型</span></span><br><span class="line">linear_model=w*x+b</span><br><span class="line"><span class="comment">#创建y节点，用来输入实验中得到的的输出数据，用于损失模型计算</span></span><br><span class="line">y=tf.placeholder(tf.float32)</span><br><span class="line"><span class="comment">#创建损失模型</span></span><br><span class="line">loss=tf.reduce_sum(tf.square(linear_model-y))</span><br><span class="line"><span class="comment">#创建Session用来计算模型</span></span><br><span class="line">sess=tf.Session()</span><br><span class="line">print(sess.run(w))</span><br><span class="line"><span class="comment">#提示打印w的值未初始化的异常</span></span><br><span class="line"><span class="comment">#变量tensor需要经过下面的init过程后才能使用：</span></span><br></pre></td></tr></table></figure><p><img src="/2018/12/15/TensorFlow入门demo/1.jpg" alt="demo"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#初始化变量</span></span><br><span class="line"><span class="comment">#再打印w的值，就可以看到之前赋的初始值</span></span><br><span class="line">init=tf.global_variables_initializer()</span><br><span class="line">sess.run(init)</span><br><span class="line">print(sess.run(w))</span><br><span class="line"><span class="comment">#变量初始化完之后，可以先对上面的w和b设置的初始值0.1和-0.1</span></span><br><span class="line"><span class="comment">#运行一下我们的线性模型看看结果：</span></span><br><span class="line">print(sess.run(linear_model,&#123;x:[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">6</span>,<span class="number">8</span>]&#125;))</span><br><span class="line"><span class="comment">#貌似与我们实验的实际输出差距很大，我们在运行一下损失模型</span></span><br><span class="line">print(sess.run(loss,&#123;x:[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">6</span>,<span class="number">8</span>],y:[<span class="number">4.8</span>,<span class="number">8.5</span>,<span class="number">10.4</span>,<span class="number">21</span>,<span class="number">25.3</span>]&#125;))</span><br></pre></td></tr></table></figure><p><img src="/2018/12/15/TensorFlow入门demo/2.jpg" alt="demo"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span clas
      
    
    </summary>
    
      <category term="TensorFlow" scheme="http://yoursite.com/categories/TensorFlow/"/>
    
    
  </entry>
  
  <entry>
    <title>Windows10环境下的安装Python+TensorFlow</title>
    <link href="http://yoursite.com/2018/12/15/Windows10%E7%8E%AF%E5%A2%83%E4%B8%8B%E7%9A%84%E5%AE%89%E8%A3%85Python-TensorFlow/"/>
    <id>http://yoursite.com/2018/12/15/Windows10环境下的安装Python-TensorFlow/</id>
    <published>2018-12-15T02:55:16.000Z</published>
    <updated>2018-12-15T06:33:44.919Z</updated>
    
    <content type="html"><![CDATA[<p><strong>Windows10环境下的安装python+tensorflow</strong></p><p>使用Anoconda简单安装CPU版tensorflow的方法，此方法能够一键安装常用<br>机器学习所需的科学计算包，免去了手动配置python环境变量的过程。<br>安装环境：</p><ul><li>Windows10 64位</li><li>Python 3.5.2</li><li>Anocoda3 4.2.0</li><li>tensorflow 0.1.2<br><strong>1、tensorflow</strong><br>谷歌发布的开源框架，涉及到自然语言处理NLP<br>机器翻译，图像描述，图像分类等一些列技术<br>tensorflow为我们封装了大量机器学习，神经网络的函数，帮助我们高效的解决问题<br><strong>2、Anoconda</strong><br>Anoconda可以看做是Python的一个集成安装环境。<br>安装它就默认安装了python，IPython，集成开发环境Spyder和众多的包和模块<br>非常方便。而且Anaconda会为我们安装<strong>pip</strong>（强大的包管理程序）<br>我们可以在Windows的命令行中使用pip直接安装我们需要的包<br><strong>3、下载Anaconda</strong><br>官网的最新版Anaconda-4.4.0继承了python-3.6<br>但是tensorflow仍然不能支持3.6，因为我们安装集成python-3.5的Anaconda3-4.2.0<br>Anaconda下载地址：<a href="https://mirrors.tsinghua.edu.cn/anaconda/archive/" target="_blank" rel="noopener">Anaconda</a></li></ul><p>安装过后可以再命令行，输入conda list来查看Anaconda为我们集成的环境。<br>ps:快捷点Win+R,然后输入cmd，然后输入conda list来查看Anaconda为我们集成的环境。</p><p><img src="/2018/12/15/Windows10环境下的安装Python-TensorFlow/anoconda.jpg" alt="anaconda list"></p><p>环境里包含：<br>numpy(一个python的科学计算包，高效存储和处理大型矩阵)<br>pandas(包含了大量库和一些标准的数据模型，提供高效的操作大型数据集所需要的工具)<br>pip(简单好用的包管理工具)</p><p><strong>4、安装TensorFlow</strong></p><ul><li>使用pip自动安装TensorFlow</li></ul><p>​        pip install tensorflow</p><ul><li>当遇到提示更新pip版本的时候，先进行pip更新到最新版本</li></ul><p>​        python -m pip install –upgrade pip</p><ul><li>安装的时候尽量使用管理员身份来打开命令行窗口（ps:Win10系统点击左下角Win图标右键，选择管理员身份就行）</li><li>安装TensorFlow</li></ul><p>​        pip install tensorflow</p><ul><li>测试是否安装成功</li></ul><p>​        输入python进入python环境</p><p>​        输入import tensorflow,若没有红字报错，表示环境配置成功</p><p><img src="/2018/12/15/Windows10环境下的安装Python-TensorFlow/tensorflow.jpg" alt="tensorflow"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;strong&gt;Windows10环境下的安装python+tensorflow&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;使用Anoconda简单安装CPU版tensorflow的方法，此方法能够一键安装常用&lt;br&gt;机器学习所需的科学计算包，免去了手动配置python环境变量的过程
      
    
    </summary>
    
      <category term="TensorFlow" scheme="http://yoursite.com/categories/TensorFlow/"/>
    
    
  </entry>
  
  <entry>
    <title>异步与多线程</title>
    <link href="http://yoursite.com/2018/12/15/%E5%BC%82%E6%AD%A5%E4%B8%8E%E5%A4%9A%E7%BA%BF%E7%A8%8B/"/>
    <id>http://yoursite.com/2018/12/15/异步与多线程/</id>
    <published>2018-12-15T02:51:30.000Z</published>
    <updated>2018-12-15T02:52:51.072Z</updated>
    
    <content type="html"><![CDATA[<p>其实异步是目的，多线程是实现这个目的方法。</p><ul><li>异步：A发起一个操作后，可以继续自顾自的处理它自己的事儿，不用干等着这个耗时操作返回</li><li>随着拥有多线程CPU,多核的普及，多线程和异步操作等并发程序设计方法也受到更多</li></ul><p><strong>1、多线程和异步操作的异同</strong></p><ul><li>多线程和异步操作两者都可以达到避免调用线程阻塞的目的，从而提高软件的可响应性。</li><li>甚至有些时候我们认为多线程和异步操作是等同的。但是多线程和异步操作还是有一些区别</li></ul><p><strong>2、异步操作的本质：</strong></p><ul><li>所有的程序最终都会由计算机硬件来执行。所以为了更好的理解异步操作的本质，网卡，声卡，显卡都是有DMA功能的DMA就是<strong>直接内存访问</strong>的意思（Driect Memory Access）</li><li>也就是说，拥有DMA功能的硬件在和内存进行数据交换的时候可以不消耗CPU资源</li><li>只要CPU在发起数据传输时发送一个指令，硬件就开始自己和内存交换数据在传输完成后硬件会触发一个中断来通知操作完成这些无需消耗CPU时间的I/O操作正是异步操作的硬件基础。</li><li>所以即使在DOS这样的单进程（无线程概念）系统中也同样可以发起异步的DMA操作</li></ul><p><strong>3、线程的本质</strong></p><ul><li>线程不是一个计算机硬件的功能，而是操作系统提供的一种逻辑功能</li><li>线程本质上是<strong>进程中一段并发运行的代码</strong>，所以线程需要操作系统投入CPU资源来运行和调度</li></ul><p><strong>4、异步操作的特点</strong></p><ul><li>因为异步操作 不需要额外的线程负担，并且使用<strong>回调的方式</strong>进行处理在设计良好的情况下，处理函数可以不必使用共享变量，即使无法完全不用，最起码也可以减少共享变量的数量。减少了死锁的可能。</li><li>当然异步操作也并非完美无瑕。编写异步操作的复杂程度较高，程序主要使用回调方式进行处理与普通人的思维方式有些初入，而且难以调试。</li></ul><p><strong>5、多线程的优缺点</strong></p><ul><li>线程中的护理程序依然是顺序执行，符合普通人的思维习惯。所以编程简单但是多线程的缺点也明显</li><li>线程的滥用会给系统带来上下文切换的额外负担并且线程间的共享变量可能造成死锁的出现。</li><li>当需要执行I/O操作的时候，使用异步操作比使用线程+同步I/O操作更合还是I/O操作不仅包括了直接的文件，网络的读写，还包括数据库操作，WebService，HttpRequest等跨进程的调用</li><li>线程的适用范围是需要长时间CPU运算的场合比如耗时较长图形处理和算法执行，但是往往由于线程编程的简单和符合习惯，所以很多朋友往往适用线程来执行耗时较长的IO操作，这样在之后又少数几个并发操作的时候还行，需要处理大量的并发操作就不合适了，因为要不停的上下文切换</li></ul><p><strong>6、异步调用</strong></p><ul><li>异步调用并不是要减少线程的开销，目的是让调用方法的主线程不需要同步等待这个函数</li><li>调用上，从而可以让主线程继续执行他下面的代码，同时系统会从ThreadPool线程池中取一个线程来执行，帮助我们将我们要写/读的数据发送到网卡，由于不需要我们等待，我们等于同时做了两件事情</li></ul><p><strong>7、线程池的实现方法</strong></p><ul><li>线程池的实现方法与线程不一样的，初始化时候，在线程池的线程为0，当进程需要一个线程的时候，创建一个线程由此线程执行用户的方法，这个线程执行完后并不立即销毁，而是挂起等待，如果有其他方法需要</li><li>执行，则唤醒进行处理 ，只有当它等待到40秒还没有任务执行的时候才唤醒自己，并销毁自己</li><li>当然如果线程池中的线程不够处理任务的时候，会再次创建一个新的线程来执行。</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;其实异步是目的，多线程是实现这个目的方法。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;异步：A发起一个操作后，可以继续自顾自的处理它自己的事儿，不用干等着这个耗时操作返回&lt;/li&gt;
&lt;li&gt;随着拥有多线程CPU,多核的普及，多线程和异步操作等并发程序设计方法也受到更多&lt;/li&gt;
&lt;/ul&gt;
      
    
    </summary>
    
      <category term="JAVA" scheme="http://yoursite.com/categories/JAVA/"/>
    
    
  </entry>
  
  <entry>
    <title>同步与异步</title>
    <link href="http://yoursite.com/2018/12/15/%E5%90%8C%E6%AD%A5%E4%B8%8E%E5%BC%82%E6%AD%A5/"/>
    <id>http://yoursite.com/2018/12/15/同步与异步/</id>
    <published>2018-12-15T02:50:05.000Z</published>
    <updated>2018-12-15T02:50:59.235Z</updated>
    
    <content type="html"><![CDATA[<p><strong>1、同步交互</strong>：发送一个请求，需要等待返回，然后才能够发送下一个请求，有个等待过程</p><p><strong>2、异步交互</strong>：发送一个请求，不需要等待返回，随时可以再发送下一个请求，也就是不需要等待</p><ul><li>区别：一个需要等待，一个不需要等待，再部分情况下，我们的项目开发中都会优先选择不需要等待的异步交互方式。</li><li>同步：用来确保资源一次只能被一个县城使用的过程。同步对于单线程程序没有任何好处。</li></ul><p>使用同步比非同步的性能差三到四倍</p><p>线程都是独立的，而且异步执行，也就是说每个线程都包含了运行时所需要的数据或者方法</p><p>而不需要外部的资源或者方法，也不必关心其他线程的状态或者行为。</p><p>但是经常有些同时运行的线程需要共享数据，此时就需要考虑其他线程的状态和行为。</p><p>否则就不能保证程序的运行结果的正确性</p><p>需要做的是允许一个线程彻底完成他的任务后，在允许执行下一个线程执行。必须保证一个共享的</p><p>资源一次只能被一个线程使用，实现的目的过程叫做同步。</p><ul><li>比如广播就是一个异步的例子。发起者不关心接受者的状态，不需要等待接受者的返回信息</li><li>比如电话就是一个同步的例子，发起者需要等待接收者，接通电话后，通信才能开始，需要等待接收者的返回信息。</li></ul><p><strong>同步和异步的区别：</strong></p><p>在进行网络编程的时候，我们会看到同步，异步，阻塞，非阻塞4种调用方式以及他们的组合</p><p>其中同步方式，异步方式主要是由客户端client控制的</p><p><strong>同步sync：</strong></p><p>就是发起一个功能调用的时候，在没有得到结果之前，该调用就不返回或者继续执行后续操作</p><p>Java中所有方法都是同步调用，因为必须要等待结果后才能继续执行</p><p>我们说的同步，异步的时候，一般而言是指那些需要其他端写作或者需要一定时间完成的任务</p><p>简单而言：同步就是必须一件一件事做，等前一件做完了后才能做下一件事</p><p>B/S模式的表单提交：客户端提交请求–》等待服务器处理–》处理完毕返回</p><p>在这个过程中客户端（浏览器）不能做其他事情</p><p><strong>异步（Async）</strong></p><p>异步与同步相反，当一个异步过程调用发出后，调用者在没有得到结果之前，就可以继续执行后续操作</p><p>当这个调用完成后，一般通过状态，通知和回调来通知调用者。</p><p>对于异步调用，调用的返回并不受调用者控制</p><p>对于通知调用者的三种方式：</p><ul><li>状态：监听被调用者的状态（轮询），调用者需要每隔一定时间检查一次，效率会很低</li><li>通知：当被调用者执行完成后，发出通知告知调用者，不需要消耗太多性能</li><li>回调：与通知类似，当被调用者执行完成后，会调用调用者提供的回调函数</li></ul><p>B/S模式的ajax的请求，具体过程：客户端发出ajax请求–》服务端处理–》处理完毕执行客户端回调</p><p>在客户端（浏览器）发出请求后，仍然可以做其他的事情</p><p>同步与异步的区别：请求发出后，是否需要等待结果，才能继续执行其他操作。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;strong&gt;1、同步交互&lt;/strong&gt;：发送一个请求，需要等待返回，然后才能够发送下一个请求，有个等待过程&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;2、异步交互&lt;/strong&gt;：发送一个请求，不需要等待返回，随时可以再发送下一个请求，也就是不需要等待&lt;/p&gt;
&lt;ul&gt;
&lt;l
      
    
    </summary>
    
      <category term="JAVA" scheme="http://yoursite.com/categories/JAVA/"/>
    
    
  </entry>
  
  <entry>
    <title>单例模式</title>
    <link href="http://yoursite.com/2018/12/15/%E5%8D%95%E4%BE%8B%E6%A8%A1%E5%BC%8F/"/>
    <id>http://yoursite.com/2018/12/15/单例模式/</id>
    <published>2018-12-15T02:39:05.000Z</published>
    <updated>2018-12-15T02:41:37.117Z</updated>
    
    <content type="html"><![CDATA[<p>单例模式要求一个类只能有一个实例，并且提供了一个全局的访问点。</p><p>比如说，中国主席的职位是Singleton，法律规定主席选举，任何时间只能有一个主席。</p><h3 id="1、逻辑模型图："><a href="#1、逻辑模型图：" class="headerlink" title="1、逻辑模型图："></a>1、逻辑模型图：</h3><p><img src="https://images2018.cnblogs.com/blog/811614/201803/811614-20180315144708730-499694311.png" alt="img"></p><h3 id="2、物理模型图："><a href="#2、物理模型图：" class="headerlink" title="2、物理模型图："></a>2、物理模型图：</h3><p><img src="https://images2018.cnblogs.com/blog/811614/201803/811614-20180315145332295-339822939.png" alt="img"></p><h3 id="3、具体实现："><a href="#3、具体实现：" class="headerlink" title="3、具体实现："></a>3、具体实现：</h3><h4 id="3-1、懒汉式单例："><a href="#3-1、懒汉式单例：" class="headerlink" title="3.1、懒汉式单例："></a>3.1、懒汉式单例：</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">//懒汉式单例</span><br><span class="line">public sealed class Singleton//类设置属性为密封，不能被继承。</span><br><span class="line">&#123;</span><br><span class="line">    static Singleton instance=null;</span><br><span class="line">    Singleton()</span><br><span class="line">    &#123;&#125;</span><br><span class="line">    </span><br><span class="line">    public static Singleton Instance</span><br><span class="line">    &#123;</span><br><span class="line">        get</span><br><span class="line">        &#123;</span><br><span class="line">            if(instance==null)</span><br><span class="line">            &#123;</span><br><span class="line">                instance=new Singleton();</span><br><span class="line">            &#125;</span><br><span class="line">            return instance;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line">//这种实现方式，不是线程安全的，因为多线程的时候，可能会得到Singleton类的多个实例。</span><br><span class="line">//假如有两个线程都去判断if(instance==null)并且得到结果为true，这时两个线程</span><br><span class="line">//都会创建类Singleton的实例，违背了单例模式只能有一个类对象的实例的原则。</span><br></pre></td></tr></table></figure><h4 id="3-2、线程安全的单例"><a href="#3-2、线程安全的单例" class="headerlink" title="3.2、线程安全的单例"></a>3.2、线程安全的单例</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">public sealed class Singleton</span><br><span class="line">&#123;</span><br><span class="line">    static Singleton instance=null;</span><br><span class="line">    static readonly object padlock=new object();//进程辅助对象</span><br><span class="line">    </span><br><span class="line">    Singleton()</span><br><span class="line">    &#123;</span><br><span class="line">    </span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    public static Singleton Instance</span><br><span class="line">    &#123;</span><br><span class="line">        get</span><br><span class="line">        &#123;</span><br><span class="line">            lock(padlock)</span><br><span class="line">            &#123;</span><br><span class="line">                if(instance==null)</span><br><span class="line">                &#123;</span><br><span class="line">                    instance=new Singleton();</span><br><span class="line">                &#125;</span><br><span class="line">                return instance;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line">//对于线程来说是安全的，首先创建了一个进程的辅助对象，线程进入时先对padlock加锁</span><br><span class="line">//然后在检测对象是否被创建，这样可以确保只有一个实例被创建，而且加入了锁，导致程序只有</span><br><span class="line">//一个线程可以进去，对象实例有最先进入的那个线程创建，后面进来的线程进入时if(instance==null)为false</span><br><span class="line">//不会再去创建对象实例，这种方式增加了额外的开销，损失了性能。</span><br></pre></td></tr></table></figure><h4 id="3-3、双重锁定"><a href="#3-3、双重锁定" class="headerlink" title="3.3、双重锁定"></a>3.3、双重锁定</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">public sealed class Singleton</span><br><span class="line">&#123;</span><br><span class="line">    static Singleton instance=null;</span><br><span class="line">    static readonly object padlock=new object();</span><br><span class="line">    </span><br><span class="line">    Singleton()</span><br><span class="line">    &#123;</span><br><span class="line">    </span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    public static Singleton Instance</span><br><span class="line">    &#123;</span><br><span class="line">        get</span><br><span class="line">        &#123;</span><br><span class="line">            if(instance==null)</span><br><span class="line">            &#123;</span><br><span class="line">                lock(padlock)</span><br><span class="line">                &#123;</span><br><span class="line">                    if(instance==null)</span><br><span class="line">                    &#123;</span><br><span class="line">                        instance=new Singleton();</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">            return instance;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line">//对于多线程是安全的，而且并不是线程每次都加锁，只有判断对象实例没有被创建的时候才会加锁</span><br><span class="line">//加锁还再进行对象是否被创建的判断。解决了线程并发的问题，但是实际上需要这样实现，大多数我们会</span><br><span class="line">//使用静态初始化，但是静态初始化有自己的缺点，无法实现延迟初始化</span><br></pre></td></tr></table></figure><h4 id="3-4、静态初始化"><a href="#3-4、静态初始化" class="headerlink" title="3.4、静态初始化"></a>3.4、静态初始化</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">public sealed class Singleton</span><br><span class="line">&#123;</span><br><span class="line">    static readonly Singleton instance=new Singleton();</span><br><span class="line">    //静态初始化的无参构造方法</span><br><span class="line">    static Singleton()</span><br><span class="line">    &#123;</span><br><span class="line">    </span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    Singleton()</span><br><span class="line">    &#123;</span><br><span class="line">    </span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    public static Singleton Instance</span><br><span class="line">    &#123;</span><br><span class="line">        get</span><br><span class="line">        &#123;</span><br><span class="line">            return instance;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line">//类标记为sealed不能继承，因为继承可能会增加实例。</span><br><span class="line">//变量标记为readonly，表示只能在静态初始化时候或者在构造方法中分配变量。</span><br></pre></td></tr></table></figure><h4 id="3-5、延迟初始化"><a href="#3-5、延迟初始化" class="headerlink" title="3.5、延迟初始化"></a>3.5、延迟初始化</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">//如果类，结构，代理，枚举是外置，那么修饰符只有public和internal，默认是internal</span><br><span class="line">//如果是内置的，修饰符默认是private</span><br><span class="line">//举例： class A&#123;&#125;//类A是internal，internal表示同一个程序集中所有代码都可以访问类型或者成员</span><br><span class="line">//其他程序集不可以访问。</span><br><span class="line">//举例：class A</span><br><span class="line">//            &#123;</span><br><span class="line">//                class B&#123;&#125;//类B是private</span><br><span class="line">//            &#125;</span><br><span class="line">public sealed class Singleton</span><br><span class="line">&#123;</span><br><span class="line">    Singleton()//类的构造函数默认为private</span><br><span class="line">    &#123;</span><br><span class="line">    </span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    public static Singleton Instance</span><br><span class="line">    &#123;</span><br><span class="line">        get</span><br><span class="line">        &#123;</span><br><span class="line">            return Lazy.instance;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    class Lazy//lazy默认是internal</span><br><span class="line">    &#123;</span><br><span class="line">        static Lazy()</span><br><span class="line">        &#123;</span><br><span class="line">        </span><br><span class="line">        &#125;</span><br><span class="line">        internal static readonly Singleton instance=new Singleton();</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">&#125;</span><br><span class="line">//初始化的时候有一个Lazy类的一个静态成员来完成，这样实现了延迟初始化。</span><br></pre></td></tr></table></figure><h3 id="4、实现总结："><a href="#4、实现总结：" class="headerlink" title="4、实现总结："></a>4、实现总结：</h3><p>Singleton单例模式构造方法可以设置为protected，允许子类继承。</p><p>单例模式不要支持实例化，可能导致多个对象实例，也不要支持Icloneable接口，导致多个对象的实例，单例模式只考虑了对象创建的管理，没有考虑对象销毁的管理，支持垃圾回收的平台的对象的开销来讲，没必要对其销毁进行特殊的管理。</p><p>理解：Singleton模式核心：如何控制用户使用new对一个类的构造方法任意调用。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;单例模式要求一个类只能有一个实例，并且提供了一个全局的访问点。&lt;/p&gt;
&lt;p&gt;比如说，中国主席的职位是Singleton，法律规定主席选举，任何时间只能有一个主席。&lt;/p&gt;
&lt;h3 id=&quot;1、逻辑模型图：&quot;&gt;&lt;a href=&quot;#1、逻辑模型图：&quot; class=&quot;heade
      
    
    </summary>
    
      <category term="设计模式" scheme="http://yoursite.com/categories/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"/>
    
    
  </entry>
  
  <entry>
    <title>JAVA中的流(续)</title>
    <link href="http://yoursite.com/2018/12/14/JAVA%E4%B8%AD%E7%9A%84%E6%B5%81-%E7%BB%AD/"/>
    <id>http://yoursite.com/2018/12/14/JAVA中的流-续/</id>
    <published>2018-12-14T14:07:45.000Z</published>
    <updated>2018-12-14T14:11:45.603Z</updated>
    
    <content type="html"><![CDATA[<h1 id="一、流（Stream）"><a href="#一、流（Stream）" class="headerlink" title="一、流（Stream）"></a>一、流（Stream）</h1><h4 id="所谓流-Stream-，就是一系列的数据。"><a href="#所谓流-Stream-，就是一系列的数据。" class="headerlink" title="所谓流(Stream)，就是一系列的数据。"></a>所谓流(Stream)，就是一系列的数据。</h4><h4 id="当不同的介质之间有数据交互的时候，java就会使用流来实现。"><a href="#当不同的介质之间有数据交互的时候，java就会使用流来实现。" class="headerlink" title="当不同的介质之间有数据交互的时候，java就会使用流来实现。"></a>当不同的介质之间有数据交互的时候，java就会使用流来实现。</h4><h4 id="数据源可以使文件，还可以是数据库，网络，甚至是其他的程序"><a href="#数据源可以使文件，还可以是数据库，网络，甚至是其他的程序" class="headerlink" title="数据源可以使文件，还可以是数据库，网络，甚至是其他的程序"></a>数据源可以使文件，还可以是数据库，网络，甚至是其他的程序</h4><h4 id="不如读取文件的数据到程序中，站在程序的角度来看，就叫做输入流"><a href="#不如读取文件的数据到程序中，站在程序的角度来看，就叫做输入流" class="headerlink" title="不如读取文件的数据到程序中，站在程序的角度来看，就叫做输入流"></a>不如读取文件的数据到程序中，站在程序的角度来看，就叫做输入流</h4><ul><li><p>字节输入流：InputStream</p></li><li><p>字符输入流：Reader　　　　</p></li><li>缓存字符输入流：BufferedReader　　</li><li><p>数据输入流  ：DataInputStream               </p></li><li><p>字节输出流：OutputStream　　　　　　　　</p></li><li>字符输出流：Writer　　　　</li><li>缓存字符输出流：PrintWriter　　　　</li><li>数据输出流：DataOutputStream　</li></ul><p><img src="https://images2018.cnblogs.com/blog/811614/201802/811614-20180227120014277-347291744.png" alt="img"></p><h3 id="1、文件输入流"><a href="#1、文件输入流" class="headerlink" title="1、文件输入流"></a>1、文件输入流</h3><p>可以用来把数据从硬盘的文件，读取到JVM(内存)</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> stream;</span><br><span class="line"><span class="keyword">import</span> java.io.File;</span><br><span class="line"><span class="keyword">import</span> java.io.FileInputStream;</span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TestStream</span></span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span></span>&#123;</span><br><span class="line">        <span class="keyword">try</span>&#123;</span><br><span class="line">                File f=<span class="keyword">new</span> File(<span class="string">"d:/lol.txt"</span>);</span><br><span class="line">                <span class="comment">//创建基于文件的输入流</span></span><br><span class="line">                FileInputStream fis=<span class="keyword">new</span> FileInputStream(f);</span><br><span class="line">                <span class="comment">//通过这个输入流，就可以把数据从硬盘，读取到java的虚拟机中，也就是读取到内存中</span></span><br><span class="line">        &#125;<span class="keyword">catch</span>(Exception e)&#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="2、字节流"><a href="#2、字节流" class="headerlink" title="2、字节流"></a>2、字节流</h3><ul><li><p>InputStream：字节输入流</p></li><li><p>用以字节的形式读取和写入数据</p></li><li><p>InputStream是字节输入流，同时也是抽象类，只提供方法声明，不提供方法的具体实现。</p></li><li><p>FileInputStream是InputStream的子类，可以进行直接使用</p></li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> stream;</span><br><span class="line"><span class="keyword">import</span> java.io.File;</span><br><span class="line"><span class="keyword">import</span> java.io.FileInputStream;</span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TestStream</span></span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span></span>&#123;</span><br><span class="line">        <span class="keyword">try</span>&#123;</span><br><span class="line">                File f=<span class="keyword">new</span> File(<span class="string">"d:/lol.txt"</span>);</span><br><span class="line">                <span class="comment">//创建基于文件的输入流</span></span><br><span class="line">                FileInputStream fis=<span class="keyword">new</span> FileInputStream(f);</span><br><span class="line">                <span class="comment">//通过这个输入流，就可以把数据从硬盘，读取到java的虚拟机中，也就是读取到内存中</span></span><br><span class="line">                <span class="comment">//创建字节数组，其长度就是文件的长度</span></span><br><span class="line">                <span class="keyword">byte</span>[] all=<span class="keyword">new</span> <span class="keyword">byte</span>[(<span class="keyword">int</span>)f.length()];</span><br><span class="line">                <span class="comment">//以字节流的形式读取文件所有内容</span></span><br><span class="line">                fis.read(all);</span><br><span class="line">                <span class="keyword">for</span>(<span class="keyword">byte</span> b:all)&#123;</span><br><span class="line">                    System.out.println(b);</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="comment">//每次使用完流，都应该进行关闭</span></span><br><span class="line">                fis.close()</span><br><span class="line">        &#125;<span class="keyword">catch</span>(Exception e)&#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li><p>OutputStream：字节输出流，同事也是抽象类，只提供方法声明，不提供方法的具体实现。</p></li><li><p>FileOutputStream是OutputStream子类，可以进行直接使用</p></li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> stream;</span><br><span class="line"><span class="keyword">import</span> java.io.File;</span><br><span class="line"><span class="keyword">import</span> java.io.FileInputStream;</span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TestStream</span></span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span></span>&#123;</span><br><span class="line">        <span class="keyword">try</span>&#123;</span><br><span class="line">                File f=<span class="keyword">new</span> File(<span class="string">"d:/lol.txt"</span>);</span><br><span class="line">                <span class="keyword">byte</span> data[]=&#123;<span class="number">88</span>,<span class="number">89</span>&#125;;</span><br><span class="line">                <span class="comment">//创建基于文件的输出流</span></span><br><span class="line">                FileOutputStream fos=<span class="keyword">new</span> FileOutputStream(f);</span><br><span class="line">                </span><br><span class="line">                <span class="comment">//把数据写入到输出流上</span></span><br><span class="line">                fos.write(data);</span><br><span class="line">                <span class="comment">//每次使用完流，都应该进行关闭</span></span><br><span class="line">                fos.close()</span><br><span class="line">        &#125;<span class="keyword">catch</span>(Exception e)&#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>注：所有的流，不论是输入流还是输出流，使用完毕之后，都应该关闭，如果不关闭，会产生对资源占用的浪费，当量比较大时，会影响业务的正常开展。</p><p>3.流的关闭方式</p><h4 id="在finally中关闭："><a href="#在finally中关闭：" class="headerlink" title="在finally中关闭："></a>在finally中关闭：</h4><ul><li><p>首先把流的引用声明在try的外面，如果声明在try里面，其作用与无法抵达finally</p></li><li><p>在finally关闭之前，要先判断该引用是否为空</p></li><li><p>关闭的时候，要再一次的进行try。。catch处理</p></li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> stream;</span><br><span class="line"><span class="keyword">import</span> java.io.File;</span><br><span class="line"><span class="keyword">import</span> java.io.FileInputStream;</span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TestStream</span></span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span></span>&#123;</span><br><span class="line">        File f=<span class="keyword">new</span> File(<span class="string">"d/source/LOL.exe"</span>);</span><br><span class="line">        FileInputStream fis=<span class="keyword">null</span>;</span><br><span class="line">        <span class="keyword">try</span>&#123;</span><br><span class="line">                fis=<span class="keyword">new</span> FileInputStream(f);</span><br><span class="line">                <span class="keyword">byte</span>[] all=<span class="keyword">new</span> <span class="keyword">byte</span>[(<span class="keyword">int</span>)f.length()];</span><br><span class="line">                fis.read(all);</span><br><span class="line">                <span class="keyword">for</span>(<span class="keyword">byte</span> b:all)&#123;</span><br><span class="line">                    System.out.println(b);</span><br><span class="line">                &#125;</span><br><span class="line">        &#125;<span class="keyword">catch</span>(IOException e)&#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;<span class="keyword">finally</span>&#123;</span><br><span class="line">            <span class="comment">//在finally里关闭流</span></span><br><span class="line">                <span class="keyword">if</span>(<span class="keyword">null</span>!=fis)</span><br><span class="line">                    <span class="keyword">try</span>&#123;</span><br><span class="line">                        fis.close();</span><br><span class="line">                    &#125;<span class="keyword">catch</span>(IOException e)&#123;</span><br><span class="line">                         e.printStackTrace();</span><br><span class="line">                    &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="使用try-方式"><a href="#使用try-方式" class="headerlink" title="使用try()方式"></a>使用try()方式</h4><ul><li><p>把流定义在try（）里，try，catch或者finally结束的时候，会自动关闭</p></li><li><p>这种编写代码的方式叫做try-with-resources，这是JDK7开始支持的技术</p></li><li><p>所有的流，都实现了一个接口叫做AutoCloseable，任何类实现了这个接口，都可以在try()中进行实例化，并且在try，catch，finally结束的时候自动关闭，回收相关资源。</p></li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TestStream</span></span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span></span>&#123;</span><br><span class="line">        File f=<span class="keyword">new</span> File(<span class="string">"d/source/LOL.exe"</span>);</span><br><span class="line">        <span class="comment">//把流定义在try()里，try，catch或者finally结束的时候，会自动关闭</span></span><br><span class="line">        <span class="keyword">try</span>(FileInputStream fis=<span class="keyword">new</span> FileInputStream(f))&#123;</span><br><span class="line">            <span class="keyword">byte</span>[] all=<span class="keyword">new</span> <span class="keyword">byte</span>[(<span class="keyword">int</span>)f.length()];</span><br><span class="line">            fis.read(all);</span><br><span class="line">            <span class="keyword">for</span>(<span class="keyword">byte</span> b:all)&#123;</span><br><span class="line">                System.out.println(b);</span><br><span class="line">            &#125;<span class="keyword">catch</span>(IOException e)&#123;</span><br><span class="line">                    e.printStackTrace();</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;一、流（Stream）&quot;&gt;&lt;a href=&quot;#一、流（Stream）&quot; class=&quot;headerlink&quot; title=&quot;一、流（Stream）&quot;&gt;&lt;/a&gt;一、流（Stream）&lt;/h1&gt;&lt;h4 id=&quot;所谓流-Stream-，就是一系列的数据。&quot;&gt;&lt;a hre
      
    
    </summary>
    
      <category term="JAVA" scheme="http://yoursite.com/categories/JAVA/"/>
    
    
  </entry>
  
</feed>
